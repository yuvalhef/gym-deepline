{
  "MinMaxScaler0UnivariateSelectChiKbest1PCA_LAPACK2BalancedRandomForestClassifier3QuantileTransformer0f_classifFDR5_2PCA_Randomized6_1SVC7_4Normalizer0_1f_classifFPR9FastICA10_5GaussianNBClassifier11_6MajorityVoting4_8_12": {
    "id": "MinMaxScaler0UnivariateSelectChiKbest1PCA_LAPACK2BalancedRandomForestClassifier3QuantileTransformer0f_classifFDR5_2PCA_Randomized6_1SVC7_4Normalizer0_1f_classifFPR9FastICA10_5GaussianNBClassifier11_6MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "QuantileTransformer0f_classifFPR1PCA_Randomized2GaussianProcessClassifierPrim3StandardScaler0mutual_info_classifKbest5_1PCA_LAPACK6ExtraTreesClassifier7_1f_classifFPR0RandomTreesEmbedding9_7SVC10_3MajorityVoting4_8_11": {
    "id": "QuantileTransformer0f_classifFPR1PCA_Randomized2GaussianProcessClassifierPrim3StandardScaler0mutual_info_classifKbest5_1PCA_LAPACK6ExtraTreesClassifier7_1f_classifFPR0RandomTreesEmbedding9_7SVC10_3MajorityVoting4_8_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          7
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          8,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "MinMaxScaler0UnivariateSelectChiFWE1PCA_Randomized2RUSBoostClassifier3MinMaxScaler0RFE_GradientBoosting5RandomTreesEmbedding6_1RidgeClassifier7_3StandardScaler0_5f_classifPercentile9_6PCA_Randomized10ExtraTreesClassifier11_8MajorityVoting4_8_12": {
    "id": "MinMaxScaler0UnivariateSelectChiFWE1PCA_Randomized2RUSBoostClassifier3MinMaxScaler0RFE_GradientBoosting5RandomTreesEmbedding6_1RidgeClassifier7_3StandardScaler0_5f_classifPercentile9_6PCA_Randomized10ExtraTreesClassifier11_8MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          8
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOrdinal0f_classifFPR1InteractionFeatures2RUSBoostClassifier3PowerTransformer0RFE_GradientBoosting5_1KernelPCA6_3BaggingClassifier7_2KBinsDiscretizerOneHot0_5UnivariateSelectChiPercentile9_2PolynomialFeatures10_6LGBMClassifier11_1MajorityVoting4_8_12": {
    "id": "KBinsDiscretizerOrdinal0f_classifFPR1InteractionFeatures2RUSBoostClassifier3PowerTransformer0RFE_GradientBoosting5_1KernelPCA6_3BaggingClassifier7_2KBinsDiscretizerOneHot0_5UnivariateSelectChiPercentile9_2PolynomialFeatures10_6LGBMClassifier11_1MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "MinMaxScaler0UnivariateSelectChiFPR1TruncatedSVD2LogisticRegression3MinMaxScaler0UnivariateSelectChiFPR5_2PCA_LAPACK6_3GaussianProcessClassifierPrim7_4MinMaxScaler0mutual_info_classifPercentile9_5PCA_Randomized10_5BalancedRandomForestClassifier11_1RandomForestMeta4_8_12": {
    "id": "MinMaxScaler0UnivariateSelectChiFPR1TruncatedSVD2LogisticRegression3MinMaxScaler0UnivariateSelectChiFPR5_2PCA_LAPACK6_3GaussianProcessClassifierPrim7_4MinMaxScaler0mutual_info_classifPercentile9_5PCA_Randomized10_5BalancedRandomForestClassifier11_1RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0mutual_info_classifPercentile1RandomTreesEmbedding2RidgeClassifier3QuantileTransformer0_1f_classifPercentile5_2TruncatedSVD6_3QuadraticDiscriminantAnalysis7_4QuantileTransformer0_5f_classifKbest9_1RandomTreesEmbedding10_1ComplementNBClassifier11_3RandomForestMeta4_8_12": {
    "id": "QuantileTransformer0mutual_info_classifPercentile1RandomTreesEmbedding2RidgeClassifier3QuantileTransformer0_1f_classifPercentile5_2TruncatedSVD6_3QuadraticDiscriminantAnalysis7_4QuantileTransformer0_5f_classifKbest9_1RandomTreesEmbedding10_1ComplementNBClassifier11_3RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          3
        ],
        "primitive": {
          "name": "ComplementNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "The Complement Naive Bayes classifier described in Rennie et al. (2003). The Complement Naive Bayes classifier was designed to correct the \u201csevere assumptions\u201d made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0f_classifFPR1InteractionFeatures2EasyEnsembleClassifier3MinMaxScaler0_1mutual_info_classifKbest5_1PCA_LAPACK6SGDClassifier7_4Normalizer0_1mutual_info_classifPercentile9_5IncrementalPCA10_7RidgeClassifier11MajorityVoting4_8_12": {
    "id": "PowerTransformer0f_classifFPR1InteractionFeatures2EasyEnsembleClassifier3MinMaxScaler0_1mutual_info_classifKbest5_1PCA_LAPACK6SGDClassifier7_4Normalizer0_1mutual_info_classifPercentile9_5IncrementalPCA10_7RidgeClassifier11MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "QuantileTransformer0mutual_info_classifPercentile1IncrementalPCA2KNeighborsClassifierPrim3QuantileTransformer0_1UnivariateSelectChiFDR5_2IncrementalPCA6_2XGBClassifier7_2MinMaxScaler0_1UnivariateSelectChiFWE9_1PCA_Randomized10_1LogisticRegressionCV11_1RandomForestMeta4_8_12": {
    "id": "QuantileTransformer0mutual_info_classifPercentile1IncrementalPCA2KNeighborsClassifierPrim3QuantileTransformer0_1UnivariateSelectChiFDR5_2IncrementalPCA6_2XGBClassifier7_2MinMaxScaler0_1UnivariateSelectChiFWE9_1PCA_Randomized10_1LogisticRegressionCV11_1RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with Chi-square. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0RFE_GradientBoosting1InteractionFeatures2RF_classifier3PowerTransformer0_1RFE_RandomForest5_2TruncatedSVD6_2GaussianNBClassifier7PowerTransformer0_5RFE_RandomForest9_5PCA_LAPACK10_1AdaBoostClassifier11_1MajorityVoting4_8_12": {
    "id": "Normalizer0RFE_GradientBoosting1InteractionFeatures2RF_classifier3PowerTransformer0_1RFE_RandomForest5_2TruncatedSVD6_2GaussianNBClassifier7PowerTransformer0_5RFE_RandomForest9_5PCA_LAPACK10_1AdaBoostClassifier11_1MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "RFE_GradientBoosting0PolynomialFeatures1LogisticRegressionCV2QuantileTransformer0TruncatedSVD4_2BaggingClassifier5_1QuantileTransformer0f_classifPercentile7_1PCA_Randomized8_4QuadraticDiscriminantAnalysis9_6MajorityVoting3_6_10": {
    "id": "RFE_GradientBoosting0PolynomialFeatures1LogisticRegressionCV2QuantileTransformer0TruncatedSVD4_2BaggingClassifier5_1QuantileTransformer0f_classifPercentile7_1PCA_Randomized8_4QuadraticDiscriminantAnalysis9_6MajorityVoting3_6_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          3,
          6,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "StandardScaler0RFE_RandomForest1RF_classifier2RFE_RandomForest0_2InteractionFeatures4_1RidgeClassifier5_3RobustScaler0mutual_info_classifPercentile7InteractionFeatures8XGBClassifier9_4MajorityVoting3_6_10": {
    "id": "StandardScaler0RFE_RandomForest1RF_classifier2RFE_RandomForest0_2InteractionFeatures4_1RidgeClassifier5_3RobustScaler0mutual_info_classifPercentile7InteractionFeatures8XGBClassifier9_4MajorityVoting3_6_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          4
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          3,
          6,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "MinMaxScaler0f_classifFPR1PCA_Randomized2GradientBoostingClassifier3KBinsDiscretizerOneHot0_1f_classifFPR5PCA_Randomized6KNeighborsClassifierPrim7_3Normalizer0_1f_classifFWE9_5PolynomialFeatures10_5MultinomialNB11_2RandomForestMeta4_8_12": {
    "id": "MinMaxScaler0f_classifFPR1PCA_Randomized2GradientBoostingClassifier3KBinsDiscretizerOneHot0_1f_classifFPR5PCA_Randomized6KNeighborsClassifierPrim7_3Normalizer0_1f_classifFWE9_5PolynomialFeatures10_5MultinomialNB11_2RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          2
        ],
        "primitive": {
          "name": "MultinomialNB",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multinomial models. The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0mutual_info_classifPercentile1PCA_LAPACK2GradientBoostingClassifier3PowerTransformer0RFE_GradientBoosting5_2RandomTreesEmbedding6GradientBoostingClassifier7_4RobustScaler0VarianceThreshold9_2RandomTreesEmbedding10_6KNeighborsClassifierPrim11RandomForestMeta4_8_12": {
    "id": "MaxAbsScaler0mutual_info_classifPercentile1PCA_LAPACK2GradientBoostingClassifier3PowerTransformer0RFE_GradientBoosting5_2RandomTreesEmbedding6GradientBoostingClassifier7_4RobustScaler0VarianceThreshold9_2RandomTreesEmbedding10_6KNeighborsClassifierPrim11RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0f_classifPercentile1FastICA2BalancedRandomForestClassifier3KBinsDiscretizerOrdinal0VarianceThreshold5FastICA6GaussianNBClassifier7_4MaxAbsScaler0_5RFE_RandomForest9TruncatedSVD10_5LGBMClassifier11_5RandomForestMeta4_8_12": {
    "id": "PowerTransformer0f_classifPercentile1FastICA2BalancedRandomForestClassifier3KBinsDiscretizerOrdinal0VarianceThreshold5FastICA6GaussianNBClassifier7_4MaxAbsScaler0_5RFE_RandomForest9TruncatedSVD10_5LGBMClassifier11_5RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          5
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0RFE_RandomForest1InteractionFeatures2RidgeClassifier3RobustScaler0mutual_info_classifKbest5_2RandomTreesEmbedding6_2AdaBoostClassifier7_2MaxAbsScaler0_5mutual_info_classifPercentile9KernelPCA10_6BaggingClassifier11MajorityVoting4_8_12": {
    "id": "QuantileTransformer0RFE_RandomForest1InteractionFeatures2RidgeClassifier3RobustScaler0mutual_info_classifKbest5_2RandomTreesEmbedding6_2AdaBoostClassifier7_2MaxAbsScaler0_5mutual_info_classifPercentile9KernelPCA10_6BaggingClassifier11MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifPercentile1IncrementalPCA2RidgeClassifierCV3Normalizer0_1f_classifFWE5InteractionFeatures6_1KNeighborsClassifierPrim7_1RFE_GradientBoosting0_1PCA_Randomized9_5LinearDiscriminantAnalysisPrim10_7RandomForestMeta4_8_11": {
    "id": "MaxAbsScaler0f_classifPercentile1IncrementalPCA2RidgeClassifierCV3Normalizer0_1f_classifFWE5InteractionFeatures6_1KNeighborsClassifierPrim7_1RFE_GradientBoosting0_1PCA_Randomized9_5LinearDiscriminantAnalysisPrim10_7RandomForestMeta4_8_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          8,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0KernelPCA1GaussianProcessClassifierPrim2MaxAbsScaler0mutual_info_classifKbest4IncrementalPCA5_2PassiveAggressiveClassifier6_2StandardScaler0_1VarianceThreshold8_5InteractionFeatures9_2QuadraticDiscriminantAnalysis10_4MajorityVoting3_7_11": {
    "id": "StandardScaler0KernelPCA1GaussianProcessClassifierPrim2MaxAbsScaler0mutual_info_classifKbest4IncrementalPCA5_2PassiveAggressiveClassifier6_2StandardScaler0_1VarianceThreshold8_5InteractionFeatures9_2QuadraticDiscriminantAnalysis10_4MajorityVoting3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          5
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          4
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifFPR1PCA_LAPACK2LogisticRegression3RobustScaler0VarianceThreshold5_2TruncatedSVD6_2KNeighborsClassifierPrim7_4MinMaxScaler0_1RFE_RandomForest9_6TruncatedSVD10_1SVC11_8RandomForestMeta4_8_12": {
    "id": "MaxAbsScaler0f_classifFPR1PCA_LAPACK2LogisticRegression3RobustScaler0VarianceThreshold5_2TruncatedSVD6_2KNeighborsClassifierPrim7_4MinMaxScaler0_1RFE_RandomForest9_6TruncatedSVD10_1SVC11_8RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          8
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0mutual_info_classifKbest1TruncatedSVD2ExtraTreesClassifier3ImputerMedian0StandardScaler5_1f_classifPercentile6_1PCA_LAPACK7RidgeClassifierCV8_2imputer0StandardScaler10f_classifFWE11_6KernelPCA12_5LogisticRegression13_3RandomForestMeta4_9_14": {
    "id": "ImputerEncoderPrim0mutual_info_classifKbest1TruncatedSVD2ExtraTreesClassifier3ImputerMedian0StandardScaler5_1f_classifPercentile6_1PCA_LAPACK7RidgeClassifierCV8_2imputer0StandardScaler10f_classifFWE11_6KernelPCA12_5LogisticRegression13_3RandomForestMeta4_9_14",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          5
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          4,
          9,
          14
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0mutual_info_classifKbest1TruncatedSVD2ExtraTreesClassifier3ImputerMedian0StandardScaler5_1f_classifPercentile6_1PCA_LAPACK7RidgeClassifierCV8_2imputer0StandardScaler10f_classifFWE11_6KernelPCA12_5BaggingClassifier13_2RandomForestMeta4_9_14": {
    "id": "ImputerEncoderPrim0mutual_info_classifKbest1TruncatedSVD2ExtraTreesClassifier3ImputerMedian0StandardScaler5_1f_classifPercentile6_1PCA_LAPACK7RidgeClassifierCV8_2imputer0StandardScaler10f_classifFWE11_6KernelPCA12_5BaggingClassifier13_2RandomForestMeta4_9_14",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          5
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          2
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          4,
          9,
          14
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0RFE_GradientBoosting1KernelPCA2KNeighborsClassifierPrim3RobustScaler0_1VarianceThreshold5InteractionFeatures6GaussianProcessClassifierPrim7_3RobustScaler0_5f_classifPercentile9_5PCA_LAPACK10ExtraTreesClassifier11_2RandomForestMeta4_8_12": {
    "id": "PowerTransformer0RFE_GradientBoosting1KernelPCA2KNeighborsClassifierPrim3RobustScaler0_1VarianceThreshold5InteractionFeatures6GaussianProcessClassifierPrim7_3RobustScaler0_5f_classifPercentile9_5PCA_LAPACK10ExtraTreesClassifier11_2RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          2
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifPercentile1PCA_ARPACK2LogisticRegression3StandardScaler0_1f_classifFPR5FastICA6QuadraticDiscriminantAnalysis7_4Normalizer0VarianceThreshold9_1IncrementalPCA10_5GradientBoostingClassifier11MajorityVoting4_8_12": {
    "id": "MaxAbsScaler0f_classifPercentile1PCA_ARPACK2LogisticRegression3StandardScaler0_1f_classifFPR5FastICA6QuadraticDiscriminantAnalysis7_4Normalizer0VarianceThreshold9_1IncrementalPCA10_5GradientBoostingClassifier11MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "RobustScaler0mutual_info_classifKbest1FastICA2GaussianProcessClassifierPrim3PowerTransformer0f_classifFWE5_2TruncatedSVD6_3GradientBoostingClassifier7_3MinMaxScaler0f_classifFPR9_5PolynomialFeatures10_5LinearDiscriminantAnalysisPrim11_3RandomForestMeta4_8_12": {
    "id": "RobustScaler0mutual_info_classifKbest1FastICA2GaussianProcessClassifierPrim3PowerTransformer0f_classifFWE5_2TruncatedSVD6_3GradientBoostingClassifier7_3MinMaxScaler0f_classifFPR9_5PolynomialFeatures10_5LinearDiscriminantAnalysisPrim11_3RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          3
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "imputerIndicator0MaxAbsScaler1f_classifFPR2RandomTreesEmbedding3RUSBoostClassifier4ImputerEncoderPrim0_1Normalizer6RFE_GradientBoosting7_2IncrementalPCA8_4RF_classifier9_4ImputerMedian0PowerTransformer11_7mutual_info_classifKbest12_3IncrementalPCA13_8PassiveAggressiveClassifier14_10MajorityVoting5_10_15": {
    "id": "imputerIndicator0MaxAbsScaler1f_classifFPR2RandomTreesEmbedding3RUSBoostClassifier4ImputerEncoderPrim0_1Normalizer6RFE_GradientBoosting7_2IncrementalPCA8_4RF_classifier9_4ImputerMedian0PowerTransformer11_7mutual_info_classifKbest12_3IncrementalPCA13_8PassiveAggressiveClassifier14_10MajorityVoting5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          4
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          3
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          8
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          10
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "f_classifFDR0FastICA1XGBClassifier2StandardScaler0RFE_GradientBoosting4_1IncrementalPCA5_1GaussianNBClassifier6_1QuantileTransformer0f_classifFDR8_1IncrementalPCA9AdaBoostClassifier10_2RandomForestMeta3_7_11": {
    "id": "f_classifFDR0FastICA1XGBClassifier2StandardScaler0RFE_GradientBoosting4_1IncrementalPCA5_1GaussianNBClassifier6_1QuantileTransformer0f_classifFDR8_1IncrementalPCA9AdaBoostClassifier10_2RandomForestMeta3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0f_classifFWE1TruncatedSVD2RidgeClassifier3StandardScaler0mutual_info_classifKbest5_2InteractionFeatures6_1SVC7_2StandardScaler0_5f_classifFPR9FastICA10_1RF_classifier11_8MajorityVoting4_8_12": {
    "id": "MinMaxScaler0f_classifFWE1TruncatedSVD2RidgeClassifier3StandardScaler0mutual_info_classifKbest5_2InteractionFeatures6_1SVC7_2StandardScaler0_5f_classifFPR9FastICA10_1RF_classifier11_8MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          8
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "imputer0MinMaxScaler1f_classifPercentile2TruncatedSVD3BalancedRandomForestClassifier4imputerIndicator0_1QuantileTransformer6_1f_classifFDR7_1KernelPCA8_1BaggingClassifier9_1ImputerMedian0_1MaxAbsScaler11_1mutual_info_classifPercentile12_6PCA_ARPACK13_1LinearSVC14_2MajorityVoting5_10_15": {
    "id": "imputer0MinMaxScaler1f_classifPercentile2TruncatedSVD3BalancedRandomForestClassifier4imputerIndicator0_1QuantileTransformer6_1f_classifFDR7_1KernelPCA8_1BaggingClassifier9_1ImputerMedian0_1MaxAbsScaler11_1mutual_info_classifPercentile12_6PCA_ARPACK13_1LinearSVC14_2MajorityVoting5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          6
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          2
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "imputer0PowerTransformer1f_classifPercentile2PCA_LAPACK3GaussianNBClassifier4RandomForestMeta5": {
    "id": "imputer0PowerTransformer1f_classifPercentile2PCA_LAPACK3GaussianNBClassifier4RandomForestMeta5",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0RFE_RandomForest1BalancedRandomForestClassifier2imputerIndicator0_1QuantileTransformer4_1f_classifPercentile5_1TruncatedSVD6SGDClassifier7_3imputer0_1StandardScaler9f_classifKbest10_1IncrementalPCA11_4BernoulliNBClassifier12_4MajorityVoting3_8_13": {
    "id": "ImputerEncoderPrim0RFE_RandomForest1BalancedRandomForestClassifier2imputerIndicator0_1QuantileTransformer4_1f_classifPercentile5_1TruncatedSVD6SGDClassifier7_3imputer0_1StandardScaler9f_classifKbest10_1IncrementalPCA11_4BernoulliNBClassifier12_4MajorityVoting3_8_13",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          4
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          4
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          3,
          8,
          13
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0mutual_info_classifKbest1TruncatedSVD2ExtraTreesClassifier3ImputerMedian0StandardScaler5_1f_classifPercentile6_1PCA_LAPACK7RidgeClassifierCV8_2imputer0StandardScaler10f_classifFWE11_6KernelPCA12_5EasyEnsembleClassifier13_4RandomForestMeta4_9_14": {
    "id": "ImputerEncoderPrim0mutual_info_classifKbest1TruncatedSVD2ExtraTreesClassifier3ImputerMedian0StandardScaler5_1f_classifPercentile6_1PCA_LAPACK7RidgeClassifierCV8_2imputer0StandardScaler10f_classifFWE11_6KernelPCA12_5EasyEnsembleClassifier13_4RandomForestMeta4_9_14",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          5
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          4
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          4,
          9,
          14
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0RFE_GradientBoosting1PCA_LAPACK2KNeighborsClassifierPrim3RobustScaler0_1VarianceThreshold5TruncatedSVD6GaussianNBClassifier7_4f_classifKbest0_5IncrementalPCA9GaussianNBClassifier10_8RandomForestMeta4_8_11": {
    "id": "Normalizer0RFE_GradientBoosting1PCA_LAPACK2KNeighborsClassifierPrim3RobustScaler0_1VarianceThreshold5TruncatedSVD6GaussianNBClassifier7_4f_classifKbest0_5IncrementalPCA9GaussianNBClassifier10_8RandomForestMeta4_8_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          8
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          8,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0PowerTransformer1VarianceThreshold2RandomTreesEmbedding3GradientBoostingClassifier4ImputerEncoderPrim0QuantileTransformer6_1f_classifFWE7_2IncrementalPCA8_4LogisticRegression9_3imputer0_6PowerTransformer11_2f_classifFDR12_3KernelPCA13_6GradientBoostingClassifier14_5RandomForestMeta5_10_15": {
    "id": "ImputerMedian0PowerTransformer1VarianceThreshold2RandomTreesEmbedding3GradientBoostingClassifier4ImputerEncoderPrim0QuantileTransformer6_1f_classifFWE7_2IncrementalPCA8_4LogisticRegression9_3imputer0_6PowerTransformer11_2f_classifFDR12_3KernelPCA13_6GradientBoostingClassifier14_5RandomForestMeta5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0,
          6
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          2
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          3
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          6
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          5
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0PCA_Randomized1GaussianProcessClassifierPrim2imputer0KBinsDiscretizerOneHot4RFE_RandomForest5_1IncrementalPCA6EasyEnsembleClassifier7ImputerMedian0Normalizer9_1f_classifKbest10_5PCA_ARPACK11_6EasyEnsembleClassifier12_4MajorityVoting3_8_13": {
    "id": "ImputerEncoderPrim0PCA_Randomized1GaussianProcessClassifierPrim2imputer0KBinsDiscretizerOneHot4RFE_RandomForest5_1IncrementalPCA6EasyEnsembleClassifier7ImputerMedian0Normalizer9_1f_classifKbest10_5PCA_ARPACK11_6EasyEnsembleClassifier12_4MajorityVoting3_8_13",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          4
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          3,
          8,
          13
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0VarianceThreshold1PolynomialFeatures2GradientBoostingClassifier3imputerIndicator0_1StandardScaler5_1f_classifFDR6IncrementalPCA7_2RF_classifier8_4ImputerMedian0_5PowerTransformer10_5VarianceThreshold11_5TruncatedSVD12_8RidgeClassifierCV13_2MajorityVoting4_9_14": {
    "id": "ImputerEncoderPrim0VarianceThreshold1PolynomialFeatures2GradientBoostingClassifier3imputerIndicator0_1StandardScaler5_1f_classifFDR6IncrementalPCA7_2RF_classifier8_4ImputerMedian0_5PowerTransformer10_5VarianceThreshold11_5TruncatedSVD12_8RidgeClassifierCV13_2MajorityVoting4_9_14",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          5
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          8
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          2
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          4,
          9,
          14
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "MinMaxScaler0VarianceThreshold1TruncatedSVD2EasyEnsembleClassifier3MaxAbsScaler0UnivariateSelectChiFWE5_1PCA_LAPACK6_2GaussianProcessClassifierPrim7_2PowerTransformer0_1mutual_info_classifKbest9_5IncrementalPCA10RUSBoostClassifier11_8RandomForestMeta4_8_12": {
    "id": "MinMaxScaler0VarianceThreshold1TruncatedSVD2EasyEnsembleClassifier3MaxAbsScaler0UnivariateSelectChiFWE5_1PCA_LAPACK6_2GaussianProcessClassifierPrim7_2PowerTransformer0_1mutual_info_classifKbest9_5IncrementalPCA10RUSBoostClassifier11_8RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          8
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0RFE_RandomForest1BalancedRandomForestClassifier2imputerIndicator0_1f_classifPercentile4_1TruncatedSVD5BernoulliNBClassifier6_1ImputerEncoderPrim0_1StandardScaler8mutual_info_classifKbest9_1RandomTreesEmbedding10_2QuadraticDiscriminantAnalysis11_2MajorityVoting3_7_12": {
    "id": "ImputerEncoderPrim0RFE_RandomForest1BalancedRandomForestClassifier2imputerIndicator0_1f_classifPercentile4_1TruncatedSVD5BernoulliNBClassifier6_1ImputerEncoderPrim0_1StandardScaler8mutual_info_classifKbest9_1RandomTreesEmbedding10_2QuadraticDiscriminantAnalysis11_2MajorityVoting3_7_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          2
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          3,
          7,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0f_classifFDR1InteractionFeatures2GradientBoostingClassifier3imputerIndicator0_1StandardScaler5_1f_classifFDR6TruncatedSVD7_1LinearDiscriminantAnalysisPrim8_3ImputerMedian0_1RobustScaler10f_classifFDR11_5FastICA12_3RidgeClassifier13_2MajorityVoting4_9_14": {
    "id": "ImputerEncoderPrim0f_classifFDR1InteractionFeatures2GradientBoostingClassifier3imputerIndicator0_1StandardScaler5_1f_classifFDR6TruncatedSVD7_1LinearDiscriminantAnalysisPrim8_3ImputerMedian0_1RobustScaler10f_classifFDR11_5FastICA12_3RidgeClassifier13_2MajorityVoting4_9_14",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          3
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          5
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          2
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          4,
          9,
          14
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "QuantileTransformer0UnivariateSelectChiFDR1IncrementalPCA2ExtraTreesClassifier3RobustScaler0mutual_info_classifPercentile5PCA_LAPACK6_2SGDClassifier7_4PowerTransformer0_1mutual_info_classifKbest9_5PCA_ARPACK10RidgeClassifierCV11_5RandomForestMeta4_8_12": {
    "id": "QuantileTransformer0UnivariateSelectChiFDR1IncrementalPCA2ExtraTreesClassifier3RobustScaler0mutual_info_classifPercentile5PCA_LAPACK6_2SGDClassifier7_4PowerTransformer0_1mutual_info_classifKbest9_5PCA_ARPACK10RidgeClassifierCV11_5RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with Chi-square. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          5
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0RFE_RandomForest1KernelPCA2AdaBoostClassifier3QuantileTransformer0_1f_classifFDR5_2PCA_Randomized6_1GaussianProcessClassifierPrim7_3MaxAbsScaler0_5f_classifKbest9PolynomialFeatures10_2SVC11_7MajorityVoting4_8_12": {
    "id": "Normalizer0RFE_RandomForest1KernelPCA2AdaBoostClassifier3QuantileTransformer0_1f_classifFDR5_2PCA_Randomized6_1GaussianProcessClassifierPrim7_3MaxAbsScaler0_5f_classifKbest9PolynomialFeatures10_2SVC11_7MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "QuantileTransformer0UnivariateSelectChiKbest1PCA_Randomized2SGDClassifier3Normalizer0f_classifKbest5TruncatedSVD6_2DecisionTreeClassifier7_4MinMaxScaler0f_classifFDR9IncrementalPCA10_1GradientBoostingClassifier11_1RandomForestMeta4_8_12": {
    "id": "QuantileTransformer0UnivariateSelectChiKbest1PCA_Randomized2SGDClassifier3Normalizer0f_classifKbest5TruncatedSVD6_2DecisionTreeClassifier7_4MinMaxScaler0f_classifFDR9IncrementalPCA10_1GradientBoostingClassifier11_1RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0RFE_GradientBoosting1PCA_LAPACK2QuadraticDiscriminantAnalysis3Normalizer0mutual_info_classifKbest5PCA_LAPACK6_2NearestCentroid7_1PowerTransformer0f_classifKbest9_6PCA_ARPACK10_5NearestCentroid11_2MajorityVoting4_8_12": {
    "id": "StandardScaler0RFE_GradientBoosting1PCA_LAPACK2QuadraticDiscriminantAnalysis3Normalizer0mutual_info_classifKbest5PCA_LAPACK6_2NearestCentroid7_1PowerTransformer0f_classifKbest9_6PCA_ARPACK10_5NearestCentroid11_2MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          2
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "RFE_RandomForest0KernelPCA1XGBClassifier2QuantileTransformer0UnivariateSelectChiFDR4PolynomialFeatures5_1KNeighborsClassifierPrim6_2MaxAbsScaler0_4f_classifFPR8_5PolynomialFeatures9_1BernoulliNBClassifier10_5MajorityVoting3_7_11": {
    "id": "RFE_RandomForest0KernelPCA1XGBClassifier2QuantileTransformer0UnivariateSelectChiFDR4PolynomialFeatures5_1KNeighborsClassifierPrim6_2MaxAbsScaler0_4f_classifFPR8_5PolynomialFeatures9_1BernoulliNBClassifier10_5MajorityVoting3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "UnivariateSelectChiFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with Chi-square. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          5
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "Normalizer0VarianceThreshold1PolynomialFeatures2BernoulliNBClassifier3MaxAbsScaler0_1f_classifFDR5IncrementalPCA6_2QuadraticDiscriminantAnalysis7_4MinMaxScaler0_5f_classifKbest9_2TruncatedSVD10_5SVC11_3MajorityVoting4_8_12": {
    "id": "Normalizer0VarianceThreshold1PolynomialFeatures2BernoulliNBClassifier3MaxAbsScaler0_1f_classifFDR5IncrementalPCA6_2QuadraticDiscriminantAnalysis7_4MinMaxScaler0_5f_classifKbest9_2TruncatedSVD10_5SVC11_3MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "QuantileTransformer0f_classifKbest1TruncatedSVD2LinearSVC3QuantileTransformer0f_classifFWE5_2InteractionFeatures6_1BernoulliNBClassifier7Normalizer0_1f_classifPercentile9_2KernelPCA10BaggingClassifier11MajorityVoting4_8_12": {
    "id": "QuantileTransformer0f_classifKbest1TruncatedSVD2LinearSVC3QuantileTransformer0f_classifFWE5_2InteractionFeatures6_1BernoulliNBClassifier7Normalizer0_1f_classifPercentile9_2KernelPCA10BaggingClassifier11MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "UnivariateSelectChiFWE0PCA_ARPACK1BalancedRandomForestClassifier2KBinsDiscretizerOrdinal0f_classifFDR4PolynomialFeatures5_2BalancedRandomForestClassifier6_1StandardScaler0f_classifKbest8_1RandomTreesEmbedding9_1MultinomialNB10_1RandomForestMeta3_7_11": {
    "id": "UnivariateSelectChiFWE0PCA_ARPACK1BalancedRandomForestClassifier2KBinsDiscretizerOrdinal0f_classifFDR4PolynomialFeatures5_2BalancedRandomForestClassifier6_1StandardScaler0f_classifKbest8_1RandomTreesEmbedding9_1MultinomialNB10_1RandomForestMeta3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "MultinomialNB",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multinomial models. The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFPR1PCA_ARPACK2XGBClassifier3RobustScaler0_1VarianceThreshold5TruncatedSVD6RUSBoostClassifier7_4KBinsDiscretizerOrdinal0_5mutual_info_classifKbest9_5PCA_Randomized10RF_classifier11_7MajorityVoting4_8_12": {
    "id": "StandardScaler0f_classifFPR1PCA_ARPACK2XGBClassifier3RobustScaler0_1VarianceThreshold5TruncatedSVD6RUSBoostClassifier7_4KBinsDiscretizerOrdinal0_5mutual_info_classifKbest9_5PCA_Randomized10RF_classifier11_7MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "StandardScaler0mutual_info_classifKbest1RandomTreesEmbedding2AdaBoostClassifier3MinMaxScaler0mutual_info_classifPercentile5InteractionFeatures6_2LogisticRegressionCV7_1Normalizer0_1f_classifFWE9_2IncrementalPCA10_2AdaBoostClassifier11_3RandomForestMeta4_8_12": {
    "id": "StandardScaler0mutual_info_classifKbest1RandomTreesEmbedding2AdaBoostClassifier3MinMaxScaler0mutual_info_classifPercentile5InteractionFeatures6_2LogisticRegressionCV7_1Normalizer0_1f_classifFWE9_2IncrementalPCA10_2AdaBoostClassifier11_3RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          3
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0mutual_info_classifKbest1InteractionFeatures2LogisticRegression3PowerTransformer0f_classifPercentile5_2PCA_LAPACK6_3GradientBoostingClassifier7_2PowerTransformer0_1f_classifFDR9_2PCA_ARPACK10_7GaussianNBClassifier11_3RandomForestMeta4_8_12": {
    "id": "PowerTransformer0mutual_info_classifKbest1InteractionFeatures2LogisticRegression3PowerTransformer0f_classifPercentile5_2PCA_LAPACK6_3GradientBoostingClassifier7_2PowerTransformer0_1f_classifFDR9_2PCA_ARPACK10_7GaussianNBClassifier11_3RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          3
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "mutual_info_classifPercentile0PCA_LAPACK1KNeighborsClassifierPrim2RobustScaler0f_classifFWE4_1PolynomialFeatures5_1PassiveAggressiveClassifier6_1QuantileTransformer0f_classifFDR8_4IncrementalPCA9_2PassiveAggressiveClassifier10RandomForestMeta3_7_11": {
    "id": "mutual_info_classifPercentile0PCA_LAPACK1KNeighborsClassifierPrim2RobustScaler0f_classifFWE4_1PolynomialFeatures5_1PassiveAggressiveClassifier6_1QuantileTransformer0f_classifFDR8_4IncrementalPCA9_2PassiveAggressiveClassifier10RandomForestMeta3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOrdinal0f_classifKbest1GradientBoostingClassifier2RobustScaler0_1f_classifFPR4_1FastICA5_1LinearDiscriminantAnalysisPrim6PowerTransformer0f_classifFDR8_5FastICA9_5RF_classifier10_1RandomForestMeta3_7_11": {
    "id": "KBinsDiscretizerOrdinal0f_classifKbest1GradientBoostingClassifier2RobustScaler0_1f_classifFPR4_1FastICA5_1LinearDiscriminantAnalysisPrim6PowerTransformer0f_classifFDR8_5FastICA9_5RF_classifier10_1RandomForestMeta3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          5
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0UnivariateSelectChiFPR1RandomTreesEmbedding2QuadraticDiscriminantAnalysis3MinMaxScaler0_1f_classifPercentile5_2InteractionFeatures6DecisionTreeClassifier7_1PowerTransformer0VarianceThreshold9_1PCA_ARPACK10_1LogisticRegression11_1RandomForestMeta4_8_12": {
    "id": "MinMaxScaler0UnivariateSelectChiFPR1RandomTreesEmbedding2QuadraticDiscriminantAnalysis3MinMaxScaler0_1f_classifPercentile5_2InteractionFeatures6DecisionTreeClassifier7_1PowerTransformer0VarianceThreshold9_1PCA_ARPACK10_1LogisticRegression11_1RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0mutual_info_classifKbest1PCA_ARPACK2GradientBoostingClassifier3QuantileTransformer0f_classifFPR5_2TruncatedSVD6_3BalancedRandomForestClassifier7_2StandardScaler0_1VarianceThreshold9_6PolynomialFeatures10_1LinearSVC11MajorityVoting4_8_12": {
    "id": "PowerTransformer0mutual_info_classifKbest1PCA_ARPACK2GradientBoostingClassifier3QuantileTransformer0f_classifFPR5_2TruncatedSVD6_3BalancedRandomForestClassifier7_2StandardScaler0_1VarianceThreshold9_6PolynomialFeatures10_1LinearSVC11MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "PowerTransformer0VarianceThreshold1InteractionFeatures2GaussianProcessClassifierPrim3Normalizer0VarianceThreshold5_1FastICA6GaussianProcessClassifierPrim7_3QuantileTransformer0_1f_classifFPR9FastICA10_2BalancedRandomForestClassifier11_8MajorityVoting4_8_12": {
    "id": "PowerTransformer0VarianceThreshold1InteractionFeatures2GaussianProcessClassifierPrim3Normalizer0VarianceThreshold5_1FastICA6GaussianProcessClassifierPrim7_3QuantileTransformer0_1f_classifFPR9FastICA10_2BalancedRandomForestClassifier11_8MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          8
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "Normalizer0f_classifFWE1PCA_ARPACK2PassiveAggressiveClassifier3RobustScaler0_1f_classifFDR5_2KernelPCA6ExtraTreesClassifier7MinMaxScaler0_5UnivariateSelectChiPercentile9PCA_ARPACK10EasyEnsembleClassifier11_7MajorityVoting4_8_12": {
    "id": "Normalizer0f_classifFWE1PCA_ARPACK2PassiveAggressiveClassifier3RobustScaler0_1f_classifFDR5_2KernelPCA6ExtraTreesClassifier7MinMaxScaler0_5UnivariateSelectChiPercentile9PCA_ARPACK10EasyEnsembleClassifier11_7MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "PowerTransformer0f_classifPercentile1PolynomialFeatures2LogisticRegression3RobustScaler0VarianceThreshold5_2TruncatedSVD6_2AdaBoostClassifier7_1MinMaxScaler0_5mutual_info_classifKbest9_6TruncatedSVD10_3EasyEnsembleClassifier11RandomForestMeta4_8_12": {
    "id": "PowerTransformer0f_classifPercentile1PolynomialFeatures2LogisticRegression3RobustScaler0VarianceThreshold5_2TruncatedSVD6_2AdaBoostClassifier7_1MinMaxScaler0_5mutual_info_classifKbest9_6TruncatedSVD10_3EasyEnsembleClassifier11RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0VarianceThreshold1PolynomialFeatures2LogisticRegression3RobustScaler0VarianceThreshold5_2TruncatedSVD6_2DecisionTreeClassifier7_3PowerTransformer0_1RFE_RandomForest9_5IncrementalPCA10SGDClassifier11RandomForestMeta4_8_12": {
    "id": "Normalizer0VarianceThreshold1PolynomialFeatures2LogisticRegression3RobustScaler0VarianceThreshold5_2TruncatedSVD6_2DecisionTreeClassifier7_3PowerTransformer0_1RFE_RandomForest9_5IncrementalPCA10SGDClassifier11RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "UnivariateSelectChiKbest0RandomTreesEmbedding1RidgeClassifier2RobustScaler0KernelPCA4_2XGBClassifier5_2KBinsDiscretizerOrdinal0_4RFE_RandomForest7_4PolynomialFeatures8_1DecisionTreeClassifier9_1RandomForestMeta3_6_10": {
    "id": "UnivariateSelectChiKbest0RandomTreesEmbedding1RidgeClassifier2RobustScaler0KernelPCA4_2XGBClassifier5_2KBinsDiscretizerOrdinal0_4RFE_RandomForest7_4PolynomialFeatures8_1DecisionTreeClassifier9_1RandomForestMeta3_6_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          3,
          6,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0mutual_info_classifKbest1IncrementalPCA2KNeighborsClassifierPrim3MaxAbsScaler0mutual_info_classifPercentile5PCA_LAPACK6_2BalancedRandomForestClassifier7_1PowerTransformer0_1f_classifPercentile9PCA_LAPACK10ExtraTreesClassifier11_8MajorityVoting4_8_12": {
    "id": "RobustScaler0mutual_info_classifKbest1IncrementalPCA2KNeighborsClassifierPrim3MaxAbsScaler0mutual_info_classifPercentile5PCA_LAPACK6_2BalancedRandomForestClassifier7_1PowerTransformer0_1f_classifPercentile9PCA_LAPACK10ExtraTreesClassifier11_8MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          8
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "Normalizer0VarianceThreshold1PCA_ARPACK2GaussianProcessClassifierPrim3StandardScaler0_1f_classifFWE5_2KernelPCA6PassiveAggressiveClassifier7_4MaxAbsScaler0_5f_classifFDR9_1PCA_ARPACK10_3RidgeClassifier11_7MajorityVoting4_8_12": {
    "id": "Normalizer0VarianceThreshold1PCA_ARPACK2GaussianProcessClassifierPrim3StandardScaler0_1f_classifFWE5_2KernelPCA6PassiveAggressiveClassifier7_4MaxAbsScaler0_5f_classifFDR9_1PCA_ARPACK10_3RidgeClassifier11_7MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0VarianceThreshold1KernelPCA2SVC3StandardScaler0_1mutual_info_classifPercentile5KernelPCA6_2RidgeClassifierCV7_1MinMaxScaler0_1PCA_Randomized9_6GradientBoostingClassifier10_1MajorityVoting4_8_11": {
    "id": "KBinsDiscretizerOneHot0VarianceThreshold1KernelPCA2SVC3StandardScaler0_1mutual_info_classifPercentile5KernelPCA6_2RidgeClassifierCV7_1MinMaxScaler0_1PCA_Randomized9_6GradientBoostingClassifier10_1MajorityVoting4_8_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          8,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "StandardScaler0mutual_info_classifPercentile1PCA_Randomized2LogisticRegression3StandardScaler0RFE_GradientBoosting5_1PCA_LAPACK6_2GradientBoostingClassifier7f_classifPercentile0_2FastICA9_5RidgeClassifier10_7MajorityVoting4_8_11": {
    "id": "StandardScaler0mutual_info_classifPercentile1PCA_Randomized2LogisticRegression3StandardScaler0RFE_GradientBoosting5_1PCA_LAPACK6_2GradientBoostingClassifier7f_classifPercentile0_2FastICA9_5RidgeClassifier10_7MajorityVoting4_8_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          8,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "f_classifFPR0FastICA1RF_classifier2KBinsDiscretizerOneHot0f_classifFWE4PCA_LAPACK5_2ExtraTreesClassifier6_1StandardScaler0mutual_info_classifKbest8_4PCA_LAPACK9GaussianNBClassifier10_7MajorityVoting3_7_11": {
    "id": "f_classifFPR0FastICA1RF_classifier2KBinsDiscretizerOneHot0f_classifFWE4PCA_LAPACK5_2ExtraTreesClassifier6_1StandardScaler0mutual_info_classifKbest8_4PCA_LAPACK9GaussianNBClassifier10_7MajorityVoting3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "RobustScaler0PCA_ARPACK1RidgeClassifier2RobustScaler0_1RFE_RandomForest4_1PolynomialFeatures5RF_classifier6PowerTransformer0_1RFE_GradientBoosting8_4IncrementalPCA9_5BalancedRandomForestClassifier10_2RandomForestMeta3_7_11": {
    "id": "RobustScaler0PCA_ARPACK1RidgeClassifier2RobustScaler0_1RFE_RandomForest4_1PolynomialFeatures5RF_classifier6PowerTransformer0_1RFE_GradientBoosting8_4IncrementalPCA9_5BalancedRandomForestClassifier10_2RandomForestMeta3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOrdinal0VarianceThreshold1TruncatedSVD2LinearSVC3Normalizer0RFE_GradientBoosting5_1PCA_Randomized6LinearDiscriminantAnalysisPrim7_2MinMaxScaler0_5UnivariateSelectChiFPR9_2InteractionFeatures10_6MultinomialNB11_4MajorityVoting4_8_12": {
    "id": "KBinsDiscretizerOrdinal0VarianceThreshold1TruncatedSVD2LinearSVC3Normalizer0RFE_GradientBoosting5_1PCA_Randomized6LinearDiscriminantAnalysisPrim7_2MinMaxScaler0_5UnivariateSelectChiFPR9_2InteractionFeatures10_6MultinomialNB11_4MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          4
        ],
        "primitive": {
          "name": "MultinomialNB",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multinomial models. The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "RobustScaler0mutual_info_classifPercentile1InteractionFeatures2RidgeClassifier3PowerTransformer0f_classifFPR5_2KernelPCA6_3RidgeClassifierCV7_3RobustScaler0RFE_RandomForest9_2InteractionFeatures10_5GaussianNBClassifier11_3MajorityVoting4_8_12": {
    "id": "RobustScaler0mutual_info_classifPercentile1InteractionFeatures2RidgeClassifier3PowerTransformer0f_classifFPR5_2KernelPCA6_3RidgeClassifierCV7_3RobustScaler0RFE_RandomForest9_2InteractionFeatures10_5GaussianNBClassifier11_3MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          3
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0RFE_RandomForest1TruncatedSVD2SGDClassifier3RobustScaler0f_classifKbest5_1IncrementalPCA6_1QuadraticDiscriminantAnalysis7_3KBinsDiscretizerOrdinal0_1mutual_info_classifKbest9_6PolynomialFeatures10LogisticRegressionCV11_5RandomForestMeta4_8_12": {
    "id": "MaxAbsScaler0RFE_RandomForest1TruncatedSVD2SGDClassifier3RobustScaler0f_classifKbest5_1IncrementalPCA6_1QuadraticDiscriminantAnalysis7_3KBinsDiscretizerOrdinal0_1mutual_info_classifKbest9_6PolynomialFeatures10LogisticRegressionCV11_5RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          5
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "f_classifFPR0IncrementalPCA1GaussianNBClassifier2Normalizer0f_classifFPR4TruncatedSVD5_2AdaBoostClassifier6_1Normalizer0UnivariateSelectChiPercentile8_1InteractionFeatures9_5RidgeClassifierCV10_1RandomForestMeta3_7_11": {
    "id": "f_classifFPR0IncrementalPCA1GaussianNBClassifier2Normalizer0f_classifFPR4TruncatedSVD5_2AdaBoostClassifier6_1Normalizer0UnivariateSelectChiPercentile8_1InteractionFeatures9_5RidgeClassifierCV10_1RandomForestMeta3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0RFE_GradientBoosting1PolynomialFeatures2SVC3MinMaxScaler0f_classifFPR5IncrementalPCA6_3KNeighborsClassifierPrim7_3MinMaxScaler0_5UnivariateSelectChiFPR9_6InteractionFeatures10AdaBoostClassifier11_2MajorityVoting4_8_12": {
    "id": "RobustScaler0RFE_GradientBoosting1PolynomialFeatures2SVC3MinMaxScaler0f_classifFPR5IncrementalPCA6_3KNeighborsClassifierPrim7_3MinMaxScaler0_5UnivariateSelectChiFPR9_6InteractionFeatures10AdaBoostClassifier11_2MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "MinMaxScaler0UnivariateSelectChiFDR1IncrementalPCA2RidgeClassifier3StandardScaler0f_classifKbest5KernelPCA6RF_classifier7_4Normalizer0f_classifFWE9FastICA10_6RidgeClassifierCV11_1RandomForestMeta4_8_12": {
    "id": "MinMaxScaler0UnivariateSelectChiFDR1IncrementalPCA2RidgeClassifier3StandardScaler0f_classifKbest5KernelPCA6RF_classifier7_4Normalizer0f_classifFWE9FastICA10_6RidgeClassifierCV11_1RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with Chi-square. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0PCA_LAPACK1BernoulliNBClassifier2Normalizer0_1f_classifPercentile4_1RandomTreesEmbedding5BalancedRandomForestClassifier6_1MaxAbsScaler0_4mutual_info_classifPercentile8_5InteractionFeatures9DecisionTreeClassifier10_7RandomForestMeta3_7_11": {
    "id": "RobustScaler0PCA_LAPACK1BernoulliNBClassifier2Normalizer0_1f_classifPercentile4_1RandomTreesEmbedding5BalancedRandomForestClassifier6_1MaxAbsScaler0_4mutual_info_classifPercentile8_5InteractionFeatures9DecisionTreeClassifier10_7RandomForestMeta3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          5
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0RFE_RandomForest1TruncatedSVD2SGDClassifier3StandardScaler0f_classifFWE5_2PCA_ARPACK6_2BernoulliNBClassifier7MinMaxScaler0_1VarianceThreshold9_6RandomTreesEmbedding10_2KNeighborsClassifierPrim11_2RandomForestMeta4_8_12": {
    "id": "MaxAbsScaler0RFE_RandomForest1TruncatedSVD2SGDClassifier3StandardScaler0f_classifFWE5_2PCA_ARPACK6_2BernoulliNBClassifier7MinMaxScaler0_1VarianceThreshold9_6RandomTreesEmbedding10_2KNeighborsClassifierPrim11_2RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          2
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0VarianceThreshold1IncrementalPCA2PassiveAggressiveClassifier3QuantileTransformer0_1RFE_GradientBoosting5_2PCA_ARPACK6_1LGBMClassifier7PowerTransformer0_5f_classifFPR9_1PCA_ARPACK10_2GaussianProcessClassifierPrim11_4RandomForestMeta4_8_12": {
    "id": "StandardScaler0VarianceThreshold1IncrementalPCA2PassiveAggressiveClassifier3QuantileTransformer0_1RFE_GradientBoosting5_2PCA_ARPACK6_1LGBMClassifier7PowerTransformer0_5f_classifFPR9_1PCA_ARPACK10_2GaussianProcessClassifierPrim11_4RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          4
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0RFE_RandomForest1KernelPCA2QuadraticDiscriminantAnalysis3QuantileTransformer0RFE_RandomForest5_2InteractionFeatures6_2AdaBoostClassifier7_3Normalizer0_1mutual_info_classifPercentile9_6KernelPCA10_6AdaBoostClassifier11_7RandomForestMeta4_8_12": {
    "id": "Normalizer0RFE_RandomForest1KernelPCA2QuadraticDiscriminantAnalysis3QuantileTransformer0RFE_RandomForest5_2InteractionFeatures6_2AdaBoostClassifier7_3Normalizer0_1mutual_info_classifPercentile9_6KernelPCA10_6AdaBoostClassifier11_7RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifPercentile1IncrementalPCA2LogisticRegression3RobustScaler0VarianceThreshold5_2TruncatedSVD6_2AdaBoostClassifier7_1PowerTransformer0mutual_info_classifKbest9_5PCA_Randomized10_2NearestCentroid11_4RandomForestMeta4_8_12": {
    "id": "MaxAbsScaler0f_classifPercentile1IncrementalPCA2LogisticRegression3RobustScaler0VarianceThreshold5_2TruncatedSVD6_2AdaBoostClassifier7_1PowerTransformer0mutual_info_classifKbest9_5PCA_Randomized10_2NearestCentroid11_4RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          4
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0f_classifFWE1RandomTreesEmbedding2LogisticRegressionCV3StandardScaler0f_classifFWE5_2PCA_LAPACK6_1AdaBoostClassifier7_2Normalizer0_5f_classifFPR9_2PCA_Randomized10_7RidgeClassifierCV11_7MajorityVoting4_8_12": {
    "id": "RobustScaler0f_classifFWE1RandomTreesEmbedding2LogisticRegressionCV3StandardScaler0f_classifFWE5_2PCA_LAPACK6_1AdaBoostClassifier7_2Normalizer0_5f_classifFPR9_2PCA_Randomized10_7RidgeClassifierCV11_7MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "StandardScaler0mutual_info_classifKbest1TruncatedSVD2BernoulliNBClassifier3StandardScaler0_1f_classifFPR5_1PCA_ARPACK6_1XGBClassifier7_2StandardScaler0_5VarianceThreshold9_5RandomTreesEmbedding10_1BernoulliNBClassifier11_6MajorityVoting4_8_12": {
    "id": "StandardScaler0mutual_info_classifKbest1TruncatedSVD2BernoulliNBClassifier3StandardScaler0_1f_classifFPR5_1PCA_ARPACK6_1XGBClassifier7_2StandardScaler0_5VarianceThreshold9_5RandomTreesEmbedding10_1BernoulliNBClassifier11_6MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "StandardScaler0mutual_info_classifPercentile1AdaBoostClassifier2QuantileTransformer0_1UnivariateSelectChiKbest4IncrementalPCA5BernoulliNBClassifier6_1QuantileTransformer0_4f_classifFPR8_5PCA_LAPACK9_2RidgeClassifier10_6RandomForestMeta3_7_11": {
    "id": "StandardScaler0mutual_info_classifPercentile1AdaBoostClassifier2QuantileTransformer0_1UnivariateSelectChiKbest4IncrementalPCA5BernoulliNBClassifier6_1QuantileTransformer0_4f_classifFPR8_5PCA_LAPACK9_2RidgeClassifier10_6RandomForestMeta3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          5
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifKbest1InteractionFeatures2ComplementNBClassifier3RobustScaler0RFE_RandomForest5FastICA6_2LinearSVC7MinMaxScaler0_1RFE_GradientBoosting9_2PCA_LAPACK10_3SGDClassifier11_2MajorityVoting4_8_12": {
    "id": "MaxAbsScaler0f_classifKbest1InteractionFeatures2ComplementNBClassifier3RobustScaler0RFE_RandomForest5FastICA6_2LinearSVC7MinMaxScaler0_1RFE_GradientBoosting9_2PCA_LAPACK10_3SGDClassifier11_2MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ComplementNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "The Complement Naive Bayes classifier described in Rennie et al. (2003). The Complement Naive Bayes classifier was designed to correct the \u201csevere assumptions\u201d made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          2
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "Normalizer0f_classifPercentile1FastICA2LogisticRegressionCV3RFE_GradientBoosting0InteractionFeatures5LinearDiscriminantAnalysisPrim6_4MinMaxScaler0mutual_info_classifKbest8_2FastICA9_2GradientBoostingClassifier10_2RandomForestMeta4_7_11": {
    "id": "Normalizer0f_classifPercentile1FastICA2LogisticRegressionCV3RFE_GradientBoosting0InteractionFeatures5LinearDiscriminantAnalysisPrim6_4MinMaxScaler0mutual_info_classifKbest8_2FastICA9_2GradientBoostingClassifier10_2RandomForestMeta4_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          4
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0f_classifFPR1PCA_ARPACK2DecisionTreeClassifier3KBinsDiscretizerOneHot0_1f_classifFPR5_2TruncatedSVD6GaussianNBClassifier7_3Normalizer0_5VarianceThreshold9_2FastICA10_5LGBMClassifier11_7RandomForestMeta4_8_12": {
    "id": "MinMaxScaler0f_classifFPR1PCA_ARPACK2DecisionTreeClassifier3KBinsDiscretizerOneHot0_1f_classifFPR5_2TruncatedSVD6GaussianNBClassifier7_3Normalizer0_5VarianceThreshold9_2FastICA10_5LGBMClassifier11_7RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0f_classifFDR1KernelPCA2EasyEnsembleClassifier3MinMaxScaler0mutual_info_classifPercentile5_2FastICA6_1RUSBoostClassifier7_2MaxAbsScaler0_5UnivariateSelectChiFWE9_1FastICA10_5AdaBoostClassifier11RandomForestMeta4_8_12": {
    "id": "MinMaxScaler0f_classifFDR1KernelPCA2EasyEnsembleClassifier3MinMaxScaler0mutual_info_classifPercentile5_2FastICA6_1RUSBoostClassifier7_2MaxAbsScaler0_5UnivariateSelectChiFWE9_1FastICA10_5AdaBoostClassifier11RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0mutual_info_classifPercentile1RandomTreesEmbedding2MultinomialNB3MinMaxScaler0_1f_classifPercentile5InteractionFeatures6RF_classifier7_4StandardScaler0f_classifKbest9_2RandomTreesEmbedding10_2ComplementNBClassifier11_3RandomForestMeta4_8_12": {
    "id": "MaxAbsScaler0mutual_info_classifPercentile1RandomTreesEmbedding2MultinomialNB3MinMaxScaler0_1f_classifPercentile5InteractionFeatures6RF_classifier7_4StandardScaler0f_classifKbest9_2RandomTreesEmbedding10_2ComplementNBClassifier11_3RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "MultinomialNB",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multinomial models. The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          3
        ],
        "primitive": {
          "name": "ComplementNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "The Complement Naive Bayes classifier described in Rennie et al. (2003). The Complement Naive Bayes classifier was designed to correct the \u201csevere assumptions\u201d made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0f_classifFPR1IncrementalPCA2GaussianProcessClassifierPrim3MaxAbsScaler0_1f_classifFDR5PolynomialFeatures6MinMaxScaler0_1VarianceThreshold8_5IncrementalPCA9_6BalancedRandomForestClassifier10_5RandomForestMeta4_11_7": {
    "id": "PowerTransformer0f_classifFPR1IncrementalPCA2GaussianProcessClassifierPrim3MaxAbsScaler0_1f_classifFDR5PolynomialFeatures6MinMaxScaler0_1VarianceThreshold8_5IncrementalPCA9_6BalancedRandomForestClassifier10_5RandomForestMeta4_11_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          5
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          11,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0RFE_GradientBoosting1IncrementalPCA2SVC3f_classifPercentile0_1PolynomialFeatures5GaussianProcessClassifierPrim6_3VarianceThreshold0_2TruncatedSVD8_5SVC9_6RandomForestMeta4_7_10": {
    "id": "MinMaxScaler0RFE_GradientBoosting1IncrementalPCA2SVC3f_classifPercentile0_1PolynomialFeatures5GaussianProcessClassifierPrim6_3VarianceThreshold0_2TruncatedSVD8_5SVC9_6RandomForestMeta4_7_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          5
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          4,
          7,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "RandomTreesEmbedding0LinearSVC1MaxAbsScaler0f_classifFPR3PCA_ARPACK4_1SVC5_1KBinsDiscretizerOrdinal0_3UnivariateSelectChiKbest7_4TruncatedSVD8_5XGBClassifier9_4MajorityVoting2_6_10": {
    "id": "RandomTreesEmbedding0LinearSVC1MaxAbsScaler0f_classifFPR3PCA_ARPACK4_1SVC5_1KBinsDiscretizerOrdinal0_3UnivariateSelectChiKbest7_4TruncatedSVD8_5XGBClassifier9_4MajorityVoting2_6_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          0,
          3
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          5
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          4
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          2,
          6,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "RobustScaler0RFE_GradientBoosting1PolynomialFeatures2SVC3KBinsDiscretizerOneHot0RFE_RandomForest5_2InteractionFeatures6KNeighborsClassifierPrim7MinMaxScaler0mutual_info_classifKbest9_1RandomTreesEmbedding10_3AdaBoostClassifier11_2MajorityVoting4_8_12": {
    "id": "RobustScaler0RFE_GradientBoosting1PolynomialFeatures2SVC3KBinsDiscretizerOneHot0RFE_RandomForest5_2InteractionFeatures6KNeighborsClassifierPrim7MinMaxScaler0mutual_info_classifKbest9_1RandomTreesEmbedding10_3AdaBoostClassifier11_2MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOrdinal0UnivariateSelectChiFPR1IncrementalPCA2RidgeClassifier3Normalizer0f_classifKbest5_1IncrementalPCA6_3LinearSVC7RobustScaler0VarianceThreshold9_5KernelPCA10GaussianProcessClassifierPrim11_8MajorityVoting4_8_12": {
    "id": "KBinsDiscretizerOrdinal0UnivariateSelectChiFPR1IncrementalPCA2RidgeClassifier3Normalizer0f_classifKbest5_1IncrementalPCA6_3LinearSVC7RobustScaler0VarianceThreshold9_5KernelPCA10GaussianProcessClassifierPrim11_8MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          8
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "Normalizer0f_classifFWE1PCA_Randomized2LogisticRegressionCV3StandardScaler0f_classifFDR5_2TruncatedSVD6_1PassiveAggressiveClassifier7_1StandardScaler0RFE_RandomForest9_6PCA_Randomized10_7LinearDiscriminantAnalysisPrim11_5MajorityVoting4_8_12": {
    "id": "Normalizer0f_classifFWE1PCA_Randomized2LogisticRegressionCV3StandardScaler0f_classifFDR5_2TruncatedSVD6_1PassiveAggressiveClassifier7_1StandardScaler0RFE_RandomForest9_6PCA_Randomized10_7LinearDiscriminantAnalysisPrim11_5MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          5
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0VarianceThreshold1PCA_LAPACK2XGBClassifier3MaxAbsScaler0_1RFE_RandomForest5_2InteractionFeatures6_1LogisticRegressionCV7_4MinMaxScaler0_1mutual_info_classifKbest9_5IncrementalPCA10_7BalancedRandomForestClassifier11_6MajorityVoting4_8_12": {
    "id": "MaxAbsScaler0VarianceThreshold1PCA_LAPACK2XGBClassifier3MaxAbsScaler0_1RFE_RandomForest5_2InteractionFeatures6_1LogisticRegressionCV7_4MinMaxScaler0_1mutual_info_classifKbest9_5IncrementalPCA10_7BalancedRandomForestClassifier11_6MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "MinMaxScaler0f_classifKbest1RandomTreesEmbedding2LinearSVC3QuantileTransformer0_1VarianceThreshold5_2TruncatedSVD6_1RUSBoostClassifier7_3MinMaxScaler0_5f_classifKbest9_1PCA_Randomized10BaggingClassifier11_7MajorityVoting4_8_12": {
    "id": "MinMaxScaler0f_classifKbest1RandomTreesEmbedding2LinearSVC3QuantileTransformer0_1VarianceThreshold5_2TruncatedSVD6_1RUSBoostClassifier7_3MinMaxScaler0_5f_classifKbest9_1PCA_Randomized10BaggingClassifier11_7MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1IncrementalPCA2DecisionTreeClassifier3QuantileTransformer0_1VarianceThreshold5RandomTreesEmbedding6_1BernoulliNBClassifier7_3f_classifKbest0KernelPCA9_5GaussianProcessClassifierPrim10_3RandomForestMeta4_8_11": {
    "id": "StandardScaler0f_classifFWE1IncrementalPCA2DecisionTreeClassifier3QuantileTransformer0_1VarianceThreshold5RandomTreesEmbedding6_1BernoulliNBClassifier7_3f_classifKbest0KernelPCA9_5GaussianProcessClassifierPrim10_3RandomForestMeta4_8_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          8,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "f_classifFWE0IncrementalPCA1RF_classifier2MaxAbsScaler0f_classifKbest4_1PCA_LAPACK5PassiveAggressiveClassifier6_1PowerTransformer0VarianceThreshold8_1PolynomialFeatures9_5NearestCentroid10_5MajorityVoting3_7_11": {
    "id": "f_classifFWE0IncrementalPCA1RF_classifier2MaxAbsScaler0f_classifKbest4_1PCA_LAPACK5PassiveAggressiveClassifier6_1PowerTransformer0VarianceThreshold8_1PolynomialFeatures9_5NearestCentroid10_5MajorityVoting3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiFWE1PCA_ARPACK2SGDClassifier3Normalizer0f_classifFPR5PolynomialFeatures6_1QuadraticDiscriminantAnalysis7_2Normalizer0_5f_classifPercentile9_2KernelPCA10_5GradientBoostingClassifier11_5MajorityVoting4_8_12": {
    "id": "MaxAbsScaler0UnivariateSelectChiFWE1PCA_ARPACK2SGDClassifier3Normalizer0f_classifFPR5PolynomialFeatures6_1QuadraticDiscriminantAnalysis7_2Normalizer0_5f_classifPercentile9_2KernelPCA10_5GradientBoostingClassifier11_5MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          5
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFPR1PCA_ARPACK2BalancedRandomForestClassifier3MaxAbsScaler0_1PCA_Randomized5_1LogisticRegression6_1MinMaxScaler0_5mutual_info_classifPercentile8_1RandomTreesEmbedding9_1RF_classifier10_1MajorityVoting4_7_11": {
    "id": "RobustScaler0f_classifFPR1PCA_ARPACK2BalancedRandomForestClassifier3MaxAbsScaler0_1PCA_Randomized5_1LogisticRegression6_1MinMaxScaler0_5mutual_info_classifPercentile8_1RandomTreesEmbedding9_1RF_classifier10_1MajorityVoting4_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "Normalizer0f_classifFWE1IncrementalPCA2LogisticRegression3RobustScaler0VarianceThreshold5_2TruncatedSVD6_2RUSBoostClassifier7_4PowerTransformer0RFE_RandomForest9_2PCA_LAPACK10_5LogisticRegressionCV11_1MajorityVoting4_8_12": {
    "id": "Normalizer0f_classifFWE1IncrementalPCA2LogisticRegression3RobustScaler0VarianceThreshold5_2TruncatedSVD6_2RUSBoostClassifier7_4PowerTransformer0RFE_RandomForest9_2PCA_LAPACK10_5LogisticRegressionCV11_1MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiFPR1PCA_ARPACK2LinearDiscriminantAnalysisPrim3MinMaxScaler0_1f_classifKbest5TruncatedSVD6LinearSVC7_3MaxAbsScaler0_1VarianceThreshold9_1IncrementalPCA10_1SVC11_1RandomForestMeta4_8_12": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiFPR1PCA_ARPACK2LinearDiscriminantAnalysisPrim3MinMaxScaler0_1f_classifKbest5TruncatedSVD6LinearSVC7_3MaxAbsScaler0_1VarianceThreshold9_1IncrementalPCA10_1SVC11_1RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0PCA_Randomized1MaxAbsScaler0_1f_classifFDR3TruncatedSVD4_2AdaBoostClassifier5KBinsDiscretizerOneHot0RFE_GradientBoosting7_3KernelPCA8SGDClassifier9_5MajorityVoting6_10": {
    "id": "RobustScaler0PCA_Randomized1MaxAbsScaler0_1f_classifFDR3TruncatedSVD4_2AdaBoostClassifier5KBinsDiscretizerOneHot0RFE_GradientBoosting7_3KernelPCA8SGDClassifier9_5MajorityVoting6_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          6,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "QuantileTransformer0f_classifPercentile1RandomTreesEmbedding2LinearDiscriminantAnalysisPrim3KBinsDiscretizerOrdinal0UnivariateSelectChiPercentile5_2PCA_LAPACK6_1LinearDiscriminantAnalysisPrim7_1MinMaxScaler0_1f_classifPercentile9PolynomialFeatures10NearestCentroid11_4MajorityVoting4_8_12": {
    "id": "QuantileTransformer0f_classifPercentile1RandomTreesEmbedding2LinearDiscriminantAnalysisPrim3KBinsDiscretizerOrdinal0UnivariateSelectChiPercentile5_2PCA_LAPACK6_1LinearDiscriminantAnalysisPrim7_1MinMaxScaler0_1f_classifPercentile9PolynomialFeatures10NearestCentroid11_4MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          4
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "QuantileTransformer0f_classifFWE1FastICA2EasyEnsembleClassifier3MaxAbsScaler0f_classifFDR5_1RandomTreesEmbedding6_3PowerTransformer0f_classifFWE8InteractionFeatures9_5NearestCentroid10_7MajorityVoting4_11": {
    "id": "QuantileTransformer0f_classifFWE1FastICA2EasyEnsembleClassifier3MaxAbsScaler0f_classifFDR5_1RandomTreesEmbedding6_3PowerTransformer0f_classifFWE8InteractionFeatures9_5NearestCentroid10_7MajorityVoting4_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "PowerTransformer0RFE_GradientBoosting1QuadraticDiscriminantAnalysis2RobustScaler0mutual_info_classifPercentile4PCA_Randomized5PassiveAggressiveClassifier6_2MaxAbsScaler0_4f_classifFWE8_5TruncatedSVD9_2LogisticRegressionCV10_4RandomForestMeta3_7_11": {
    "id": "PowerTransformer0RFE_GradientBoosting1QuadraticDiscriminantAnalysis2RobustScaler0mutual_info_classifPercentile4PCA_Randomized5PassiveAggressiveClassifier6_2MaxAbsScaler0_4f_classifFWE8_5TruncatedSVD9_2LogisticRegressionCV10_4RandomForestMeta3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          5
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          4
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "mutual_info_classifPercentile0InteractionFeatures1AdaBoostClassifier2MaxAbsScaler0f_classifPercentile4TruncatedSVD5_2GaussianProcessClassifierPrim6QuantileTransformer0_4f_classifFPR8IncrementalPCA9_2LGBMClassifier10_6MajorityVoting3_7_11": {
    "id": "mutual_info_classifPercentile0InteractionFeatures1AdaBoostClassifier2MaxAbsScaler0f_classifPercentile4TruncatedSVD5_2GaussianProcessClassifierPrim6QuantileTransformer0_4f_classifFPR8IncrementalPCA9_2LGBMClassifier10_6MajorityVoting3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "PowerTransformer0mutual_info_classifKbest1PCA_Randomized2ExtraTreesClassifier3StandardScaler0RFE_RandomForest5_1FastICA6_1BalancedRandomForestClassifier7MaxAbsScaler0_1mutual_info_classifKbest9_2IncrementalPCA10_2KNeighborsClassifierPrim11_3MajorityVoting4_8_12": {
    "id": "PowerTransformer0mutual_info_classifKbest1PCA_Randomized2ExtraTreesClassifier3StandardScaler0RFE_RandomForest5_1FastICA6_1BalancedRandomForestClassifier7MaxAbsScaler0_1mutual_info_classifKbest9_2IncrementalPCA10_2KNeighborsClassifierPrim11_3MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFDR1KernelPCA2SVC3QuantileTransformer0UnivariateSelectChiFWE5PCA_LAPACK6_2GaussianNBClassifier7QuantileTransformer0f_classifFWE9_5TruncatedSVD10_2BalancedRandomForestClassifier11_4MajorityVoting4_8_12": {
    "id": "RobustScaler0f_classifFDR1KernelPCA2SVC3QuantileTransformer0UnivariateSelectChiFWE5PCA_LAPACK6_2GaussianNBClassifier7QuantileTransformer0f_classifFWE9_5TruncatedSVD10_2BalancedRandomForestClassifier11_4MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          4
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "mutual_info_classifKbest0IncrementalPCA1DecisionTreeClassifier2MaxAbsScaler0f_classifPercentile4TruncatedSVD5_1ExtraTreesClassifier6_1PowerTransformer0_4f_classifPercentile8_5InteractionFeatures9BernoulliNBClassifier10_4MajorityVoting3_7_11": {
    "id": "mutual_info_classifKbest0IncrementalPCA1DecisionTreeClassifier2MaxAbsScaler0f_classifPercentile4TruncatedSVD5_1ExtraTreesClassifier6_1PowerTransformer0_4f_classifPercentile8_5InteractionFeatures9BernoulliNBClassifier10_4MajorityVoting3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          5
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          4
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "PowerTransformer0f_classifKbest1PCA_Randomized2GradientBoostingClassifier3RobustScaler0f_classifFPR5_1PCA_LAPACK6_1BaggingClassifier7_4MinMaxScaler0_5f_classifFWE9_6PassiveAggressiveClassifier10_5MajorityVoting4_8_11": {
    "id": "PowerTransformer0f_classifKbest1PCA_Randomized2GradientBoostingClassifier3RobustScaler0f_classifFPR5_1PCA_LAPACK6_1BaggingClassifier7_4MinMaxScaler0_5f_classifFWE9_6PassiveAggressiveClassifier10_5MajorityVoting4_8_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          8,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "MinMaxScaler0UnivariateSelectChiFWE1PolynomialFeatures2BalancedRandomForestClassifier3MinMaxScaler0f_classifPercentile5_1PCA_Randomized6_3EasyEnsembleClassifier7_4MaxAbsScaler0VarianceThreshold9_6TruncatedSVD10_1LogisticRegressionCV11_1MajorityVoting4_8_12": {
    "id": "MinMaxScaler0UnivariateSelectChiFWE1PolynomialFeatures2BalancedRandomForestClassifier3MinMaxScaler0f_classifPercentile5_1PCA_Randomized6_3EasyEnsembleClassifier7_4MaxAbsScaler0VarianceThreshold9_6TruncatedSVD10_1LogisticRegressionCV11_1MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "PowerTransformer0RFE_GradientBoosting1PolynomialFeatures2LogisticRegression3KBinsDiscretizerOneHot0_1mutual_info_classifKbest5_1PCA_Randomized6_3KNeighborsClassifierPrim7_4RobustScaler0_5RFE_GradientBoosting9_5TruncatedSVD10_1GaussianNBClassifier11RandomForestMeta4_8_12": {
    "id": "PowerTransformer0RFE_GradientBoosting1PolynomialFeatures2LogisticRegression3KBinsDiscretizerOneHot0_1mutual_info_classifKbest5_1PCA_Randomized6_3KNeighborsClassifierPrim7_4RobustScaler0_5RFE_GradientBoosting9_5TruncatedSVD10_1GaussianNBClassifier11RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PolynomialFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate polynomial and interaction features. Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFDR1LogisticRegression2QuantileTransformer0f_classifKbest4_2InteractionFeatures5_2LinearDiscriminantAnalysisPrim6_2PowerTransformer0_4f_classifPercentile8_2InteractionFeatures9_1LogisticRegression10_2MajorityVoting3_7_11": {
    "id": "StandardScaler0f_classifFDR1LogisticRegression2QuantileTransformer0f_classifKbest4_2InteractionFeatures5_2LinearDiscriminantAnalysisPrim6_2PowerTransformer0_4f_classifPercentile8_2InteractionFeatures9_1LogisticRegression10_2MajorityVoting3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "InteractionFeatures",
          "hyperparams_run": {
            "default": true
          },
          "description": "Generate interaction features.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "PowerTransformer0f_classifFWE1PCA_ARPACK2KNeighborsClassifierPrim3KBinsDiscretizerOrdinal0mutual_info_classifKbest5_1TruncatedSVD6_3BalancedRandomForestClassifier7_1QuantileTransformer0_5f_classifFPR9_5PCA_LAPACK10_5LinearSVC11_6RandomForestMeta4_8_12": {
    "id": "PowerTransformer0f_classifFWE1PCA_ARPACK2KNeighborsClassifierPrim3KBinsDiscretizerOrdinal0mutual_info_classifKbest5_1TruncatedSVD6_3BalancedRandomForestClassifier7_1QuantileTransformer0_5f_classifFPR9_5PCA_LAPACK10_5LinearSVC11_6RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "f_classifPercentile0PCA_ARPACK1LogisticRegression2Normalizer0f_classifFWE4KernelPCA5_1AdaBoostClassifier6_2RFE_RandomForest0_4RandomTreesEmbedding8_4GradientBoostingClassifier9_4MajorityVoting3_7_10": {
    "id": "f_classifPercentile0PCA_ARPACK1LogisticRegression2Normalizer0f_classifFWE4KernelPCA5_1AdaBoostClassifier6_2RFE_RandomForest0_4RandomTreesEmbedding8_4GradientBoostingClassifier9_4MajorityVoting3_7_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          4
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          3,
          7,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "Ensemble"
        }
      }
    }
  },
  "Normalizer0RFE_RandomForest1PCA_ARPACK2RF_classifier3": {
    "id": "Normalizer0RFE_RandomForest1PCA_ARPACK2RF_classifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "OneHotEncoder0StandardScaler1f_classifFDR2KernelPCA3BalancedRandomForestClassifier4": {
    "id": "OneHotEncoder0StandardScaler1f_classifFDR2KernelPCA3BalancedRandomForestClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      }
    }
  },
  "LabelEncoder0RobustScaler1f_classifFPR2IncrementalPCA3LinearDiscriminantAnalysisPrim4": {
    "id": "LabelEncoder0RobustScaler1f_classifFPR2IncrementalPCA3LinearDiscriminantAnalysisPrim4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputer0Normalizer1VarianceThreshold2PCA_LAPACK3LinearDiscriminantAnalysisPrim4": {
    "id": "imputer0Normalizer1VarianceThreshold2PCA_LAPACK3LinearDiscriminantAnalysisPrim4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0QuantileTransformer1UnivariateSelectChiKbest2PCA_Randomized3AdaBoostClassifier4OneHotEncoder0MaxAbsScaler6_1f_classifKbest7_2RandomTreesEmbedding8_2ExtraTreesClassifier9_2RandomForestMeta5_10": {
    "id": "ImputerMedian0QuantileTransformer1UnivariateSelectChiKbest2PCA_Randomized3AdaBoostClassifier4OneHotEncoder0MaxAbsScaler6_1f_classifKbest7_2RandomTreesEmbedding8_2ExtraTreesClassifier9_2RandomForestMeta5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0f_classifFPR1XGBClassifier2StandardScaler0_1VarianceThreshold4_2PCA_ARPACK5BalancedRandomForestClassifier6_2Normalizer0_1f_classifFDR8_5PCA_LAPACK9_1KNeighborsClassifierPrim10_4RandomForestMeta3_7_11": {
    "id": "QuantileTransformer0f_classifFPR1XGBClassifier2StandardScaler0_1VarianceThreshold4_2PCA_ARPACK5BalancedRandomForestClassifier6_2Normalizer0_1f_classifFDR8_5PCA_LAPACK9_1KNeighborsClassifierPrim10_4RandomForestMeta3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          5
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          4
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0StandardScaler1RFE_RandomForest2SVC3": {
    "id": "ImputerEncoderPrim0StandardScaler1RFE_RandomForest2SVC3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0f_classifFDR1PCA_ARPACK2SGDClassifier3RobustScaler0f_classifFDR5_2KernelPCA6_2LGBMClassifier7MajorityVoting4_8": {
    "id": "PowerTransformer0f_classifFDR1PCA_ARPACK2SGDClassifier3RobustScaler0f_classifFDR5_2KernelPCA6_2LGBMClassifier7MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0mutual_info_classifPercentile1PCA_LAPACK2DecisionTreeClassifier3": {
    "id": "MinMaxScaler0mutual_info_classifPercentile1PCA_LAPACK2DecisionTreeClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      }
    }
  },
  "LabelEncoder0MaxAbsScaler1VarianceThreshold2PCA_Randomized3KNeighborsClassifierPrim4imputerIndicator0_1Normalizer6_1f_classifFPR7_3PCA_Randomized8_1RUSBoostClassifier9_4imputerIndicator0_6QuantileTransformer11f_classifKbest12_2TruncatedSVD13SVC14_5MajorityVoting5_10_15": {
    "id": "LabelEncoder0MaxAbsScaler1VarianceThreshold2PCA_Randomized3KNeighborsClassifierPrim4imputerIndicator0_1Normalizer6_1f_classifFPR7_3PCA_Randomized8_1RUSBoostClassifier9_4imputerIndicator0_6QuantileTransformer11f_classifKbest12_2TruncatedSVD13SVC14_5MajorityVoting5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          4
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0,
          6
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          5
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFWE1FastICA2BernoulliNBClassifier3Normalizer0_1RFE_RandomForest5_1TruncatedSVD6_2RidgeClassifier7RobustScaler0_5f_classifKbest9_5PCA_Randomized10_3PassiveAggressiveClassifier11_5RandomForestMeta4_8_12": {
    "id": "RobustScaler0f_classifFWE1FastICA2BernoulliNBClassifier3Normalizer0_1RFE_RandomForest5_1TruncatedSVD6_2RidgeClassifier7RobustScaler0_5f_classifKbest9_5PCA_Randomized10_3PassiveAggressiveClassifier11_5RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          5
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0f_classifPercentile1AdaBoostClassifier2": {
    "id": "MinMaxScaler0f_classifPercentile1AdaBoostClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      }
    }
  },
  "LabelEncoder0PowerTransformer1mutual_info_classifPercentile2PCA_Randomized3RidgeClassifier4imputerIndicator0_1Normalizer6_1f_classifKbest7_1FastICA8_3LinearSVC9_1RandomForestMeta5_10": {
    "id": "LabelEncoder0PowerTransformer1mutual_info_classifPercentile2PCA_Randomized3RidgeClassifier4imputerIndicator0_1Normalizer6_1f_classifKbest7_1FastICA8_3LinearSVC9_1RandomForestMeta5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "NumericData0XGBClassifier1": {
    "id": "NumericData0XGBClassifier1",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "NumericData",
          "hyperparams_run": {
            "default": true
          },
          "description": "Extracts only numeric data columns from input.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0f_classifFPR1IncrementalPCA2DecisionTreeClassifier3": {
    "id": "MinMaxScaler0f_classifFPR1IncrementalPCA2DecisionTreeClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0mutual_info_classifPercentile1PCA_LAPACK2QuadraticDiscriminantAnalysis3StandardScaler0_1mutual_info_classifPercentile5_1FastICA6_2KNeighborsClassifierPrim7StandardScaler0_1f_classifKbest9PCA_LAPACK10_7XGBClassifier11_7MajorityVoting4_8_12": {
    "id": "MinMaxScaler0mutual_info_classifPercentile1PCA_LAPACK2QuadraticDiscriminantAnalysis3StandardScaler0_1mutual_info_classifPercentile5_1FastICA6_2KNeighborsClassifierPrim7StandardScaler0_1f_classifKbest9PCA_LAPACK10_7XGBClassifier11_7MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "LabelEncoder0QuantileTransformer1UnivariateSelectChiFPR2SVC3imputerIndicator0_1QuantileTransformer5_2KernelPCA6RUSBoostClassifier7_3MajorityVoting4_8": {
    "id": "LabelEncoder0QuantileTransformer1UnivariateSelectChiFPR2SVC3imputerIndicator0_1QuantileTransformer5_2KernelPCA6RUSBoostClassifier7_3MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerMedian0Normalizer1RFE_RandomForest2KernelPCA3GaussianProcessClassifierPrim4": {
    "id": "ImputerMedian0Normalizer1RFE_RandomForest2KernelPCA3GaussianProcessClassifierPrim4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0QuantileTransformer1mutual_info_classifPercentile2TruncatedSVD3LogisticRegressionCV4": {
    "id": "ImputerEncoderPrim0QuantileTransformer1mutual_info_classifPercentile2TruncatedSVD3LogisticRegressionCV4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      }
    }
  },
  "LabelEncoder0RobustScaler1mutual_info_classifPercentile2IncrementalPCA3ExtraTreesClassifier4ImputerEncoderPrim0RobustScaler6_2RFE_RandomForest7_2PCA_Randomized8_1LinearSVC9_3imputerIndicator0_6QuantileTransformer11_1VarianceThreshold12_2KernelPCA13PassiveAggressiveClassifier14_9MajorityVoting5_10_15": {
    "id": "LabelEncoder0RobustScaler1mutual_info_classifPercentile2IncrementalPCA3ExtraTreesClassifier4ImputerEncoderPrim0RobustScaler6_2RFE_RandomForest7_2PCA_Randomized8_1LinearSVC9_3imputerIndicator0_6QuantileTransformer11_1VarianceThreshold12_2KernelPCA13PassiveAggressiveClassifier14_9MajorityVoting5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0,
          6
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          9
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerMedian0MaxAbsScaler1f_classifFPR2PCA_LAPACK3ExtraTreesClassifier4LabelEncoder0f_classifPercentile6_3TruncatedSVD7_4KNeighborsClassifierPrim8_3LabelEncoder0_1StandardScaler10_6VarianceThreshold11_6PCA_Randomized12_7BernoulliNBClassifier13RandomForestMeta5_9_14": {
    "id": "ImputerMedian0MaxAbsScaler1f_classifFPR2PCA_LAPACK3ExtraTreesClassifier4LabelEncoder0f_classifPercentile6_3TruncatedSVD7_4KNeighborsClassifierPrim8_3LabelEncoder0_1StandardScaler10_6VarianceThreshold11_6PCA_Randomized12_7BernoulliNBClassifier13RandomForestMeta5_9_14",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          7
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          5,
          9,
          14
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerMedian0MaxAbsScaler1f_classifFWE2DecisionTreeClassifier3ImputerEncoderPrim0_1Normalizer5_2f_classifKbest6_1IncrementalPCA7BernoulliNBClassifier8ImputerEncoderPrim0_1MaxAbsScaler10_6VarianceThreshold11_6PCA_Randomized12_5SVC13_2RandomForestMeta4_9_14": {
    "id": "ImputerMedian0MaxAbsScaler1f_classifFWE2DecisionTreeClassifier3ImputerEncoderPrim0_1Normalizer5_2f_classifKbest6_1IncrementalPCA7BernoulliNBClassifier8ImputerEncoderPrim0_1MaxAbsScaler10_6VarianceThreshold11_6PCA_Randomized12_5SVC13_2RandomForestMeta4_9_14",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          5
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          2
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          4,
          9,
          14
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerOneHotEncoderPrim0QuantileTransformer1VarianceThreshold2FastICA3AdaBoostClassifier4": {
    "id": "ImputerOneHotEncoderPrim0QuantileTransformer1VarianceThreshold2FastICA3AdaBoostClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerOneHotEncoderPrim0Normalizer1VarianceThreshold2IncrementalPCA3RandomForestMeta4": {
    "id": "ImputerOneHotEncoderPrim0Normalizer1VarianceThreshold2IncrementalPCA3RandomForestMeta4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "PowerTransformer0mutual_info_classifPercentile1IncrementalPCA2LGBMClassifier3": {
    "id": "PowerTransformer0mutual_info_classifPercentile1IncrementalPCA2LGBMClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0mutual_info_classifPercentile1TruncatedSVD2BalancedRandomForestClassifier3": {
    "id": "MinMaxScaler0mutual_info_classifPercentile1TruncatedSVD2BalancedRandomForestClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifFPR1IncrementalPCA2KBinsDiscretizerOneHot0UnivariateSelectChiFWE4TruncatedSVD5RUSBoostClassifier6_1": {
    "id": "KBinsDiscretizerOneHot0f_classifFPR1IncrementalPCA2KBinsDiscretizerOneHot0UnivariateSelectChiFWE4TruncatedSVD5RUSBoostClassifier6_1",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0RFE_GradientBoosting1IncrementalPCA2LinearSVC3": {
    "id": "MaxAbsScaler0RFE_GradientBoosting1IncrementalPCA2LinearSVC3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiFPR1PCA_Randomized2KNeighborsClassifierPrim3KBinsDiscretizerOrdinal0f_classifFPR5_1PCA_Randomized6_3LinearDiscriminantAnalysisPrim7_2MinMaxScaler0_1f_classifPercentile9_5FastICA10_7AdaBoostClassifier11_4RandomForestMeta4_8_12": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiFPR1PCA_Randomized2KNeighborsClassifierPrim3KBinsDiscretizerOrdinal0f_classifFPR5_1PCA_Randomized6_3LinearDiscriminantAnalysisPrim7_2MinMaxScaler0_1f_classifPercentile9_5FastICA10_7AdaBoostClassifier11_4RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          4
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "imputer0StandardScaler1RFE_RandomForest2LinearDiscriminantAnalysisPrim3": {
    "id": "imputer0StandardScaler1RFE_RandomForest2LinearDiscriminantAnalysisPrim3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0f_classifFPR1TruncatedSVD2XGBClassifier3": {
    "id": "Normalizer0f_classifFPR1TruncatedSVD2XGBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0f_classifPercentile1TruncatedSVD2RidgeClassifierCV3f_classifPercentile0_2IncrementalPCA5_3LinearDiscriminantAnalysisPrim6_3StandardScaler0mutual_info_classifKbest8_2TruncatedSVD9_2NearestCentroid10_6RandomForestMeta4_7_11": {
    "id": "RobustScaler0f_classifPercentile1TruncatedSVD2RidgeClassifierCV3f_classifPercentile0_2IncrementalPCA5_3LinearDiscriminantAnalysisPrim6_3StandardScaler0mutual_info_classifKbest8_2TruncatedSVD9_2NearestCentroid10_6RandomForestMeta4_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFWE1IncrementalPCA2LogisticRegressionCV3": {
    "id": "RobustScaler0f_classifFWE1IncrementalPCA2LogisticRegressionCV3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      }
    }
  },
  "OneHotEncoder0QuantileTransformer1UnivariateSelectChiFWE2GaussianProcessClassifierPrim3ImputerMedian0Normalizer5_1f_classifFPR6_3TruncatedSVD7_3EasyEnsembleClassifier8_3ImputerOneHotEncoderPrim0_1MaxAbsScaler10_2VarianceThreshold11_7PCA_ARPACK12_3AdaBoostClassifier13_4RandomForestMeta4_9_14": {
    "id": "OneHotEncoder0QuantileTransformer1UnivariateSelectChiFWE2GaussianProcessClassifierPrim3ImputerMedian0Normalizer5_1f_classifFPR6_3TruncatedSVD7_3EasyEnsembleClassifier8_3ImputerOneHotEncoderPrim0_1MaxAbsScaler10_2VarianceThreshold11_7PCA_ARPACK12_3AdaBoostClassifier13_4RandomForestMeta4_9_14",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          3
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          4
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          4,
          9,
          14
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "VarianceThreshold0RandomTreesEmbedding1ComplementNBClassifier2": {
    "id": "VarianceThreshold0RandomTreesEmbedding1ComplementNBClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "ComplementNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "The Complement Naive Bayes classifier described in Rennie et al. (2003). The Complement Naive Bayes classifier was designed to correct the \u201csevere assumptions\u201d made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0PowerTransformer1f_classifFPR2KNeighborsClassifierPrim3ImputerMedian0_1PowerTransformer5f_classifFPR6TruncatedSVD7_3ExtraTreesClassifier8_2ImputerEncoderPrim0MinMaxScaler10_6mutual_info_classifKbest11_6KernelPCA12SGDClassifier13_4RandomForestMeta4_9_14": {
    "id": "ImputerEncoderPrim0PowerTransformer1f_classifFPR2KNeighborsClassifierPrim3ImputerMedian0_1PowerTransformer5f_classifFPR6TruncatedSVD7_3ExtraTreesClassifier8_2ImputerEncoderPrim0MinMaxScaler10_6mutual_info_classifKbest11_6KernelPCA12SGDClassifier13_4RandomForestMeta4_9_14",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          4
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          4,
          9,
          14
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0PCA_ARPACK1BalancedRandomForestClassifier2": {
    "id": "RobustScaler0PCA_ARPACK1BalancedRandomForestClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0RobustScaler1mutual_info_classifPercentile2LabelEncoder0_1MaxAbsScaler4VarianceThreshold5_2RandomTreesEmbedding6_3BaggingClassifier7_2": {
    "id": "ImputerMedian0RobustScaler1mutual_info_classifPercentile2LabelEncoder0_1MaxAbsScaler4VarianceThreshold5_2RandomTreesEmbedding6_3BaggingClassifier7_2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0RFE_GradientBoosting1PCA_LAPACK2RandomForestMeta3": {
    "id": "MaxAbsScaler0RFE_GradientBoosting1PCA_LAPACK2RandomForestMeta3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "Normalizer0mutual_info_classifPercentile1PCA_Randomized2RidgeClassifierCV3": {
    "id": "Normalizer0mutual_info_classifPercentile1PCA_Randomized2RidgeClassifierCV3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerOneHotEncoderPrim0MinMaxScaler1UnivariateSelectChiFDR2PCA_LAPACK3DecisionTreeClassifier4ImputerMedian0PowerTransformer6_1f_classifPercentile7_3BernoulliNBClassifier8_3ImputerEncoderPrim0_1PowerTransformer10_7mutual_info_classifKbest11_8PCA_Randomized12LGBMClassifier13_7MajorityVoting5_9_14": {
    "id": "ImputerOneHotEncoderPrim0MinMaxScaler1UnivariateSelectChiFDR2PCA_LAPACK3DecisionTreeClassifier4ImputerMedian0PowerTransformer6_1f_classifPercentile7_3BernoulliNBClassifier8_3ImputerEncoderPrim0_1PowerTransformer10_7mutual_info_classifKbest11_8PCA_Randomized12LGBMClassifier13_7MajorityVoting5_9_14",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with Chi-square. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          8
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          7
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          5,
          9,
          14
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0UnivariateSelectChiFWE1IncrementalPCA2LogisticRegression3": {
    "id": "MinMaxScaler0UnivariateSelectChiFWE1IncrementalPCA2LogisticRegression3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFPR1KernelPCA2LogisticRegression3StandardScaler0_1mutual_info_classifPercentile5_1IncrementalPCA6_3XGBClassifier7_4RobustScaler0f_classifFWE9IncrementalPCA10_5LogisticRegression11_5MajorityVoting4_8_12": {
    "id": "StandardScaler0f_classifFPR1KernelPCA2LogisticRegression3StandardScaler0_1mutual_info_classifPercentile5_1IncrementalPCA6_3XGBClassifier7_4RobustScaler0f_classifFWE9IncrementalPCA10_5LogisticRegression11_5MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          5
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0f_classifFWE1IncrementalPCA2RF_classifier3": {
    "id": "QuantileTransformer0f_classifFWE1IncrementalPCA2RF_classifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0f_classifFWE1TruncatedSVD2ExtraTreesClassifier3": {
    "id": "RobustScaler0f_classifFWE1TruncatedSVD2ExtraTreesClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0RFE_GradientBoosting1IncrementalPCA2NearestCentroid3": {
    "id": "MaxAbsScaler0RFE_GradientBoosting1IncrementalPCA2NearestCentroid3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputer0KBinsDiscretizerOneHot1UnivariateSelectChiPercentile2PCA_LAPACK3GaussianProcessClassifierPrim4imputer0KBinsDiscretizerOrdinal6f_classifKbest7_1FastICA8_4KNeighborsClassifierPrim9_5ImputerMedian0_6MaxAbsScaler11_6RFE_GradientBoosting12_1PCA_ARPACK13_1LGBMClassifier14_1RandomForestMeta5_10_15": {
    "id": "imputer0KBinsDiscretizerOneHot1UnivariateSelectChiPercentile2PCA_LAPACK3GaussianProcessClassifierPrim4imputer0KBinsDiscretizerOrdinal6f_classifKbest7_1FastICA8_4KNeighborsClassifierPrim9_5ImputerMedian0_6MaxAbsScaler11_6RFE_GradientBoosting12_1PCA_ARPACK13_1LGBMClassifier14_1RandomForestMeta5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0,
          6
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          1
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "PowerTransformer0mutual_info_classifKbest1KernelPCA2KNeighborsClassifierPrim3": {
    "id": "PowerTransformer0mutual_info_classifKbest1KernelPCA2KNeighborsClassifierPrim3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0f_classifFWE1FastICA2LGBMClassifier3": {
    "id": "QuantileTransformer0f_classifFWE1FastICA2LGBMClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0QuantileTransformer1f_classifPercentile2TruncatedSVD3RidgeClassifierCV4": {
    "id": "ImputerMedian0QuantileTransformer1f_classifPercentile2TruncatedSVD3RidgeClassifierCV4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0VarianceThreshold1IncrementalPCA2PassiveAggressiveClassifier3MaxAbsScaler0RFE_GradientBoosting5_1PCA_Randomized6_3LGBMClassifier7_4f_classifPercentile0PCA_LAPACK9_3SVC10_8RandomForestMeta4_8_11": {
    "id": "RobustScaler0VarianceThreshold1IncrementalPCA2PassiveAggressiveClassifier3MaxAbsScaler0RFE_GradientBoosting5_1PCA_Randomized6_3LGBMClassifier7_4f_classifPercentile0PCA_LAPACK9_3SVC10_8RandomForestMeta4_8_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          8
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          8,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "OneHotEncoder0Normalizer1f_classifFPR2TruncatedSVD3LogisticRegressionCV4": {
    "id": "OneHotEncoder0Normalizer1f_classifFPR2TruncatedSVD3LogisticRegressionCV4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      }
    }
  },
  "LabelEncoder0MaxAbsScaler1VarianceThreshold2KernelPCA3RidgeClassifierCV4": {
    "id": "LabelEncoder0MaxAbsScaler1VarianceThreshold2KernelPCA3RidgeClassifierCV4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      }
    }
  },
  "OneHotEncoder0MinMaxScaler1f_classifFWE2FastICA3SVC4": {
    "id": "OneHotEncoder0MinMaxScaler1f_classifFWE2FastICA3SVC4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0VarianceThreshold1PCA_ARPACK2RUSBoostClassifier3": {
    "id": "MinMaxScaler0VarianceThreshold1PCA_ARPACK2RUSBoostClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0RobustScaler1VarianceThreshold2FastICA3LGBMClassifier4imputerIndicator0_1RFE_RandomForest6_1RandomTreesEmbedding7_4LinearDiscriminantAnalysisPrim8_3RandomForestMeta5_9": {
    "id": "ImputerMedian0RobustScaler1VarianceThreshold2FastICA3LGBMClassifier4imputerIndicator0_1RFE_RandomForest6_1RandomTreesEmbedding7_4LinearDiscriminantAnalysisPrim8_3RandomForestMeta5_9",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          3
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          5,
          9
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "PowerTransformer0RFE_RandomForest1PCA_Randomized2BalancedRandomForestClassifier3": {
    "id": "PowerTransformer0RFE_RandomForest1PCA_Randomized2BalancedRandomForestClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0mutual_info_classifPercentile1PCA_LAPACK2KNeighborsClassifierPrim3f_classifFPR0_2RandomTreesEmbedding5BernoulliNBClassifier6RandomForestMeta4_7": {
    "id": "PowerTransformer0mutual_info_classifPercentile1PCA_LAPACK2KNeighborsClassifierPrim3f_classifFPR0_2RandomTreesEmbedding5BernoulliNBClassifier6RandomForestMeta4_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          4,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "OneHotEncoder0StandardScaler1f_classifFWE2PCA_LAPACK3LogisticRegressionCV4ImputerEncoderPrim0_1QuantileTransformer6_2f_classifPercentile7_1IncrementalPCA8_2ExtraTreesClassifier9_1MajorityVoting5_10": {
    "id": "OneHotEncoder0StandardScaler1f_classifFWE2PCA_LAPACK3LogisticRegressionCV4ImputerEncoderPrim0_1QuantileTransformer6_2f_classifPercentile7_1IncrementalPCA8_2ExtraTreesClassifier9_1MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerOneHotEncoderPrim0StandardScaler1f_classifFPR2QuadraticDiscriminantAnalysis3": {
    "id": "ImputerOneHotEncoderPrim0StandardScaler1f_classifFPR2QuadraticDiscriminantAnalysis3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0VarianceThreshold1TruncatedSVD2SGDClassifier3OneHotEncoder0RobustScaler5_1f_classifFWE6TruncatedSVD7_3NearestCentroid8_2imputerIndicator0_5MinMaxScaler10mutual_info_classifPercentile11PCA_Randomized12_3XGBClassifier13_1MajorityVoting4_9_14": {
    "id": "ImputerMedian0VarianceThreshold1TruncatedSVD2SGDClassifier3OneHotEncoder0RobustScaler5_1f_classifFWE6TruncatedSVD7_3NearestCentroid8_2imputerIndicator0_5MinMaxScaler10mutual_info_classifPercentile11PCA_Randomized12_3XGBClassifier13_1MajorityVoting4_9_14",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          1
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          4,
          9,
          14
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "PowerTransformer0VarianceThreshold1IncrementalPCA2LogisticRegressionCV3": {
    "id": "PowerTransformer0VarianceThreshold1IncrementalPCA2LogisticRegressionCV3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0RFE_GradientBoosting1IncrementalPCA2SGDClassifier3f_classifPercentile0_2FastICA5_2ExtraTreesClassifier6_4MajorityVoting4_7": {
    "id": "PowerTransformer0RFE_GradientBoosting1IncrementalPCA2SGDClassifier3f_classifPercentile0_2FastICA5_2ExtraTreesClassifier6_4MajorityVoting4_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          4
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          4,
          7
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0RFE_RandomForest1TruncatedSVD2SGDClassifier3Normalizer0_1mutual_info_classifPercentile5_2TruncatedSVD6_2BernoulliNBClassifier7_1Normalizer0_1f_classifFWE9PCA_Randomized10_6LinearDiscriminantAnalysisPrim11_1MajorityVoting4_8_12": {
    "id": "MaxAbsScaler0RFE_RandomForest1TruncatedSVD2SGDClassifier3Normalizer0_1mutual_info_classifPercentile5_2TruncatedSVD6_2BernoulliNBClassifier7_1Normalizer0_1f_classifFWE9PCA_Randomized10_6LinearDiscriminantAnalysisPrim11_1MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0RobustScaler1RFE_RandomForest2IncrementalPCA3RidgeClassifier4": {
    "id": "ImputerEncoderPrim0RobustScaler1RFE_RandomForest2IncrementalPCA3RidgeClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0f_classifKbest1TruncatedSVD2QuadraticDiscriminantAnalysis3": {
    "id": "Normalizer0f_classifKbest1TruncatedSVD2QuadraticDiscriminantAnalysis3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0VarianceThreshold1FastICA2AdaBoostClassifier3Normalizer0_1mutual_info_classifPercentile5FastICA6BaggingClassifier7_3RobustScaler0f_classifFPR9_2PCA_LAPACK10RF_classifier11_2RandomForestMeta4_8_12": {
    "id": "Normalizer0VarianceThreshold1FastICA2AdaBoostClassifier3Normalizer0_1mutual_info_classifPercentile5FastICA6BaggingClassifier7_3RobustScaler0f_classifFPR9_2PCA_LAPACK10RF_classifier11_2RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          2
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "NumericData0ImputerEncoderPrim0_1QuantileTransformer2mutual_info_classifKbest3FastICA4XGBClassifier5": {
    "id": "NumericData0ImputerEncoderPrim0_1QuantileTransformer2mutual_info_classifKbest3FastICA4XGBClassifier5",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "NumericData",
          "hyperparams_run": {
            "default": true
          },
          "description": "Extracts only numeric data columns from input.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0f_classifKbest1PCA_LAPACK2NearestCentroid3": {
    "id": "PowerTransformer0f_classifKbest1PCA_LAPACK2NearestCentroid3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputer0f_classifFWE1PCA_ARPACK2BaggingClassifier3ImputerMedian0StandardScaler5RFE_RandomForest6_2FastICA7_2RidgeClassifierCV8_1RandomForestMeta4_9": {
    "id": "imputer0f_classifFWE1PCA_ARPACK2BaggingClassifier3ImputerMedian0StandardScaler5RFE_RandomForest6_2FastICA7_2RidgeClassifierCV8_1RandomForestMeta4_9",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          4,
          9
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0mutual_info_classifPercentile1KernelPCA2RidgeClassifierCV3": {
    "id": "MinMaxScaler0mutual_info_classifPercentile1KernelPCA2RidgeClassifierCV3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      }
    }
  },
  "LabelEncoder0PowerTransformer1mutual_info_classifPercentile2FastICA3AdaBoostClassifier4ImputerEncoderPrim0_1MaxAbsScaler6mutual_info_classifKbest7_2TruncatedSVD8_2LGBMClassifier9_4MajorityVoting5_10": {
    "id": "LabelEncoder0PowerTransformer1mutual_info_classifPercentile2FastICA3AdaBoostClassifier4ImputerEncoderPrim0_1MaxAbsScaler6mutual_info_classifKbest7_2TruncatedSVD8_2LGBMClassifier9_4MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          4
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerMedian0MinMaxScaler1UnivariateSelectChiKbest2PCA_LAPACK3RUSBoostClassifier4": {
    "id": "ImputerMedian0MinMaxScaler1UnivariateSelectChiKbest2PCA_LAPACK3RUSBoostClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0f_classifFWE1PCA_LAPACK2GaussianNBClassifier3MinMaxScaler0f_classifPercentile5_1PCA_LAPACK6LinearDiscriminantAnalysisPrim7_3RandomForestMeta4_8": {
    "id": "Normalizer0f_classifFWE1PCA_LAPACK2GaussianNBClassifier3MinMaxScaler0f_classifPercentile5_1PCA_LAPACK6LinearDiscriminantAnalysisPrim7_3RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0VarianceThreshold1TruncatedSVD2BernoulliNBClassifier3": {
    "id": "RobustScaler0VarianceThreshold1TruncatedSVD2BernoulliNBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0f_classifFWE1TruncatedSVD2LGBMClassifier3KBinsDiscretizerOrdinal0mutual_info_classifKbest5_2IncrementalPCA6_3ExtraTreesClassifier7MajorityVoting4_8": {
    "id": "PowerTransformer0f_classifFWE1TruncatedSVD2LGBMClassifier3KBinsDiscretizerOrdinal0mutual_info_classifKbest5_2IncrementalPCA6_3ExtraTreesClassifier7MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "OneHotEncoder0MinMaxScaler1RFE_GradientBoosting2KernelPCA3RUSBoostClassifier4ImputerEncoderPrim0_1Normalizer6_1mutual_info_classifKbest7_2KernelPCA8_4LGBMClassifier9_5RandomForestMeta5_10": {
    "id": "OneHotEncoder0MinMaxScaler1RFE_GradientBoosting2KernelPCA3RUSBoostClassifier4ImputerEncoderPrim0_1Normalizer6_1mutual_info_classifKbest7_2KernelPCA8_4LGBMClassifier9_5RandomForestMeta5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "LabelEncoder0RobustScaler1mutual_info_classifPercentile2IncrementalPCA3DecisionTreeClassifier4ImputerOneHotEncoderPrim0StandardScaler6RFE_RandomForest7_2TruncatedSVD8_4KNeighborsClassifierPrim9MajorityVoting5_10": {
    "id": "LabelEncoder0RobustScaler1mutual_info_classifPercentile2IncrementalPCA3DecisionTreeClassifier4ImputerOneHotEncoderPrim0StandardScaler6RFE_RandomForest7_2TruncatedSVD8_4KNeighborsClassifierPrim9MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "imputer0RobustScaler1mutual_info_classifKbest2PCA_ARPACK3BalancedRandomForestClassifier4ImputerMedian0_1RobustScaler6f_classifFPR7_3FastICA8_1RidgeClassifier9_3MajorityVoting5_10": {
    "id": "imputer0RobustScaler1mutual_info_classifKbest2PCA_ARPACK3BalancedRandomForestClassifier4ImputerMedian0_1RobustScaler6f_classifFPR7_3FastICA8_1RidgeClassifier9_3MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOrdinal0f_classifFPR1PCA_LAPACK2RidgeClassifier3": {
    "id": "KBinsDiscretizerOrdinal0f_classifFPR1PCA_LAPACK2RidgeClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0f_classifFPR1TruncatedSVD2LGBMClassifier3": {
    "id": "PowerTransformer0f_classifFPR1TruncatedSVD2LGBMClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0VarianceThreshold1PCA_Randomized2LogisticRegression3": {
    "id": "QuantileTransformer0VarianceThreshold1PCA_Randomized2LogisticRegression3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerOneHotEncoderPrim0RobustScaler1mutual_info_classifKbest2FastICA3NearestCentroid4": {
    "id": "ImputerOneHotEncoderPrim0RobustScaler1mutual_info_classifKbest2FastICA3NearestCentroid4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifFPR1PCA_Randomized2BaggingClassifier3PowerTransformer0_1VarianceThreshold5_1FastICA6_2GaussianNBClassifier7_4MajorityVoting4_8": {
    "id": "KBinsDiscretizerOneHot0f_classifFPR1PCA_Randomized2BaggingClassifier3PowerTransformer0_1VarianceThreshold5_1FastICA6_2GaussianNBClassifier7_4MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerMedian0RobustScaler1RFE_RandomForest2PCA_LAPACK3GradientBoostingClassifier4": {
    "id": "ImputerMedian0RobustScaler1RFE_RandomForest2PCA_LAPACK3GradientBoostingClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0mutual_info_classifPercentile1PCA_LAPACK2GradientBoostingClassifier3MaxAbsScaler0mutual_info_classifKbest5_1IncrementalPCA6_2LogisticRegression7_4RandomForestMeta4_8": {
    "id": "MinMaxScaler0mutual_info_classifPercentile1PCA_LAPACK2GradientBoostingClassifier3MaxAbsScaler0mutual_info_classifKbest5_1IncrementalPCA6_2LogisticRegression7_4RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "imputer0PowerTransformer1RFE_RandomForest2TruncatedSVD3GradientBoostingClassifier4ImputerOneHotEncoderPrim0_1RobustScaler6_2f_classifPercentile7_1FastICA8_1LinearDiscriminantAnalysisPrim9_4RandomForestMeta5_10": {
    "id": "imputer0PowerTransformer1RFE_RandomForest2TruncatedSVD3GradientBoostingClassifier4ImputerOneHotEncoderPrim0_1RobustScaler6_2f_classifPercentile7_1FastICA8_1LinearDiscriminantAnalysisPrim9_4RandomForestMeta5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          4
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0RobustScaler1f_classifFPR2PCA_LAPACK3SGDClassifier4imputerIndicator0_1RobustScaler6_1f_classifFPR7_2FastICA8_1XGBClassifier9_1imputer0_1RobustScaler11_7f_classifKbest12_2IncrementalPCA13_3RidgeClassifierCV14_6MajorityVoting5_10_15": {
    "id": "ImputerEncoderPrim0RobustScaler1f_classifFPR2PCA_LAPACK3SGDClassifier4imputerIndicator0_1RobustScaler6_1f_classifFPR7_2FastICA8_1XGBClassifier9_1imputer0_1RobustScaler11_7f_classifKbest12_2IncrementalPCA13_3RidgeClassifierCV14_6MajorityVoting5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          6
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifFWE1IncrementalPCA2GaussianProcessClassifierPrim3": {
    "id": "KBinsDiscretizerOneHot0f_classifFWE1IncrementalPCA2GaussianProcessClassifierPrim3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifFPR1FastICA2LogisticRegressionCV3": {
    "id": "MaxAbsScaler0f_classifFPR1FastICA2LogisticRegressionCV3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0f_classifFPR1PCA_Randomized2BernoulliNBClassifier3KBinsDiscretizerOneHot0_1UnivariateSelectChiPercentile5_2IncrementalPCA6LogisticRegression7_3RandomForestMeta4_8": {
    "id": "MinMaxScaler0f_classifFPR1PCA_Randomized2BernoulliNBClassifier3KBinsDiscretizerOneHot0_1UnivariateSelectChiPercentile5_2IncrementalPCA6LogisticRegression7_3RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0VarianceThreshold1TruncatedSVD2LinearSVC3": {
    "id": "KBinsDiscretizerOneHot0VarianceThreshold1TruncatedSVD2LinearSVC3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0RobustScaler1TruncatedSVD2BernoulliNBClassifier3": {
    "id": "ImputerEncoderPrim0RobustScaler1TruncatedSVD2BernoulliNBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0mutual_info_classifPercentile1FastICA2DecisionTreeClassifier3UnivariateSelectChiKbest0RandomTreesEmbedding5XGBClassifier6MinMaxScaler0_1RFE_GradientBoosting8_1IncrementalPCA9_5LGBMClassifier10MajorityVoting4_7_11": {
    "id": "QuantileTransformer0mutual_info_classifPercentile1FastICA2DecisionTreeClassifier3UnivariateSelectChiKbest0RandomTreesEmbedding5XGBClassifier6MinMaxScaler0_1RFE_GradientBoosting8_1IncrementalPCA9_5LGBMClassifier10MajorityVoting4_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0mutual_info_classifPercentile1FastICA2PassiveAggressiveClassifier3KBinsDiscretizerOneHot0_1mutual_info_classifPercentile5PCA_LAPACK6_1NearestCentroid7_1MajorityVoting4_8": {
    "id": "StandardScaler0mutual_info_classifPercentile1FastICA2PassiveAggressiveClassifier3KBinsDiscretizerOneHot0_1mutual_info_classifPercentile5PCA_LAPACK6_1NearestCentroid7_1MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0VarianceThreshold1TruncatedSVD2KNeighborsClassifierPrim3MinMaxScaler0_1f_classifFWE5_2IncrementalPCA6XGBClassifier7RobustScaler0_1RFE_RandomForest9_1TruncatedSVD10_3GaussianNBClassifier11_2MajorityVoting4_8_12": {
    "id": "StandardScaler0VarianceThreshold1TruncatedSVD2KNeighborsClassifierPrim3MinMaxScaler0_1f_classifFWE5_2IncrementalPCA6XGBClassifier7RobustScaler0_1RFE_RandomForest9_1TruncatedSVD10_3GaussianNBClassifier11_2MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          2
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0mutual_info_classifKbest1TruncatedSVD2LogisticRegressionCV3": {
    "id": "RobustScaler0mutual_info_classifKbest1TruncatedSVD2LogisticRegressionCV3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      }
    }
  },
  "LabelEncoder0MinMaxScaler1f_classifKbest2TruncatedSVD3BaggingClassifier4ImputerMedian0_1MinMaxScaler6f_classifFWE7_1PCA_ARPACK8_3XGBClassifier9MajorityVoting5_10": {
    "id": "LabelEncoder0MinMaxScaler1f_classifKbest2TruncatedSVD3BaggingClassifier4ImputerMedian0_1MinMaxScaler6f_classifFWE7_1PCA_ARPACK8_3XGBClassifier9MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          3
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifFPR1PCA_LAPACK2NearestCentroid3": {
    "id": "MaxAbsScaler0f_classifFPR1PCA_LAPACK2NearestCentroid3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "VarianceThreshold0IncrementalPCA1SVC2PowerTransformer0VarianceThreshold4_1KernelPCA5_2PassiveAggressiveClassifier6_3RobustScaler0_4RFE_GradientBoosting8_4PCA_Randomized9_6LogisticRegression10_3MajorityVoting3_7_11": {
    "id": "VarianceThreshold0IncrementalPCA1SVC2PowerTransformer0VarianceThreshold4_1KernelPCA5_2PassiveAggressiveClassifier6_3RobustScaler0_4RFE_GradientBoosting8_4PCA_Randomized9_6LogisticRegression10_3MajorityVoting3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerOneHotEncoderPrim0RobustScaler1f_classifFWE2LogisticRegression3ImputerMedian0_1f_classifFWE5_3IncrementalPCA6_3XGBClassifier7imputerIndicator0MaxAbsScaler9_1RFE_GradientBoosting10KernelPCA11_1AdaBoostClassifier12MajorityVoting4_8_13": {
    "id": "ImputerOneHotEncoderPrim0RobustScaler1f_classifFWE2LogisticRegression3ImputerMedian0_1f_classifFWE5_3IncrementalPCA6_3XGBClassifier7imputerIndicator0MaxAbsScaler9_1RFE_GradientBoosting10KernelPCA11_1AdaBoostClassifier12MajorityVoting4_8_13",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          3
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          4,
          8,
          13
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0StandardScaler1mutual_info_classifKbest2PCA_Randomized3LGBMClassifier4ImputerMedian0StandardScaler6_2RFE_RandomForest7IncrementalPCA8RidgeClassifierCV9_5RandomForestMeta5_10": {
    "id": "ImputerEncoderPrim0StandardScaler1mutual_info_classifKbest2PCA_Randomized3LGBMClassifier4ImputerMedian0StandardScaler6_2RFE_RandomForest7IncrementalPCA8RidgeClassifierCV9_5RandomForestMeta5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0f_classifKbest1SGDClassifier2": {
    "id": "MinMaxScaler0f_classifKbest1SGDClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1IncrementalPCA2NearestCentroid3": {
    "id": "StandardScaler0f_classifFWE1IncrementalPCA2NearestCentroid3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0IncrementalPCA1RF_classifier2KBinsDiscretizerOneHot0PCA_Randomized4_1BernoulliNBClassifier5_2RandomForestMeta3_6": {
    "id": "StandardScaler0IncrementalPCA1RF_classifier2KBinsDiscretizerOneHot0PCA_Randomized4_1BernoulliNBClassifier5_2RandomForestMeta3_6",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          3,
          6
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0Normalizer1f_classifFWE2IncrementalPCA3AdaBoostClassifier4LabelEncoder0RobustScaler6mutual_info_classifPercentile7_2RandomTreesEmbedding8_2GaussianNBClassifier9_5RandomForestMeta5_10": {
    "id": "ImputerEncoderPrim0Normalizer1f_classifFWE2IncrementalPCA3AdaBoostClassifier4LabelEncoder0RobustScaler6mutual_info_classifPercentile7_2RandomTreesEmbedding8_2GaussianNBClassifier9_5RandomForestMeta5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0RFE_GradientBoosting1FastICA2LGBMClassifier3": {
    "id": "RobustScaler0RFE_GradientBoosting1FastICA2LGBMClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0UnivariateSelectChiKbest1PCA_ARPACK2SVC3MaxAbsScaler0UnivariateSelectChiFWE5_1PCA_ARPACK6_1BaggingClassifier7_2RandomForestMeta4_8": {
    "id": "QuantileTransformer0UnivariateSelectChiKbest1PCA_ARPACK2SVC3MaxAbsScaler0UnivariateSelectChiFWE5_1PCA_ARPACK6_1BaggingClassifier7_2RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0mutual_info_classifKbest1IncrementalPCA2BernoulliNBClassifier3": {
    "id": "MaxAbsScaler0mutual_info_classifKbest1IncrementalPCA2BernoulliNBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOrdinal0UnivariateSelectChiFDR1FastICA2NearestCentroid3": {
    "id": "KBinsDiscretizerOrdinal0UnivariateSelectChiFDR1FastICA2NearestCentroid3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with Chi-square. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFPR1IncrementalPCA2NearestCentroid3": {
    "id": "StandardScaler0f_classifFPR1IncrementalPCA2NearestCentroid3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputer0KBinsDiscretizerOneHot1UnivariateSelectChiFWE2PCA_LAPACK3DecisionTreeClassifier4": {
    "id": "imputer0KBinsDiscretizerOneHot1UnivariateSelectChiFWE2PCA_LAPACK3DecisionTreeClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0mutual_info_classifPercentile1LogisticRegressionCV2": {
    "id": "MinMaxScaler0mutual_info_classifPercentile1LogisticRegressionCV2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      }
    }
  },
  "UnivariateSelectChiPercentile0PCA_Randomized1RUSBoostClassifier2QuantileTransformer0UnivariateSelectChiFWE4PCA_LAPACK5RidgeClassifier6_1PowerTransformer0_4f_classifFDR8_5RandomTreesEmbedding9_2RidgeClassifierCV10RandomForestMeta3_7_11": {
    "id": "UnivariateSelectChiPercentile0PCA_Randomized1RUSBoostClassifier2QuantileTransformer0UnivariateSelectChiFWE4PCA_LAPACK5RidgeClassifier6_1PowerTransformer0_4f_classifFDR8_5RandomTreesEmbedding9_2RidgeClassifierCV10RandomForestMeta3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          5
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0RFE_RandomForest1PCA_ARPACK2GaussianNBClassifier3": {
    "id": "RobustScaler0RFE_RandomForest1PCA_ARPACK2GaussianNBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0UnivariateSelectChiKbest1PCA_LAPACK2LogisticRegression3": {
    "id": "Normalizer0UnivariateSelectChiKbest1PCA_LAPACK2LogisticRegression3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "mutual_info_classifPercentile0IncrementalPCA1ExtraTreesClassifier2": {
    "id": "mutual_info_classifPercentile0IncrementalPCA1ExtraTreesClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0FastICA1BaggingClassifier2": {
    "id": "RobustScaler0FastICA1BaggingClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0QuantileTransformer1mutual_info_classifPercentile2KNeighborsClassifierPrim3ImputerEncoderPrim0_1MaxAbsScaler5_1f_classifKbest6_1PCA_ARPACK7_1GradientBoostingClassifier8_1ImputerMedian0_1QuantileTransformer10_1f_classifKbest11IncrementalPCA12_3ExtraTreesClassifier13_6MajorityVoting4_9_14": {
    "id": "ImputerMedian0QuantileTransformer1mutual_info_classifPercentile2KNeighborsClassifierPrim3ImputerEncoderPrim0_1MaxAbsScaler5_1f_classifKbest6_1PCA_ARPACK7_1GradientBoostingClassifier8_1ImputerMedian0_1QuantileTransformer10_1f_classifKbest11IncrementalPCA12_3ExtraTreesClassifier13_6MajorityVoting4_9_14",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          6
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          4,
          9,
          14
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "mutual_info_classifKbest0MultinomialNB1": {
    "id": "mutual_info_classifKbest0MultinomialNB1",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MultinomialNB",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multinomial models. The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.",
          "type": "Classifier"
        }
      }
    }
  },
  "RFE_GradientBoosting0SVC1": {
    "id": "RFE_GradientBoosting0SVC1",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiKbest1IncrementalPCA2StandardScaler0f_classifFPR4_1IncrementalPCA5_1GradientBoostingClassifier6_1": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiKbest1IncrementalPCA2StandardScaler0f_classifFPR4_1IncrementalPCA5_1GradientBoostingClassifier6_1",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputer0MaxAbsScaler1f_classifFPR2TruncatedSVD3LogisticRegressionCV4ImputerEncoderPrim0Normalizer6_1f_classifFWE7_2FastICA8_3BernoulliNBClassifier9_1MajorityVoting5_10": {
    "id": "imputer0MaxAbsScaler1f_classifFPR2TruncatedSVD3LogisticRegressionCV4ImputerEncoderPrim0Normalizer6_1f_classifFWE7_2FastICA8_3BernoulliNBClassifier9_1MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0f_classifFPR1PCA_Randomized2SGDClassifier3StandardScaler0_1f_classifFWE5_1RandomTreesEmbedding6KNeighborsClassifierPrim7_2Normalizer0_1f_classifPercentile9PCA_Randomized10_6LGBMClassifier11_6RandomForestMeta4_8_12": {
    "id": "StandardScaler0f_classifFPR1PCA_Randomized2SGDClassifier3StandardScaler0_1f_classifFWE5_1RandomTreesEmbedding6KNeighborsClassifierPrim7_2Normalizer0_1f_classifPercentile9PCA_Randomized10_6LGBMClassifier11_6RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifPercentile1PCA_ARPACK2RUSBoostClassifier3Normalizer0f_classifFDR5TruncatedSVD6_3RF_classifier7_2MaxAbsScaler0_5f_classifKbest9_6PCA_Randomized10_2LinearDiscriminantAnalysisPrim11_5RandomForestMeta4_8_12": {
    "id": "RobustScaler0f_classifPercentile1PCA_ARPACK2RUSBoostClassifier3Normalizer0f_classifFDR5TruncatedSVD6_3RF_classifier7_2MaxAbsScaler0_5f_classifKbest9_6PCA_Randomized10_2LinearDiscriminantAnalysisPrim11_5RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          5
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "UnivariateSelectChiKbest0PCA_LAPACK1ExtraTreesClassifier2": {
    "id": "UnivariateSelectChiKbest0PCA_LAPACK1ExtraTreesClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0mutual_info_classifPercentile1IncrementalPCA2QuadraticDiscriminantAnalysis3": {
    "id": "MinMaxScaler0mutual_info_classifPercentile1IncrementalPCA2QuadraticDiscriminantAnalysis3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifPercentile1PCA_LAPACK2XGBClassifier3KBinsDiscretizerOneHot0_1f_classifKbest5PCA_Randomized6_2EasyEnsembleClassifier7_4Normalizer0_1RFE_RandomForest9_5PCA_ARPACK10_7ExtraTreesClassifier11_1RandomForestMeta4_8_12": {
    "id": "StandardScaler0f_classifPercentile1PCA_LAPACK2XGBClassifier3KBinsDiscretizerOneHot0_1f_classifKbest5PCA_Randomized6_2EasyEnsembleClassifier7_4Normalizer0_1RFE_RandomForest9_5PCA_ARPACK10_7ExtraTreesClassifier11_1RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0f_classifFWE1PCA_ARPACK2BalancedRandomForestClassifier3Normalizer0RFE_GradientBoosting5IncrementalPCA6_3BernoulliNBClassifier7_3StandardScaler0_1RFE_GradientBoosting9_6PCA_LAPACK10_7LinearSVC11_3RandomForestMeta4_8_12": {
    "id": "MinMaxScaler0f_classifFWE1PCA_ARPACK2BalancedRandomForestClassifier3Normalizer0RFE_GradientBoosting5IncrementalPCA6_3BernoulliNBClassifier7_3StandardScaler0_1RFE_GradientBoosting9_6PCA_LAPACK10_7LinearSVC11_3RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "OneHotEncoder0MaxAbsScaler1f_classifFPR2TruncatedSVD3BalancedRandomForestClassifier4ImputerMedian0_1QuantileTransformer6_1RFE_RandomForest7_2IncrementalPCA8_4RidgeClassifierCV9_3ImputerOneHotEncoderPrim0_6StandardScaler11_6VarianceThreshold12_8FastICA13_8SVC14_8MajorityVoting5_10_15": {
    "id": "OneHotEncoder0MaxAbsScaler1f_classifFPR2TruncatedSVD3BalancedRandomForestClassifier4ImputerMedian0_1QuantileTransformer6_1RFE_RandomForest7_2IncrementalPCA8_4RidgeClassifierCV9_3ImputerOneHotEncoderPrim0_6StandardScaler11_6VarianceThreshold12_8FastICA13_8SVC14_8MajorityVoting5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0,
          6
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          8
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          8
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          8
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "f_classifKbest0FastICA1BaggingClassifier2QuantileTransformer0mutual_info_classifPercentile4TruncatedSVD5_1EasyEnsembleClassifier6_2StandardScaler0RFE_GradientBoosting8FastICA9_2RidgeClassifierCV10_4RandomForestMeta3_7_11": {
    "id": "f_classifKbest0FastICA1BaggingClassifier2QuantileTransformer0mutual_info_classifPercentile4TruncatedSVD5_1EasyEnsembleClassifier6_2StandardScaler0RFE_GradientBoosting8FastICA9_2RidgeClassifierCV10_4RandomForestMeta3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          4
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "NumericData0XGBClassifier1ImputerOneHotEncoderPrim0RobustScaler3mutual_info_classifPercentile4RandomTreesEmbedding5KNeighborsClassifierPrim6_2MajorityVoting2_7": {
    "id": "NumericData0XGBClassifier1ImputerOneHotEncoderPrim0RobustScaler3mutual_info_classifPercentile4RandomTreesEmbedding5KNeighborsClassifierPrim6_2MajorityVoting2_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "NumericData",
          "hyperparams_run": {
            "default": true
          },
          "description": "Extracts only numeric data columns from input.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          2,
          7
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0mutual_info_classifKbest1IncrementalPCA2BaggingClassifier3QuantileTransformer0UnivariateSelectChiFPR5_1PCA_LAPACK6_3GaussianProcessClassifierPrim7_1MajorityVoting4_8": {
    "id": "MinMaxScaler0mutual_info_classifKbest1IncrementalPCA2BaggingClassifier3QuantileTransformer0UnivariateSelectChiFPR5_1PCA_LAPACK6_3GaussianProcessClassifierPrim7_1MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "Normalizer0f_classifFPR1PCA_ARPACK2LinearSVC3": {
    "id": "Normalizer0f_classifFPR1PCA_ARPACK2LinearSVC3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0XGBClassifier1": {
    "id": "MinMaxScaler0XGBClassifier1",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFPR1IncrementalPCA2RF_classifier3": {
    "id": "StandardScaler0f_classifFPR1IncrementalPCA2RF_classifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0mutual_info_classifPercentile1FastICA2LogisticRegression3": {
    "id": "RobustScaler0mutual_info_classifPercentile1FastICA2LogisticRegression3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0VarianceThreshold1FastICA2NearestCentroid3": {
    "id": "PowerTransformer0VarianceThreshold1FastICA2NearestCentroid3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1PCA_Randomized2RUSBoostClassifier3": {
    "id": "StandardScaler0f_classifFWE1PCA_Randomized2RUSBoostClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0MinMaxScaler1TruncatedSVD2RandomForestMeta3": {
    "id": "ImputerMedian0MinMaxScaler1TruncatedSVD2RandomForestMeta3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "LabelEncoder0Normalizer1f_classifFWE2ImputerMedian0_1Normalizer4_1VarianceThreshold5PCA_ARPACK6_2QuadraticDiscriminantAnalysis7_3": {
    "id": "LabelEncoder0Normalizer1f_classifFWE2ImputerMedian0_1Normalizer4_1VarianceThreshold5PCA_ARPACK6_2QuadraticDiscriminantAnalysis7_3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      }
    }
  },
  "LabelEncoder0RobustScaler1f_classifFWE2PCA_LAPACK3SVC4NumericData0_1XGBClassifier6_3imputer0StandardScaler8_2f_classifPercentile9_3TruncatedSVD10_2ExtraTreesClassifier11_1MajorityVoting5_7_12": {
    "id": "LabelEncoder0RobustScaler1f_classifFWE2PCA_LAPACK3SVC4NumericData0_1XGBClassifier6_3imputer0StandardScaler8_2f_classifPercentile9_3TruncatedSVD10_2ExtraTreesClassifier11_1MajorityVoting5_7_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "NumericData",
          "hyperparams_run": {
            "default": true
          },
          "description": "Extracts only numeric data columns from input.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          3
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          5,
          7,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "OneHotEncoder0Normalizer1f_classifFDR2AdaBoostClassifier3": {
    "id": "OneHotEncoder0Normalizer1f_classifFDR2AdaBoostClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputer0QuantileTransformer1f_classifFPR2TruncatedSVD3RandomForestMeta4": {
    "id": "imputer0QuantileTransformer1f_classifFPR2TruncatedSVD3RandomForestMeta4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "LabelEncoder0MinMaxScaler1f_classifPercentile2IncrementalPCA3LinearDiscriminantAnalysisPrim4ImputerOneHotEncoderPrim0_1StandardScaler6VarianceThreshold7_1TruncatedSVD8RF_classifier9_1ImputerEncoderPrim0_6StandardScaler11_1RFE_GradientBoosting12_2FastICA13_6GradientBoostingClassifier14_10MajorityVoting5_10_15": {
    "id": "LabelEncoder0MinMaxScaler1f_classifPercentile2IncrementalPCA3LinearDiscriminantAnalysisPrim4ImputerOneHotEncoderPrim0_1StandardScaler6VarianceThreshold7_1TruncatedSVD8RF_classifier9_1ImputerEncoderPrim0_6StandardScaler11_1RFE_GradientBoosting12_2FastICA13_6GradientBoostingClassifier14_10MajorityVoting5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0,
          6
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          2
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          6
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          10
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerMedian0PowerTransformer1mutual_info_classifKbest2IncrementalPCA3GaussianProcessClassifierPrim4": {
    "id": "ImputerMedian0PowerTransformer1mutual_info_classifKbest2IncrementalPCA3GaussianProcessClassifierPrim4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0f_classifFDR1FastICA2RUSBoostClassifier3StandardScaler0f_classifKbest5_2RandomTreesEmbedding6_1RF_classifier7_2MaxAbsScaler0_5f_classifPercentile9_6IncrementalPCA10_3NearestCentroid11_1MajorityVoting4_8_12": {
    "id": "Normalizer0f_classifFDR1FastICA2RUSBoostClassifier3StandardScaler0f_classifKbest5_2RandomTreesEmbedding6_1RF_classifier7_2MaxAbsScaler0_5f_classifPercentile9_6IncrementalPCA10_3NearestCentroid11_1MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerOneHotEncoderPrim0QuantileTransformer1RFE_RandomForest2FastICA3AdaBoostClassifier4": {
    "id": "ImputerOneHotEncoderPrim0QuantileTransformer1RFE_RandomForest2FastICA3AdaBoostClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      }
    }
  },
  "NumericData0XGBClassifier1ImputerMedian0Normalizer3RFE_RandomForest4IncrementalPCA5RUSBoostClassifier6imputerIndicator0_3StandardScaler8_4f_classifFPR9RandomTreesEmbedding10_6RidgeClassifierCV11_3RandomForestMeta2_7_12": {
    "id": "NumericData0XGBClassifier1ImputerMedian0Normalizer3RFE_RandomForest4IncrementalPCA5RUSBoostClassifier6imputerIndicator0_3StandardScaler8_4f_classifFPR9RandomTreesEmbedding10_6RidgeClassifierCV11_3RandomForestMeta2_7_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "NumericData",
          "hyperparams_run": {
            "default": true
          },
          "description": "Extracts only numeric data columns from input.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          3
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          2,
          7,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiKbest1PCA_LAPACK2NearestCentroid3": {
    "id": "MaxAbsScaler0UnivariateSelectChiKbest1PCA_LAPACK2NearestCentroid3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "LabelEncoder0StandardScaler1VarianceThreshold2RandomTreesEmbedding3XGBClassifier4": {
    "id": "LabelEncoder0StandardScaler1VarianceThreshold2RandomTreesEmbedding3XGBClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0f_classifPercentile1LinearSVC2MaxAbsScaler0_1RFE_RandomForest4_1KernelPCA5_2LogisticRegression6RandomForestMeta3_7": {
    "id": "RobustScaler0f_classifPercentile1LinearSVC2MaxAbsScaler0_1RFE_RandomForest4_1KernelPCA5_2LogisticRegression6RandomForestMeta3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0VarianceThreshold1PCA_ARPACK2SGDClassifier3": {
    "id": "MaxAbsScaler0VarianceThreshold1PCA_ARPACK2SGDClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0StandardScaler1VarianceThreshold2KernelPCA3QuadraticDiscriminantAnalysis4imputerIndicator0_1Normalizer6_1RFE_RandomForest7_1RandomTreesEmbedding8RUSBoostClassifier9_5RandomForestMeta5_10": {
    "id": "ImputerEncoderPrim0StandardScaler1VarianceThreshold2KernelPCA3QuadraticDiscriminantAnalysis4imputerIndicator0_1Normalizer6_1RFE_RandomForest7_1RandomTreesEmbedding8RUSBoostClassifier9_5RandomForestMeta5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFPR1GaussianProcessClassifierPrim2StandardScaler0_1f_classifPercentile4_1FastICA5_1PassiveAggressiveClassifier6_1RandomForestMeta3_7": {
    "id": "RobustScaler0f_classifFPR1GaussianProcessClassifierPrim2StandardScaler0_1f_classifPercentile4_1FastICA5_1PassiveAggressiveClassifier6_1RandomForestMeta3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "VarianceThreshold0PCA_ARPACK1GaussianProcessClassifierPrim2": {
    "id": "VarianceThreshold0PCA_ARPACK1GaussianProcessClassifierPrim2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0StandardScaler1VarianceThreshold2TruncatedSVD3GaussianProcessClassifierPrim4ImputerMedian0MaxAbsScaler6_1f_classifFWE7_2RandomTreesEmbedding8_1NearestCentroid9_5ImputerOneHotEncoderPrim0_1QuantileTransformer11RFE_RandomForest12TruncatedSVD13_8XGBClassifier14_9MajorityVoting5_10_15": {
    "id": "ImputerMedian0StandardScaler1VarianceThreshold2TruncatedSVD3GaussianProcessClassifierPrim4ImputerMedian0MaxAbsScaler6_1f_classifFWE7_2RandomTreesEmbedding8_1NearestCentroid9_5ImputerOneHotEncoderPrim0_1QuantileTransformer11RFE_RandomForest12TruncatedSVD13_8XGBClassifier14_9MajorityVoting5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          8
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          9
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "imputer0VarianceThreshold1RandomTreesEmbedding2MultinomialNB3": {
    "id": "imputer0VarianceThreshold1RandomTreesEmbedding2MultinomialNB3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "MultinomialNB",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multinomial models. The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0UnivariateSelectChiPercentile1PCA_ARPACK2RF_classifier3": {
    "id": "QuantileTransformer0UnivariateSelectChiPercentile1PCA_ARPACK2RF_classifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0mutual_info_classifKbest1TruncatedSVD2RidgeClassifier3": {
    "id": "MaxAbsScaler0mutual_info_classifKbest1TruncatedSVD2RidgeClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0f_classifKbest1RandomTreesEmbedding2EasyEnsembleClassifier3MinMaxScaler0RFE_RandomForest5_2PCA_LAPACK6_3EasyEnsembleClassifier7MajorityVoting4_8": {
    "id": "QuantileTransformer0f_classifKbest1RandomTreesEmbedding2EasyEnsembleClassifier3MinMaxScaler0RFE_RandomForest5_2PCA_LAPACK6_3EasyEnsembleClassifier7MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0f_classifPercentile1PCA_Randomized2BalancedRandomForestClassifier3KBinsDiscretizerOneHot0f_classifFPR5_1FastICA6_1AdaBoostClassifier7_2MajorityVoting4_8": {
    "id": "StandardScaler0f_classifPercentile1PCA_Randomized2BalancedRandomForestClassifier3KBinsDiscretizerOneHot0f_classifFPR5_1FastICA6_1AdaBoostClassifier7_2MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0VarianceThreshold1PCA_LAPACK2SGDClassifier3": {
    "id": "MaxAbsScaler0VarianceThreshold1PCA_LAPACK2SGDClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFPR1PCA_LAPACK2RidgeClassifier3KBinsDiscretizerOrdinal0f_classifFWE5PCA_ARPACK6_3ExtraTreesClassifier7PowerTransformer0_5f_classifFWE9_6PCA_Randomized10_5LogisticRegressionCV11MajorityVoting4_8_12": {
    "id": "StandardScaler0f_classifFPR1PCA_LAPACK2RidgeClassifier3KBinsDiscretizerOrdinal0f_classifFWE5PCA_ARPACK6_3ExtraTreesClassifier7PowerTransformer0_5f_classifFWE9_6PCA_Randomized10_5LogisticRegressionCV11MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0f_classifFWE1TruncatedSVD2QuadraticDiscriminantAnalysis3": {
    "id": "QuantileTransformer0f_classifFWE1TruncatedSVD2QuadraticDiscriminantAnalysis3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1PCA_LAPACK2PassiveAggressiveClassifier3KBinsDiscretizerOneHot0_1f_classifFPR5_1KernelPCA6RidgeClassifier7_1RandomForestMeta4_8": {
    "id": "StandardScaler0f_classifFWE1PCA_LAPACK2PassiveAggressiveClassifier3KBinsDiscretizerOneHot0_1f_classifFPR5_1KernelPCA6RidgeClassifier7_1RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "PowerTransformer0f_classifKbest1IncrementalPCA2GradientBoostingClassifier3KBinsDiscretizerOneHot0UnivariateSelectChiFDR5KernelPCA6LinearDiscriminantAnalysisPrim7_2KBinsDiscretizerOneHot0f_classifPercentile9_6PCA_Randomized10_1BaggingClassifier11_8RandomForestMeta4_8_12": {
    "id": "PowerTransformer0f_classifKbest1IncrementalPCA2GradientBoostingClassifier3KBinsDiscretizerOneHot0UnivariateSelectChiFDR5KernelPCA6LinearDiscriminantAnalysisPrim7_2KBinsDiscretizerOneHot0f_classifPercentile9_6PCA_Randomized10_1BaggingClassifier11_8RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "UnivariateSelectChiFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with Chi-square. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          8
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RFE_GradientBoosting0IncrementalPCA1SGDClassifier2": {
    "id": "RFE_GradientBoosting0IncrementalPCA1SGDClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiKbest1IncrementalPCA2KNeighborsClassifierPrim3StandardScaler0mutual_info_classifKbest5_1KernelPCA6_1LinearDiscriminantAnalysisPrim7_1MaxAbsScaler0_1VarianceThreshold9_5KernelPCA10_1KNeighborsClassifierPrim11_6RandomForestMeta4_8_12": {
    "id": "MaxAbsScaler0UnivariateSelectChiKbest1IncrementalPCA2KNeighborsClassifierPrim3StandardScaler0mutual_info_classifKbest5_1KernelPCA6_1LinearDiscriminantAnalysisPrim7_1MaxAbsScaler0_1VarianceThreshold9_5KernelPCA10_1KNeighborsClassifierPrim11_6RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0mutual_info_classifKbest1TruncatedSVD2LogisticRegression3MinMaxScaler0_1UnivariateSelectChiFPR5TruncatedSVD6_3QuadraticDiscriminantAnalysis7QuantileTransformer0_1f_classifKbest9_6FastICA10_2BernoulliNBClassifier11_1RandomForestMeta4_8_12": {
    "id": "MaxAbsScaler0mutual_info_classifKbest1TruncatedSVD2LogisticRegression3MinMaxScaler0_1UnivariateSelectChiFPR5TruncatedSVD6_3QuadraticDiscriminantAnalysis7QuantileTransformer0_1f_classifKbest9_6FastICA10_2BernoulliNBClassifier11_1RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0MaxAbsScaler0UnivariateSelectChiFPR2PCA_Randomized3_1LogisticRegression4StandardScaler0_2f_classifFPR6FastICA7_3GaussianNBClassifier8_1RandomForestMeta5_9": {
    "id": "MaxAbsScaler0MaxAbsScaler0UnivariateSelectChiFPR2PCA_Randomized3_1LogisticRegression4StandardScaler0_2f_classifFPR6FastICA7_3GaussianNBClassifier8_1RandomForestMeta5_9",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          2
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          5,
          9
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifKbest1f_classifFWE0_2PCA_Randomized3_2LinearSVC4": {
    "id": "MaxAbsScaler0f_classifKbest1f_classifFWE0_2PCA_Randomized3_2LinearSVC4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          0,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3,
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0mutual_info_classifPercentile1PCA_Randomized2SVC3": {
    "id": "StandardScaler0mutual_info_classifPercentile1PCA_Randomized2SVC3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiKbest1PCA_ARPACK2PassiveAggressiveClassifier3MinMaxScaler0_1UnivariateSelectChiPercentile5_1PCA_Randomized6_3RidgeClassifierCV7_4QuantileTransformer0_5f_classifFWE9FastICA10_5LogisticRegressionCV11_4RandomForestMeta4_8_12": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiKbest1PCA_ARPACK2PassiveAggressiveClassifier3MinMaxScaler0_1UnivariateSelectChiPercentile5_1PCA_Randomized6_3RidgeClassifierCV7_4QuantileTransformer0_5f_classifFWE9FastICA10_5LogisticRegressionCV11_4RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          4
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifPercentile1IncrementalPCA2ExtraTreesClassifier3": {
    "id": "MaxAbsScaler0f_classifPercentile1IncrementalPCA2ExtraTreesClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      }
    }
  },
  "f_classifFDR0IncrementalPCA1RUSBoostClassifier2": {
    "id": "f_classifFDR0IncrementalPCA1RUSBoostClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0RFE_GradientBoosting1RandomTreesEmbedding2XGBClassifier3": {
    "id": "KBinsDiscretizerOneHot0RFE_GradientBoosting1RandomTreesEmbedding2XGBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifKbest1TruncatedSVD2GaussianProcessClassifierPrim3KBinsDiscretizerOrdinal0VarianceThreshold5_1TruncatedSVD6BernoulliNBClassifier7_4KBinsDiscretizerOneHot0f_classifFPR9_2PCA_LAPACK10_3LogisticRegression11_6MajorityVoting4_8_12": {
    "id": "KBinsDiscretizerOneHot0f_classifKbest1TruncatedSVD2GaussianProcessClassifierPrim3KBinsDiscretizerOrdinal0VarianceThreshold5_1TruncatedSVD6BernoulliNBClassifier7_4KBinsDiscretizerOneHot0f_classifFPR9_2PCA_LAPACK10_3LogisticRegression11_6MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifFPR1FastICA2ExtraTreesClassifier3": {
    "id": "KBinsDiscretizerOneHot0f_classifFPR1FastICA2ExtraTreesClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0VarianceThreshold1TruncatedSVD2GaussianNBClassifier3": {
    "id": "PowerTransformer0VarianceThreshold1TruncatedSVD2GaussianNBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifFWE1RidgeClassifierCV2MinMaxScaler0_1RFE_RandomForest4_1KernelPCA5EasyEnsembleClassifier6_3RandomForestMeta3_7": {
    "id": "KBinsDiscretizerOneHot0f_classifFWE1RidgeClassifierCV2MinMaxScaler0_1RFE_RandomForest4_1KernelPCA5EasyEnsembleClassifier6_3RandomForestMeta3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0mutual_info_classifPercentile1PCA_Randomized2EasyEnsembleClassifier3": {
    "id": "MinMaxScaler0mutual_info_classifPercentile1PCA_Randomized2EasyEnsembleClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0f_classifFPR1TruncatedSVD2DecisionTreeClassifier3StandardScaler0RFE_GradientBoosting5_1RandomTreesEmbedding6LogisticRegressionCV7_3RandomForestMeta4_8": {
    "id": "MinMaxScaler0f_classifFPR1TruncatedSVD2DecisionTreeClassifier3StandardScaler0RFE_GradientBoosting5_1RandomTreesEmbedding6LogisticRegressionCV7_3RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFWE1TruncatedSVD2SVC3Normalizer0_1f_classifFPR5_2PCA_ARPACK6_2EasyEnsembleClassifier7_1RandomForestMeta4_8": {
    "id": "RobustScaler0f_classifFWE1TruncatedSVD2SVC3Normalizer0_1f_classifFPR5_2PCA_ARPACK6_2EasyEnsembleClassifier7_1RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0RFE_GradientBoosting1IncrementalPCA2PassiveAggressiveClassifier3": {
    "id": "MaxAbsScaler0RFE_GradientBoosting1IncrementalPCA2PassiveAggressiveClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0f_classifFPR1FastICA2f_classifKbest0_2TruncatedSVD4QuadraticDiscriminantAnalysis5_2PowerTransformer0_1f_classifFWE7_4TruncatedSVD8_1NearestCentroid9_3RandomForestMeta6_10": {
    "id": "RobustScaler0f_classifFPR1FastICA2f_classifKbest0_2TruncatedSVD4QuadraticDiscriminantAnalysis5_2PowerTransformer0_1f_classifFWE7_4TruncatedSVD8_1NearestCentroid9_3RandomForestMeta6_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          6,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1PCA_LAPACK2GradientBoostingClassifier3": {
    "id": "StandardScaler0f_classifFWE1PCA_LAPACK2GradientBoostingClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0f_classifFPR1IncrementalPCA2SVC3": {
    "id": "Normalizer0f_classifFPR1IncrementalPCA2SVC3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      }
    }
  },
  "mutual_info_classifKbest0TruncatedSVD1ExtraTreesClassifier2": {
    "id": "mutual_info_classifKbest0TruncatedSVD1ExtraTreesClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      }
    }
  },
  "UnivariateSelectChiKbest0GradientBoostingClassifier1": {
    "id": "UnivariateSelectChiKbest0GradientBoostingClassifier1",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifFPR1PCA_Randomized2PassiveAggressiveClassifier3MinMaxScaler0_1UnivariateSelectChiPercentile5_2KernelPCA6_2BaggingClassifier7MajorityVoting4_8": {
    "id": "MaxAbsScaler0f_classifFPR1PCA_Randomized2PassiveAggressiveClassifier3MinMaxScaler0_1UnivariateSelectChiPercentile5_2KernelPCA6_2BaggingClassifier7MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0RFE_GradientBoosting1FastICA2RidgeClassifier3": {
    "id": "RobustScaler0RFE_GradientBoosting1FastICA2RidgeClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFPR1TruncatedSVD2MaxAbsScaler0mutual_info_classifKbest4KernelPCA5_1LGBMClassifier6_1Normalizer0_1VarianceThreshold8FastICA9_2LinearSVC10_3MajorityVoting7_11": {
    "id": "StandardScaler0f_classifFPR1TruncatedSVD2MaxAbsScaler0mutual_info_classifKbest4KernelPCA5_1LGBMClassifier6_1Normalizer0_1VarianceThreshold8FastICA9_2LinearSVC10_3MajorityVoting7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0mutual_info_classifPercentile1PCA_LAPACK2NearestCentroid3": {
    "id": "QuantileTransformer0mutual_info_classifPercentile1PCA_LAPACK2NearestCentroid3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifPercentile1PCA_Randomized2LinearSVC3": {
    "id": "StandardScaler0f_classifPercentile1PCA_Randomized2LinearSVC3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1PCA_ARPACK2SVC3StandardScaler0mutual_info_classifKbest5IncrementalPCA6_1RidgeClassifierCV7_4StandardScaler0_5f_classifFDR9_2PCA_ARPACK10_3SGDClassifier11_5MajorityVoting4_8_12": {
    "id": "StandardScaler0f_classifFWE1PCA_ARPACK2SVC3StandardScaler0mutual_info_classifKbest5IncrementalPCA6_1RidgeClassifierCV7_4StandardScaler0_5f_classifFDR9_2PCA_ARPACK10_3SGDClassifier11_5MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          5
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0f_classifFWE1IncrementalPCA2QuadraticDiscriminantAnalysis3": {
    "id": "MinMaxScaler0f_classifFWE1IncrementalPCA2QuadraticDiscriminantAnalysis3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiFWE1PCA_Randomized2DecisionTreeClassifier3": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiFWE1PCA_Randomized2DecisionTreeClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifFPR1VarianceThreshold0FastICA3_1AdaBoostClassifier4_1": {
    "id": "MaxAbsScaler0f_classifFPR1VarianceThreshold0FastICA3_1AdaBoostClassifier4_1",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifFPR1IncrementalPCA2QuadraticDiscriminantAnalysis3PowerTransformer0f_classifPercentile5RandomTreesEmbedding6EasyEnsembleClassifier7_2StandardScaler0mutual_info_classifKbest9_1PCA_ARPACK10_3NearestCentroid11_7MajorityVoting4_8_12": {
    "id": "MaxAbsScaler0f_classifFPR1IncrementalPCA2QuadraticDiscriminantAnalysis3PowerTransformer0f_classifPercentile5RandomTreesEmbedding6EasyEnsembleClassifier7_2StandardScaler0mutual_info_classifKbest9_1PCA_ARPACK10_3NearestCentroid11_7MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0FastICA1RidgeClassifier2MaxAbsScaler0_1UnivariateSelectChiFWE4IncrementalPCA5XGBClassifier6_3QuantileTransformer0_4VarianceThreshold8AdaBoostClassifier9_6MajorityVoting3_7_10": {
    "id": "KBinsDiscretizerOneHot0FastICA1RidgeClassifier2MaxAbsScaler0_1UnivariateSelectChiFWE4IncrementalPCA5XGBClassifier6_3QuantileTransformer0_4VarianceThreshold8AdaBoostClassifier9_6MajorityVoting3_7_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          3,
          7,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "UnivariateSelectChiPercentile0PCA_Randomized1AdaBoostClassifier2": {
    "id": "UnivariateSelectChiPercentile0PCA_Randomized1AdaBoostClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0mutual_info_classifPercentile1RUSBoostClassifier2PowerTransformer0RFE_GradientBoosting4PCA_ARPACK5_1LogisticRegressionCV6PowerTransformer0_4VarianceThreshold8_4KernelPCA9_2LGBMClassifier10_6RandomForestMeta3_7_11": {
    "id": "RobustScaler0mutual_info_classifPercentile1RUSBoostClassifier2PowerTransformer0RFE_GradientBoosting4PCA_ARPACK5_1LogisticRegressionCV6PowerTransformer0_4VarianceThreshold8_4KernelPCA9_2LGBMClassifier10_6RandomForestMeta3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0VarianceThreshold1PCA_ARPACK2RidgeClassifierCV3MinMaxScaler0RFE_RandomForest5PCA_LAPACK6_2NearestCentroid7_4RandomForestMeta4_8": {
    "id": "StandardScaler0VarianceThreshold1PCA_ARPACK2RidgeClassifierCV3MinMaxScaler0RFE_RandomForest5PCA_LAPACK6_2NearestCentroid7_4RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifFPR1PCA_ARPACK2SGDClassifier3MinMaxScaler0_1f_classifFDR5_2KernelPCA6_2RidgeClassifierCV7_1MajorityVoting4_8": {
    "id": "KBinsDiscretizerOneHot0f_classifFPR1PCA_ARPACK2SGDClassifier3MinMaxScaler0_1f_classifFDR5_2KernelPCA6_2RidgeClassifierCV7_1MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "UnivariateSelectChiFDR0KernelPCA1SGDClassifier2MaxAbsScaler0UnivariateSelectChiFWE4PCA_ARPACK5_1PassiveAggressiveClassifier6_2MajorityVoting3_7": {
    "id": "UnivariateSelectChiFDR0KernelPCA1SGDClassifier2MaxAbsScaler0UnivariateSelectChiFWE4PCA_ARPACK5_1PassiveAggressiveClassifier6_2MajorityVoting3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "UnivariateSelectChiFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with Chi-square. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "f_classifFWE0PCA_LAPACK1LogisticRegression2": {
    "id": "f_classifFWE0PCA_LAPACK1LogisticRegression2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifKbest1AdaBoostClassifier2StandardScaler0_1KernelPCA4SVC5_1PowerTransformer0_1mutual_info_classifPercentile7_2FastICA8_2LGBMClassifier9RandomForestMeta3_6_10": {
    "id": "KBinsDiscretizerOneHot0f_classifKbest1AdaBoostClassifier2StandardScaler0_1KernelPCA4SVC5_1PowerTransformer0_1mutual_info_classifPercentile7_2FastICA8_2LGBMClassifier9RandomForestMeta3_6_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          3,
          6,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiPercentile1KernelPCA2RidgeClassifier3f_classifFWE0_2PCA_ARPACK5DecisionTreeClassifier6_1KBinsDiscretizerOrdinal0f_classifFWE8_5FastICA9_2LGBMClassifier10_2RandomForestMeta4_7_11": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiPercentile1KernelPCA2RidgeClassifier3f_classifFWE0_2PCA_ARPACK5DecisionTreeClassifier6_1KBinsDiscretizerOrdinal0f_classifFWE8_5FastICA9_2LGBMClassifier10_2RandomForestMeta4_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          5
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFWE1IncrementalPCA2LogisticRegression3KBinsDiscretizerOneHot0UnivariateSelectChiFWE5PCA_ARPACK6_2RUSBoostClassifier7_4RandomForestMeta4_8": {
    "id": "RobustScaler0f_classifFWE1IncrementalPCA2LogisticRegression3KBinsDiscretizerOneHot0UnivariateSelectChiFWE5PCA_ARPACK6_2RUSBoostClassifier7_4RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "mutual_info_classifPercentile0FastICA1GaussianProcessClassifierPrim2": {
    "id": "mutual_info_classifPercentile0FastICA1GaussianProcessClassifierPrim2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiFDR1PCA_LAPACK2XGBClassifier3": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiFDR1PCA_LAPACK2XGBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with Chi-square. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0mutual_info_classifPercentile1IncrementalPCA2QuadraticDiscriminantAnalysis3QuantileTransformer0_1f_classifFPR5_2PCA_Randomized6XGBClassifier7PowerTransformer0_5f_classifFWE9_5PCA_LAPACK10NearestCentroid11_2RandomForestMeta4_8_12": {
    "id": "MaxAbsScaler0mutual_info_classifPercentile1IncrementalPCA2QuadraticDiscriminantAnalysis3QuantileTransformer0_1f_classifFPR5_2PCA_Randomized6XGBClassifier7PowerTransformer0_5f_classifFWE9_5PCA_LAPACK10NearestCentroid11_2RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          2
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiKbest1PCA_LAPACK2SVC3": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiKbest1PCA_LAPACK2SVC3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0UnivariateSelectChiFDR1PCA_ARPACK2NearestCentroid3": {
    "id": "MinMaxScaler0UnivariateSelectChiFDR1PCA_ARPACK2NearestCentroid3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with Chi-square. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiFPR1FastICA2QuadraticDiscriminantAnalysis3QuantileTransformer0_1mutual_info_classifKbest5_2PCA_LAPACK6_2RidgeClassifierCV7_4MaxAbsScaler0_5RFE_GradientBoosting9_1TruncatedSVD10_2GradientBoostingClassifier11_8MajorityVoting4_8_12": {
    "id": "MaxAbsScaler0UnivariateSelectChiFPR1FastICA2QuadraticDiscriminantAnalysis3QuantileTransformer0_1mutual_info_classifKbest5_2PCA_LAPACK6_2RidgeClassifierCV7_4MaxAbsScaler0_5RFE_GradientBoosting9_1TruncatedSVD10_2GradientBoostingClassifier11_8MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          8
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifFWE1IncrementalPCA2GaussianProcessClassifierPrim3": {
    "id": "MaxAbsScaler0f_classifFWE1IncrementalPCA2GaussianProcessClassifierPrim3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      }
    }
  },
  "VarianceThreshold0ComplementNBClassifier1": {
    "id": "VarianceThreshold0ComplementNBClassifier1",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "ComplementNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "The Complement Naive Bayes classifier described in Rennie et al. (2003). The Complement Naive Bayes classifier was designed to correct the \u201csevere assumptions\u201d made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.",
          "type": "Classifier"
        }
      }
    }
  },
  "RFE_RandomForest0BalancedRandomForestClassifier1QuantileTransformer0UnivariateSelectChiKbest3_1FastICA4_1RF_classifier5RandomForestMeta2_6": {
    "id": "RFE_RandomForest0BalancedRandomForestClassifier1QuantileTransformer0UnivariateSelectChiKbest3_1FastICA4_1RF_classifier5RandomForestMeta2_6",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3,
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          2,
          6
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0UnivariateSelectChiKbest1PCA_LAPACK2SVC3KBinsDiscretizerOneHot0_1RFE_GradientBoosting5_2PCA_LAPACK6_2SVC7KBinsDiscretizerOneHot0f_classifFPR9_1PCA_Randomized10_7BalancedRandomForestClassifier11_7MajorityVoting4_8_12": {
    "id": "MinMaxScaler0UnivariateSelectChiKbest1PCA_LAPACK2SVC3KBinsDiscretizerOneHot0_1RFE_GradientBoosting5_2PCA_LAPACK6_2SVC7KBinsDiscretizerOneHot0f_classifFPR9_1PCA_Randomized10_7BalancedRandomForestClassifier11_7MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0f_classifFWE1FastICA2PassiveAggressiveClassifier3": {
    "id": "QuantileTransformer0f_classifFWE1FastICA2PassiveAggressiveClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0f_classifFWE1PCA_Randomized2ExtraTreesClassifier3": {
    "id": "RobustScaler0f_classifFWE1PCA_Randomized2ExtraTreesClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0TruncatedSVD1GaussianNBClassifier2": {
    "id": "KBinsDiscretizerOneHot0TruncatedSVD1GaussianNBClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0VarianceThreshold1GaussianProcessClassifierPrim2QuantileTransformer0mutual_info_classifPercentile4PCA_Randomized5_1BaggingClassifier6_3RandomForestMeta3_7": {
    "id": "StandardScaler0VarianceThreshold1GaussianProcessClassifierPrim2QuantileTransformer0mutual_info_classifPercentile4PCA_Randomized5_1BaggingClassifier6_3RandomForestMeta3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiFPR1IncrementalPCA2EasyEnsembleClassifier3MaxAbsScaler0f_classifKbest5TruncatedSVD6_1RUSBoostClassifier7_3MajorityVoting4_8": {
    "id": "MaxAbsScaler0UnivariateSelectChiFPR1IncrementalPCA2EasyEnsembleClassifier3MaxAbsScaler0f_classifKbest5TruncatedSVD6_1RUSBoostClassifier7_3MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0mutual_info_classifKbest1TruncatedSVD2StandardScaler0mutual_info_classifKbest4FastICA5_2DecisionTreeClassifier6_1MinMaxScaler0_4UnivariateSelectChiFDR8KernelPCA9_4BaggingClassifier10_2RandomForestMeta7_11_3": {
    "id": "StandardScaler0mutual_info_classifKbest1TruncatedSVD2StandardScaler0mutual_info_classifKbest4FastICA5_2DecisionTreeClassifier6_1MinMaxScaler0_4UnivariateSelectChiFDR8KernelPCA9_4BaggingClassifier10_2RandomForestMeta7_11_3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "UnivariateSelectChiFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with Chi-square. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          4
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          7,
          11,
          3
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiFWE1KBinsDiscretizerOneHot0_1f_classifPercentile3KernelPCA4BalancedRandomForestClassifier5StandardScaler0f_classifKbest7PCA_LAPACK8_2LogisticRegression9_6MajorityVoting6_10": {
    "id": "MaxAbsScaler0UnivariateSelectChiFWE1KBinsDiscretizerOneHot0_1f_classifPercentile3KernelPCA4BalancedRandomForestClassifier5StandardScaler0f_classifKbest7PCA_LAPACK8_2LogisticRegression9_6MajorityVoting6_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          6,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0UnivariateSelectChiFPR1IncrementalPCA2PassiveAggressiveClassifier3KBinsDiscretizerOrdinal0UnivariateSelectChiFPR5KernelPCA6_1GradientBoostingClassifier7_2StandardScaler0_1mutual_info_classifPercentile9_2PCA_LAPACK10_6RidgeClassifierCV11_4RandomForestMeta4_8_12": {
    "id": "MinMaxScaler0UnivariateSelectChiFPR1IncrementalPCA2PassiveAggressiveClassifier3KBinsDiscretizerOrdinal0UnivariateSelectChiFPR5KernelPCA6_1GradientBoostingClassifier7_2StandardScaler0_1mutual_info_classifPercentile9_2PCA_LAPACK10_6RidgeClassifierCV11_4RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          4
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0mutual_info_classifPercentile1PCA_ARPACK2LogisticRegression3": {
    "id": "KBinsDiscretizerOneHot0mutual_info_classifPercentile1PCA_ARPACK2LogisticRegression3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0RFE_GradientBoosting1PCA_LAPACK2SVC3": {
    "id": "MaxAbsScaler0RFE_GradientBoosting1PCA_LAPACK2SVC3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      }
    }
  },
  "f_classifFWE0ComplementNBClassifier1KBinsDiscretizerOrdinal0UnivariateSelectChiPercentile3FastICA4SVC5MajorityVoting2_6": {
    "id": "f_classifFWE0ComplementNBClassifier1KBinsDiscretizerOrdinal0UnivariateSelectChiPercentile3FastICA4SVC5MajorityVoting2_6",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "ComplementNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "The Complement Naive Bayes classifier described in Rennie et al. (2003). The Complement Naive Bayes classifier was designed to correct the \u201csevere assumptions\u201d made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.",
          "type": "Classifier"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          2,
          6
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0RFE_GradientBoosting1IncrementalPCA2KNeighborsClassifierPrim3KBinsDiscretizerOneHot0_1f_classifKbest5_2PCA_Randomized6_1LinearSVC7_3MajorityVoting4_8": {
    "id": "StandardScaler0RFE_GradientBoosting1IncrementalPCA2KNeighborsClassifierPrim3KBinsDiscretizerOneHot0_1f_classifKbest5_2PCA_Randomized6_1LinearSVC7_3MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0RFE_RandomForest1IncrementalPCA2GaussianProcessClassifierPrim3KBinsDiscretizerOrdinal0RFE_GradientBoosting5PCA_Randomized6_1PassiveAggressiveClassifier7_2MajorityVoting4_8": {
    "id": "KBinsDiscretizerOneHot0RFE_RandomForest1IncrementalPCA2GaussianProcessClassifierPrim3KBinsDiscretizerOrdinal0RFE_GradientBoosting5PCA_Randomized6_1PassiveAggressiveClassifier7_2MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0f_classifFDR1FastICA2RandomForestMeta3": {
    "id": "StandardScaler0f_classifFDR1FastICA2RandomForestMeta3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "f_classifFWE0PCA_ARPACK1ExtraTreesClassifier2MaxAbsScaler0f_classifFPR4_1FastICA5_2LGBMClassifier6RandomForestMeta3_7": {
    "id": "f_classifFWE0PCA_ARPACK1ExtraTreesClassifier2MaxAbsScaler0f_classifFPR4_1FastICA5_2LGBMClassifier6RandomForestMeta3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "IncrementalPCA0RUSBoostClassifier1": {
    "id": "IncrementalPCA0RUSBoostClassifier1",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0UnivariateSelectChiPercentile1RandomTreesEmbedding2KNeighborsClassifierPrim3": {
    "id": "QuantileTransformer0UnivariateSelectChiPercentile1RandomTreesEmbedding2KNeighborsClassifierPrim3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0RFE_GradientBoosting1SGDClassifier2": {
    "id": "QuantileTransformer0RFE_GradientBoosting1SGDClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0VarianceThreshold1IncrementalPCA2XGBClassifier3MinMaxScaler0mutual_info_classifPercentile5_2PCA_ARPACK6_2DecisionTreeClassifier7MajorityVoting4_8": {
    "id": "StandardScaler0VarianceThreshold1IncrementalPCA2XGBClassifier3MinMaxScaler0mutual_info_classifPercentile5_2PCA_ARPACK6_2DecisionTreeClassifier7MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0RFE_RandomForest1PCA_ARPACK2QuadraticDiscriminantAnalysis3": {
    "id": "QuantileTransformer0RFE_RandomForest1PCA_ARPACK2QuadraticDiscriminantAnalysis3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiKbest1TruncatedSVD2ExtraTreesClassifier3": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiKbest1TruncatedSVD2ExtraTreesClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifFDR1TruncatedSVD2RidgeClassifierCV3PowerTransformer0_1mutual_info_classifKbest5PCA_Randomized6_2AdaBoostClassifier7_2RandomForestMeta4_8": {
    "id": "KBinsDiscretizerOneHot0f_classifFDR1TruncatedSVD2RidgeClassifierCV3PowerTransformer0_1mutual_info_classifKbest5PCA_Randomized6_2AdaBoostClassifier7_2RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0f_classifFPR1TruncatedSVD2LogisticRegressionCV3StandardScaler0f_classifFPR5TruncatedSVD6_2BaggingClassifier7_3RandomForestMeta4_8": {
    "id": "StandardScaler0f_classifFPR1TruncatedSVD2LogisticRegressionCV3StandardScaler0f_classifFPR5TruncatedSVD6_2BaggingClassifier7_3RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOrdinal0VarianceThreshold1FastICA2AdaBoostClassifier3": {
    "id": "KBinsDiscretizerOrdinal0VarianceThreshold1FastICA2AdaBoostClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      }
    }
  },
  "f_classifFWE0PassiveAggressiveClassifier1PowerTransformer0VarianceThreshold3IncrementalPCA4ExtraTreesClassifier5_1mutual_info_classifKbest0_1RandomTreesEmbedding7SGDClassifier8RandomForestMeta2_6_9": {
    "id": "f_classifFWE0PassiveAggressiveClassifier1PowerTransformer0VarianceThreshold3IncrementalPCA4ExtraTreesClassifier5_1mutual_info_classifKbest0_1RandomTreesEmbedding7SGDClassifier8RandomForestMeta2_6_9",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          2,
          6,
          9
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFWE1IncrementalPCA2QuadraticDiscriminantAnalysis3StandardScaler0_1f_classifFWE5PCA_Randomized6_3EasyEnsembleClassifier7_2MajorityVoting4_8": {
    "id": "RobustScaler0f_classifFWE1IncrementalPCA2QuadraticDiscriminantAnalysis3StandardScaler0_1f_classifFWE5PCA_Randomized6_3EasyEnsembleClassifier7_2MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifPercentile1PCA_LAPACK2KNeighborsClassifierPrim3StandardScaler0f_classifKbest5RandomTreesEmbedding6_3AdaBoostClassifier7_1KBinsDiscretizerOneHot0_1mutual_info_classifPercentile9_1PCA_Randomized10_3LinearDiscriminantAnalysisPrim11RandomForestMeta4_8_12": {
    "id": "MaxAbsScaler0f_classifPercentile1PCA_LAPACK2KNeighborsClassifierPrim3StandardScaler0f_classifKbest5RandomTreesEmbedding6_3AdaBoostClassifier7_1KBinsDiscretizerOneHot0_1mutual_info_classifPercentile9_1PCA_Randomized10_3LinearDiscriminantAnalysisPrim11RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "UnivariateSelectChiKbest0IncrementalPCA1DecisionTreeClassifier2": {
    "id": "UnivariateSelectChiKbest0IncrementalPCA1DecisionTreeClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifPercentile1RandomTreesEmbedding2BernoulliNBClassifier3": {
    "id": "StandardScaler0f_classifPercentile1RandomTreesEmbedding2BernoulliNBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiFWE1IncrementalPCA2QuadraticDiscriminantAnalysis3": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiFWE1IncrementalPCA2QuadraticDiscriminantAnalysis3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifPercentile1TruncatedSVD2LinearSVC3f_classifPercentile0KernelPCA5_3BaggingClassifier6_1MinMaxScaler0_1RFE_RandomForest8_1KernelPCA9_1RidgeClassifierCV10_3RandomForestMeta4_7_11": {
    "id": "KBinsDiscretizerOneHot0f_classifPercentile1TruncatedSVD2LinearSVC3f_classifPercentile0KernelPCA5_3BaggingClassifier6_1MinMaxScaler0_1RFE_RandomForest8_1KernelPCA9_1RidgeClassifierCV10_3RandomForestMeta4_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          3
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0RFE_GradientBoosting1IncrementalPCA2GaussianNBClassifier3": {
    "id": "StandardScaler0RFE_GradientBoosting1IncrementalPCA2GaussianNBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0IncrementalPCA1GaussianProcessClassifierPrim2": {
    "id": "MinMaxScaler0IncrementalPCA1GaussianProcessClassifierPrim2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0UnivariateSelectChiKbest1FastICA2PassiveAggressiveClassifier3": {
    "id": "MinMaxScaler0UnivariateSelectChiKbest1FastICA2PassiveAggressiveClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifFWE1GradientBoostingClassifier2KBinsDiscretizerOrdinal0mutual_info_classifKbest4_1PCA_ARPACK5_2XGBClassifier6RobustScaler0_1f_classifFPR8_2TruncatedSVD9_6LogisticRegressionCV10_2RandomForestMeta3_7_11": {
    "id": "KBinsDiscretizerOneHot0f_classifFWE1GradientBoostingClassifier2KBinsDiscretizerOrdinal0mutual_info_classifKbest4_1PCA_ARPACK5_2XGBClassifier6RobustScaler0_1f_classifFPR8_2TruncatedSVD9_6LogisticRegressionCV10_2RandomForestMeta3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFPR1BaggingClassifier2PowerTransformer0_1RFE_GradientBoosting4PCA_LAPACK5_1LogisticRegressionCV6_3MaxAbsScaler0f_classifFDR8_5IncrementalPCA9_2LogisticRegression10_5MajorityVoting3_7_11": {
    "id": "RobustScaler0f_classifFPR1BaggingClassifier2PowerTransformer0_1RFE_GradientBoosting4PCA_LAPACK5_1LogisticRegressionCV6_3MaxAbsScaler0f_classifFDR8_5IncrementalPCA9_2LogisticRegression10_5MajorityVoting3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          5
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "UnivariateSelectChiKbest0IncrementalPCA1GradientBoostingClassifier2": {
    "id": "UnivariateSelectChiKbest0IncrementalPCA1GradientBoostingClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOrdinal0f_classifFWE1IncrementalPCA2ExtraTreesClassifier3": {
    "id": "KBinsDiscretizerOrdinal0f_classifFWE1IncrementalPCA2ExtraTreesClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifFPR1IncrementalPCA2BernoulliNBClassifier3Normalizer0UnivariateSelectChiFWE5_1TruncatedSVD6KNeighborsClassifierPrim7_1RandomForestMeta4_8": {
    "id": "MaxAbsScaler0f_classifFPR1IncrementalPCA2BernoulliNBClassifier3Normalizer0UnivariateSelectChiFWE5_1TruncatedSVD6KNeighborsClassifierPrim7_1RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "UnivariateSelectChiKbest0DecisionTreeClassifier1": {
    "id": "UnivariateSelectChiKbest0DecisionTreeClassifier1",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0f_classifFPR1TruncatedSVD2KNeighborsClassifierPrim3MaxAbsScaler0_1f_classifPercentile5_2PCA_ARPACK6_1BernoulliNBClassifier7_1RandomForestMeta4_8": {
    "id": "PowerTransformer0f_classifFPR1TruncatedSVD2KNeighborsClassifierPrim3MaxAbsScaler0_1f_classifPercentile5_2PCA_ARPACK6_1BernoulliNBClassifier7_1RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0RFE_GradientBoosting1PCA_Randomized2PassiveAggressiveClassifier3QuantileTransformer0_1f_classifFPR5RandomTreesEmbedding6LinearSVC7_2MajorityVoting4_8": {
    "id": "RobustScaler0RFE_GradientBoosting1PCA_Randomized2PassiveAggressiveClassifier3QuantileTransformer0_1f_classifFPR5RandomTreesEmbedding6LinearSVC7_2MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFPR1PCA_Randomized2AdaBoostClassifier3": {
    "id": "RobustScaler0f_classifFPR1PCA_Randomized2AdaBoostClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiFPR1PCA_LAPACK2QuadraticDiscriminantAnalysis3MinMaxScaler0_1f_classifFPR5_1PCA_ARPACK6BaggingClassifier7RandomForestMeta4_8": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiFPR1PCA_LAPACK2QuadraticDiscriminantAnalysis3MinMaxScaler0_1f_classifFPR5_1PCA_ARPACK6BaggingClassifier7RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "PowerTransformer0f_classifFWE1PCA_ARPACK2PassiveAggressiveClassifier3KBinsDiscretizerOneHot0UnivariateSelectChiPercentile5PCA_ARPACK6_1PassiveAggressiveClassifier7_2mutual_info_classifPercentile0_2PCA_LAPACK9_3LinearDiscriminantAnalysisPrim10_1MajorityVoting4_8_11": {
    "id": "PowerTransformer0f_classifFWE1PCA_ARPACK2PassiveAggressiveClassifier3KBinsDiscretizerOneHot0UnivariateSelectChiPercentile5PCA_ARPACK6_1PassiveAggressiveClassifier7_2mutual_info_classifPercentile0_2PCA_LAPACK9_3LinearDiscriminantAnalysisPrim10_1MajorityVoting4_8_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          8,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0mutual_info_classifKbest1IncrementalPCA2EasyEnsembleClassifier3PowerTransformer0_1RFE_RandomForest5_1TruncatedSVD6_1EasyEnsembleClassifier7Normalizer0_5RFE_GradientBoosting9_2PCA_LAPACK10_3LogisticRegressionCV11_1MajorityVoting4_8_12": {
    "id": "QuantileTransformer0mutual_info_classifKbest1IncrementalPCA2EasyEnsembleClassifier3PowerTransformer0_1RFE_RandomForest5_1TruncatedSVD6_1EasyEnsembleClassifier7Normalizer0_5RFE_GradientBoosting9_2PCA_LAPACK10_3LogisticRegressionCV11_1MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0f_classifFWE1RandomTreesEmbedding2EasyEnsembleClassifier3": {
    "id": "QuantileTransformer0f_classifFWE1RandomTreesEmbedding2EasyEnsembleClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1RidgeClassifier2": {
    "id": "StandardScaler0f_classifFWE1RidgeClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOrdinal0f_classifFPR1PCA_LAPACK2RUSBoostClassifier3StandardScaler0_1f_classifFDR5_1FastICA6_3RF_classifier7_3MaxAbsScaler0_1RFE_GradientBoosting9_5FastICA10_5EasyEnsembleClassifier11_8MajorityVoting4_8_12": {
    "id": "KBinsDiscretizerOrdinal0f_classifFPR1PCA_LAPACK2RUSBoostClassifier3StandardScaler0_1f_classifFDR5_1FastICA6_3RF_classifier7_3MaxAbsScaler0_1RFE_GradientBoosting9_5FastICA10_5EasyEnsembleClassifier11_8MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          8
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "UnivariateSelectChiPercentile0LogisticRegression1": {
    "id": "UnivariateSelectChiPercentile0LogisticRegression1",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "f_classifPercentile0PCA_Randomized1SGDClassifier2KBinsDiscretizerOneHot0UnivariateSelectChiKbest4_1TruncatedSVD5_2AdaBoostClassifier6_2MajorityVoting3_7": {
    "id": "f_classifPercentile0PCA_Randomized1SGDClassifier2KBinsDiscretizerOneHot0UnivariateSelectChiKbest4_1TruncatedSVD5_2AdaBoostClassifier6_2MajorityVoting3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1PCA_ARPACK2QuadraticDiscriminantAnalysis3": {
    "id": "StandardScaler0f_classifFWE1PCA_ARPACK2QuadraticDiscriminantAnalysis3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFPR1LGBMClassifier2": {
    "id": "StandardScaler0f_classifFPR1LGBMClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0VarianceThreshold1SGDClassifier2KBinsDiscretizerOneHot0_1RFE_GradientBoosting4IncrementalPCA5_1LinearSVC6QuantileTransformer0f_classifKbest8_2PCA_Randomized9_5RidgeClassifier10_7MajorityVoting3_7_11": {
    "id": "MinMaxScaler0VarianceThreshold1SGDClassifier2KBinsDiscretizerOneHot0_1RFE_GradientBoosting4IncrementalPCA5_1LinearSVC6QuantileTransformer0f_classifKbest8_2PCA_Randomized9_5RidgeClassifier10_7MajorityVoting3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiFWE1PCA_Randomized2SGDClassifier3": {
    "id": "MaxAbsScaler0UnivariateSelectChiFWE1PCA_Randomized2SGDClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0RFE_GradientBoosting1PCA_ARPACK2LogisticRegression3": {
    "id": "StandardScaler0RFE_GradientBoosting1PCA_ARPACK2LogisticRegression3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifFPR1RandomTreesEmbedding2LGBMClassifier3StandardScaler0mutual_info_classifKbest5_2RandomTreesEmbedding6LogisticRegression7_2KBinsDiscretizerOneHot0mutual_info_classifKbest9_2TruncatedSVD10_7LinearDiscriminantAnalysisPrim11_1MajorityVoting4_8_12": {
    "id": "MaxAbsScaler0f_classifFPR1RandomTreesEmbedding2LGBMClassifier3StandardScaler0mutual_info_classifKbest5_2RandomTreesEmbedding6LogisticRegression7_2KBinsDiscretizerOneHot0mutual_info_classifKbest9_2TruncatedSVD10_7LinearDiscriminantAnalysisPrim11_1MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0RFE_GradientBoosting1TruncatedSVD2PassiveAggressiveClassifier3RobustScaler0mutual_info_classifKbest5_2RandomTreesEmbedding6_2XGBClassifier7_4MaxAbsScaler0_1VarianceThreshold9RandomTreesEmbedding10_7KNeighborsClassifierPrim11_3RandomForestMeta4_8_12": {
    "id": "MaxAbsScaler0RFE_GradientBoosting1TruncatedSVD2PassiveAggressiveClassifier3RobustScaler0mutual_info_classifKbest5_2RandomTreesEmbedding6_2XGBClassifier7_4MaxAbsScaler0_1VarianceThreshold9RandomTreesEmbedding10_7KNeighborsClassifierPrim11_3RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0mutual_info_classifKbest1IncrementalPCA2ExtraTreesClassifier3": {
    "id": "MaxAbsScaler0mutual_info_classifKbest1IncrementalPCA2ExtraTreesClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifFWE1PCA_LAPACK2RidgeClassifier3": {
    "id": "MaxAbsScaler0f_classifFWE1PCA_LAPACK2RidgeClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0mutual_info_classifPercentile1SVC2": {
    "id": "RobustScaler0mutual_info_classifPercentile1SVC2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0mutual_info_classifPercentile1FastICA2RUSBoostClassifier3": {
    "id": "RobustScaler0mutual_info_classifPercentile1FastICA2RUSBoostClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiPercentile1TruncatedSVD2GaussianProcessClassifierPrim3StandardScaler0_1mutual_info_classifPercentile5_2TruncatedSVD6GaussianNBClassifier7_3StandardScaler0_5f_classifFPR9_1KernelPCA10_5LinearDiscriminantAnalysisPrim11_5RandomForestMeta4_8_12": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiPercentile1TruncatedSVD2GaussianProcessClassifierPrim3StandardScaler0_1mutual_info_classifPercentile5_2TruncatedSVD6GaussianNBClassifier7_3StandardScaler0_5f_classifFPR9_1KernelPCA10_5LinearDiscriminantAnalysisPrim11_5RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          5
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1GaussianNBClassifier2": {
    "id": "StandardScaler0f_classifFWE1GaussianNBClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0RFE_RandomForest1KernelPCA2EasyEnsembleClassifier3PowerTransformer0f_classifFDR5_2KernelPCA6_3LogisticRegressionCV7_1RandomForestMeta4_8": {
    "id": "StandardScaler0RFE_RandomForest1KernelPCA2EasyEnsembleClassifier3PowerTransformer0f_classifFDR5_2KernelPCA6_3LogisticRegressionCV7_1RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0mutual_info_classifPercentile1KernelPCA2RidgeClassifierCV3": {
    "id": "KBinsDiscretizerOneHot0mutual_info_classifPercentile1KernelPCA2RidgeClassifierCV3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      }
    }
  },
  "f_classifPercentile0IncrementalPCA1PassiveAggressiveClassifier2": {
    "id": "f_classifPercentile0IncrementalPCA1PassiveAggressiveClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      }
    }
  },
  "f_classifFPR0IncrementalPCA1PassiveAggressiveClassifier2MaxAbsScaler0IncrementalPCA4_2AdaBoostClassifier5_3KBinsDiscretizerOrdinal0_4UnivariateSelectChiFPR7_1FastICA8_5RidgeClassifier9MajorityVoting3_6_10": {
    "id": "f_classifFPR0IncrementalPCA1PassiveAggressiveClassifier2MaxAbsScaler0IncrementalPCA4_2AdaBoostClassifier5_3KBinsDiscretizerOrdinal0_4UnivariateSelectChiFPR7_1FastICA8_5RidgeClassifier9MajorityVoting3_6_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          3
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          5
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          3,
          6,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0mutual_info_classifKbest1IncrementalPCA2GradientBoostingClassifier3": {
    "id": "MinMaxScaler0mutual_info_classifKbest1IncrementalPCA2GradientBoostingClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0VarianceThreshold1FastICA2LogisticRegressionCV3": {
    "id": "StandardScaler0VarianceThreshold1FastICA2LogisticRegressionCV3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0mutual_info_classifPercentile1GaussianProcessClassifierPrim2": {
    "id": "StandardScaler0mutual_info_classifPercentile1GaussianProcessClassifierPrim2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifKbest1IncrementalPCA2NearestCentroid3": {
    "id": "StandardScaler0f_classifKbest1IncrementalPCA2NearestCentroid3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0f_classifPercentile1LinearSVC2KBinsDiscretizerOneHot0_1f_classifFWE4PCA_Randomized5_1BernoulliNBClassifier6MajorityVoting3_7": {
    "id": "Normalizer0f_classifPercentile1LinearSVC2KBinsDiscretizerOneHot0_1f_classifFWE4PCA_Randomized5_1BernoulliNBClassifier6MajorityVoting3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiKbest1IncrementalPCA2LogisticRegression3Normalizer0f_classifFWE5PCA_Randomized6_1BalancedRandomForestClassifier7RandomForestMeta4_8": {
    "id": "MaxAbsScaler0UnivariateSelectChiKbest1IncrementalPCA2LogisticRegression3Normalizer0f_classifFWE5PCA_Randomized6_1BalancedRandomForestClassifier7RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0RFE_RandomForest1IncrementalPCA2LogisticRegression3": {
    "id": "StandardScaler0RFE_RandomForest1IncrementalPCA2LogisticRegression3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifFWE1TruncatedSVD2RUSBoostClassifier3": {
    "id": "KBinsDiscretizerOneHot0f_classifFWE1TruncatedSVD2RUSBoostClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifPercentile1IncrementalPCA2XGBClassifier3MaxAbsScaler0_1mutual_info_classifPercentile5_1TruncatedSVD6_1BaggingClassifier7_1RandomForestMeta4_8": {
    "id": "StandardScaler0f_classifPercentile1IncrementalPCA2XGBClassifier3MaxAbsScaler0_1mutual_info_classifPercentile5_1TruncatedSVD6_1BaggingClassifier7_1RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "UnivariateSelectChiFWE0RandomTreesEmbedding1KNeighborsClassifierPrim2": {
    "id": "UnivariateSelectChiFWE0RandomTreesEmbedding1KNeighborsClassifierPrim2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0VarianceThreshold1PCA_Randomized2BalancedRandomForestClassifier3": {
    "id": "RobustScaler0VarianceThreshold1PCA_Randomized2BalancedRandomForestClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0f_classifFPR1PCA_ARPACK2LogisticRegressionCV3QuantileTransformer0_1UnivariateSelectChiPercentile5PCA_LAPACK6_1RF_classifier7_1Normalizer0_1f_classifPercentile9FastICA10_1KNeighborsClassifierPrim11_1MajorityVoting4_8_12": {
    "id": "RobustScaler0f_classifFPR1PCA_ARPACK2LogisticRegressionCV3QuantileTransformer0_1UnivariateSelectChiPercentile5PCA_LAPACK6_1RF_classifier7_1Normalizer0_1f_classifPercentile9FastICA10_1KNeighborsClassifierPrim11_1MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "UnivariateSelectChiFPR0IncrementalPCA1LogisticRegressionCV2": {
    "id": "UnivariateSelectChiFPR0IncrementalPCA1LogisticRegressionCV2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0mutual_info_classifKbest1KernelPCA2AdaBoostClassifier3": {
    "id": "StandardScaler0mutual_info_classifKbest1KernelPCA2AdaBoostClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      }
    }
  },
  "UnivariateSelectChiKbest0RandomTreesEmbedding1AdaBoostClassifier2PowerTransformer0RFE_RandomForest4FastICA5_2LinearSVC6MajorityVoting3_7": {
    "id": "UnivariateSelectChiKbest0RandomTreesEmbedding1AdaBoostClassifier2PowerTransformer0RFE_RandomForest4FastICA5_2LinearSVC6MajorityVoting3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifPercentile1FastICA2GaussianProcessClassifierPrim3KBinsDiscretizerOneHot0UnivariateSelectChiPercentile5PCA_ARPACK6_3LinearSVC7_3MinMaxScaler0_5f_classifFDR9RandomTreesEmbedding10_3GradientBoostingClassifier11_1MajorityVoting4_8_12": {
    "id": "RobustScaler0f_classifPercentile1FastICA2GaussianProcessClassifierPrim3KBinsDiscretizerOneHot0UnivariateSelectChiPercentile5PCA_ARPACK6_3LinearSVC7_3MinMaxScaler0_5f_classifFDR9RandomTreesEmbedding10_3GradientBoostingClassifier11_1MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "Normalizer0mutual_info_classifKbest1TruncatedSVD2BalancedRandomForestClassifier3": {
    "id": "Normalizer0mutual_info_classifKbest1TruncatedSVD2BalancedRandomForestClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiFPR1TruncatedSVD2NearestCentroid3": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiFPR1TruncatedSVD2NearestCentroid3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0RFE_GradientBoosting1NearestCentroid2PowerTransformer0RFE_RandomForest4RandomTreesEmbedding5_1KBinsDiscretizerOneHot0_4f_classifFWE7_4PCA_ARPACK8_2RF_classifier9RandomForestMeta3_10_6": {
    "id": "KBinsDiscretizerOneHot0RFE_GradientBoosting1NearestCentroid2PowerTransformer0RFE_RandomForest4RandomTreesEmbedding5_1KBinsDiscretizerOneHot0_4f_classifFWE7_4PCA_ARPACK8_2RF_classifier9RandomForestMeta3_10_6",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          3,
          10,
          6
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0mutual_info_classifKbest1TruncatedSVD2LinearDiscriminantAnalysisPrim3PowerTransformer0_1mutual_info_classifKbest5_2FastICA6_3BalancedRandomForestClassifier7_2RandomForestMeta4_8": {
    "id": "StandardScaler0mutual_info_classifKbest1TruncatedSVD2LinearDiscriminantAnalysisPrim3PowerTransformer0_1mutual_info_classifKbest5_2FastICA6_3BalancedRandomForestClassifier7_2RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1IncrementalPCA2SGDClassifier3MinMaxScaler0_1f_classifFDR5_2RandomTreesEmbedding6_2LinearDiscriminantAnalysisPrim7_3KBinsDiscretizerOrdinal0f_classifFWE9_2PCA_LAPACK10SVC11_7MajorityVoting4_8_12": {
    "id": "StandardScaler0f_classifFWE1IncrementalPCA2SGDClassifier3MinMaxScaler0_1f_classifFDR5_2RandomTreesEmbedding6_2LinearDiscriminantAnalysisPrim7_3KBinsDiscretizerOrdinal0f_classifFWE9_2PCA_LAPACK10SVC11_7MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiKbest1PCA_Randomized2ExtraTreesClassifier3PowerTransformer0_1f_classifFDR5_2FastICA6_2GaussianProcessClassifierPrim7_4Normalizer0mutual_info_classifKbest9_2RandomTreesEmbedding10_2RUSBoostClassifier11_8RandomForestMeta4_8_12": {
    "id": "MaxAbsScaler0UnivariateSelectChiKbest1PCA_Randomized2ExtraTreesClassifier3PowerTransformer0_1f_classifFDR5_2FastICA6_2GaussianProcessClassifierPrim7_4Normalizer0mutual_info_classifKbest9_2RandomTreesEmbedding10_2RUSBoostClassifier11_8RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          8
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFWE1IncrementalPCA2DecisionTreeClassifier3": {
    "id": "RobustScaler0f_classifFWE1IncrementalPCA2DecisionTreeClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFDR1TruncatedSVD2LinearSVC3": {
    "id": "StandardScaler0f_classifFDR1TruncatedSVD2LinearSVC3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifFPR1TruncatedSVD2RidgeClassifier3": {
    "id": "KBinsDiscretizerOneHot0f_classifFPR1TruncatedSVD2RidgeClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0f_classifFWE1IncrementalPCA2LinearDiscriminantAnalysisPrim3": {
    "id": "PowerTransformer0f_classifFWE1IncrementalPCA2LinearDiscriminantAnalysisPrim3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiFDR1PCA_ARPACK2SVC3PowerTransformer0RFE_RandomForest5PCA_Randomized6_1EasyEnsembleClassifier7_1KBinsDiscretizerOrdinal0UnivariateSelectChiFDR9_2PCA_Randomized10_5XGBClassifier11_8MajorityVoting4_8_12": {
    "id": "MaxAbsScaler0UnivariateSelectChiFDR1PCA_ARPACK2SVC3PowerTransformer0RFE_RandomForest5PCA_Randomized6_1EasyEnsembleClassifier7_1KBinsDiscretizerOrdinal0UnivariateSelectChiFDR9_2PCA_Randomized10_5XGBClassifier11_8MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with Chi-square. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with Chi-square. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          8
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiFPR1RUSBoostClassifier2": {
    "id": "MaxAbsScaler0UnivariateSelectChiFPR1RUSBoostClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifPercentile1PCA_LAPACK2XGBClassifier3KBinsDiscretizerOneHot0_1f_classifKbest5PCA_Randomized6_2EasyEnsembleClassifier7_4Normalizer0_1RFE_RandomForest9_5PCA_ARPACK10_7LogisticRegression11_1RandomForestMeta4_8_12": {
    "id": "StandardScaler0f_classifPercentile1PCA_LAPACK2XGBClassifier3KBinsDiscretizerOneHot0_1f_classifKbest5PCA_Randomized6_2EasyEnsembleClassifier7_4Normalizer0_1RFE_RandomForest9_5PCA_ARPACK10_7LogisticRegression11_1RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0f_classifFPR1GaussianProcessClassifierPrim2Normalizer0_1RFE_GradientBoosting4_2PCA_ARPACK5_1NearestCentroid6RobustScaler0_1mutual_info_classifPercentile8_4PCA_LAPACK9_5BaggingClassifier10_5RandomForestMeta3_7_11": {
    "id": "QuantileTransformer0f_classifFPR1GaussianProcessClassifierPrim2Normalizer0_1RFE_GradientBoosting4_2PCA_ARPACK5_1NearestCentroid6RobustScaler0_1mutual_info_classifPercentile8_4PCA_LAPACK9_5BaggingClassifier10_5RandomForestMeta3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          2
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "PowerTransformer0f_classifFWE1PCA_Randomized2LGBMClassifier3MinMaxScaler0_1mutual_info_classifPercentile5_1FastICA6_1ExtraTreesClassifier7_3RandomForestMeta4_8": {
    "id": "PowerTransformer0f_classifFWE1PCA_Randomized2LGBMClassifier3MinMaxScaler0_1mutual_info_classifPercentile5_1FastICA6_1ExtraTreesClassifier7_3RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "LabelEncoder0PowerTransformer1RFE_RandomForest2RidgeClassifierCV3NumericData0XGBClassifier5_1RandomForestMeta4_6": {
    "id": "LabelEncoder0PowerTransformer1RFE_RandomForest2RidgeClassifierCV3NumericData0XGBClassifier5_1RandomForestMeta4_6",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "NumericData",
          "hyperparams_run": {
            "default": true
          },
          "description": "Extracts only numeric data columns from input.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          4,
          6
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0UnivariateSelectChiKbest1LogisticRegression2QuantileTransformer0UnivariateSelectChiFWE4PCA_LAPACK5_2RUSBoostClassifier6RandomForestMeta3_7": {
    "id": "QuantileTransformer0UnivariateSelectChiKbest1LogisticRegression2QuantileTransformer0UnivariateSelectChiFWE4PCA_LAPACK5_2RUSBoostClassifier6RandomForestMeta3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifFPR1IncrementalPCA2LogisticRegressionCV3KBinsDiscretizerOneHot0UnivariateSelectChiFWE5_1RandomTreesEmbedding6RF_classifier7_2RandomForestMeta4_8": {
    "id": "MaxAbsScaler0f_classifFPR1IncrementalPCA2LogisticRegressionCV3KBinsDiscretizerOneHot0UnivariateSelectChiFWE5_1RandomTreesEmbedding6RF_classifier7_2RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RFE_RandomForest0RandomTreesEmbedding1AdaBoostClassifier2MinMaxScaler0f_classifPercentile4IncrementalPCA5_1KNeighborsClassifierPrim6_2RandomForestMeta3_7": {
    "id": "RFE_RandomForest0RandomTreesEmbedding1AdaBoostClassifier2MinMaxScaler0f_classifPercentile4IncrementalPCA5_1KNeighborsClassifierPrim6_2RandomForestMeta3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "Normalizer0RFE_GradientBoosting1IncrementalPCA2LogisticRegressionCV3MinMaxScaler0VarianceThreshold5_1KernelPCA6_3BaggingClassifier7_3Normalizer0_1RFE_GradientBoosting9_2PCA_ARPACK10_1GradientBoostingClassifier11_3RandomForestMeta4_8_12": {
    "id": "Normalizer0RFE_GradientBoosting1IncrementalPCA2LogisticRegressionCV3MinMaxScaler0VarianceThreshold5_1KernelPCA6_3BaggingClassifier7_3Normalizer0_1RFE_GradientBoosting9_2PCA_ARPACK10_1GradientBoostingClassifier11_3RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          3
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0IncrementalPCA1SGDClassifier2": {
    "id": "MaxAbsScaler0IncrementalPCA1SGDClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0mutual_info_classifPercentile1PCA_ARPACK2LGBMClassifier3": {
    "id": "Normalizer0mutual_info_classifPercentile1PCA_ARPACK2LGBMClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0mutual_info_classifPercentile1LinearDiscriminantAnalysisPrim2StandardScaler0_1RFE_GradientBoosting4PCA_LAPACK5XGBClassifier6RandomForestMeta3_7": {
    "id": "QuantileTransformer0mutual_info_classifPercentile1LinearDiscriminantAnalysisPrim2StandardScaler0_1RFE_GradientBoosting4PCA_LAPACK5XGBClassifier6RandomForestMeta3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0RFE_RandomForest1PCA_LAPACK2LinearSVC3": {
    "id": "RobustScaler0RFE_RandomForest1PCA_LAPACK2LinearSVC3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0mutual_info_classifPercentile1TruncatedSVD2RidgeClassifier3": {
    "id": "ImputerMedian0mutual_info_classifPercentile1TruncatedSVD2RidgeClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0QuantileTransformer1f_classifFPR2LogisticRegression3": {
    "id": "ImputerMedian0QuantileTransformer1f_classifFPR2LogisticRegression3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0RobustScaler1f_classifFPR2IncrementalPCA3LinearSVC4OneHotEncoder0_1MaxAbsScaler6_2RFE_GradientBoosting7_1FastICA8_4DecisionTreeClassifier9_2ImputerEncoderPrim0_6PowerTransformer11f_classifFPR12_3TruncatedSVD13_6PassiveAggressiveClassifier14_9MajorityVoting5_10_15": {
    "id": "ImputerEncoderPrim0RobustScaler1f_classifFPR2IncrementalPCA3LinearSVC4OneHotEncoder0_1MaxAbsScaler6_2RFE_GradientBoosting7_1FastICA8_4DecisionTreeClassifier9_2ImputerEncoderPrim0_6PowerTransformer11f_classifFPR12_3TruncatedSVD13_6PassiveAggressiveClassifier14_9MajorityVoting5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0,
          6
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          3
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          6
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          9
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0QuantileTransformer1f_classifFWE2DecisionTreeClassifier3LabelEncoder0MaxAbsScaler5f_classifFPR6_2AdaBoostClassifier7_4MajorityVoting4_8": {
    "id": "ImputerEncoderPrim0QuantileTransformer1f_classifFWE2DecisionTreeClassifier3LabelEncoder0MaxAbsScaler5f_classifFPR6_2AdaBoostClassifier7_4MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerMedian0PowerTransformer1VarianceThreshold2IncrementalPCA3PassiveAggressiveClassifier4ImputerMedian0MinMaxScaler6_2f_classifFPR7IncrementalPCA8_3": {
    "id": "ImputerMedian0PowerTransformer1VarianceThreshold2IncrementalPCA3PassiveAggressiveClassifier4ImputerMedian0MinMaxScaler6_2f_classifFPR7IncrementalPCA8_3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      }
    }
  },
  "ImputerMedian0RobustScaler1f_classifFPR2IncrementalPCA3LinearSVC4imputerIndicator0MinMaxScaler6_2RFE_RandomForest7_1PCA_Randomized8_1XGBClassifier9_2MajorityVoting5_10": {
    "id": "ImputerMedian0RobustScaler1f_classifFPR2IncrementalPCA3LinearSVC4imputerIndicator0MinMaxScaler6_2RFE_RandomForest7_1PCA_Randomized8_1XGBClassifier9_2MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "OneHotEncoder0StandardScaler1f_classifFWE2IncrementalPCA3EasyEnsembleClassifier4": {
    "id": "OneHotEncoder0StandardScaler1f_classifFWE2IncrementalPCA3EasyEnsembleClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOrdinal0f_classifFPR1PCA_LAPACK2KNeighborsClassifierPrim3": {
    "id": "KBinsDiscretizerOrdinal0f_classifFPR1PCA_LAPACK2KNeighborsClassifierPrim3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0PowerTransformer1f_classifFWE2FastICA3GradientBoostingClassifier4imputerIndicator0_1RobustScaler6f_classifKbest7_1PCA_Randomized8_4LogisticRegression9_3MajorityVoting5_10": {
    "id": "ImputerEncoderPrim0PowerTransformer1f_classifFWE2FastICA3GradientBoostingClassifier4imputerIndicator0_1RobustScaler6f_classifKbest7_1PCA_Randomized8_4LogisticRegression9_3MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0RobustScaler1mutual_info_classifKbest2PCA_LAPACK3SGDClassifier4": {
    "id": "ImputerEncoderPrim0RobustScaler1mutual_info_classifKbest2PCA_LAPACK3SGDClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputer0PowerTransformer1mutual_info_classifPercentile2FastICA3RidgeClassifierCV4": {
    "id": "imputer0PowerTransformer1mutual_info_classifPercentile2FastICA3RidgeClassifierCV4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      }
    }
  },
  "f_classifPercentile0FastICA1KNeighborsClassifierPrim2f_classifFDR0PCA_ARPACK4_2NearestCentroid5_2RandomForestMeta3_6": {
    "id": "f_classifPercentile0FastICA1KNeighborsClassifierPrim2f_classifFDR0PCA_ARPACK4_2NearestCentroid5_2RandomForestMeta3_6",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          3,
          6
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "imputer0f_classifFWE1PCA_LAPACK2RUSBoostClassifier3ImputerMedian0MinMaxScaler5UnivariateSelectChiFWE6RandomTreesEmbedding7GaussianProcessClassifierPrim8_4ImputerEncoderPrim0Normalizer10_1f_classifFWE11_2PCA_LAPACK12GaussianProcessClassifierPrim13_8RandomForestMeta4_9_14": {
    "id": "imputer0f_classifFWE1PCA_LAPACK2RUSBoostClassifier3ImputerMedian0MinMaxScaler5UnivariateSelectChiFWE6RandomTreesEmbedding7GaussianProcessClassifierPrim8_4ImputerEncoderPrim0Normalizer10_1f_classifFWE11_2PCA_LAPACK12GaussianProcessClassifierPrim13_8RandomForestMeta4_9_14",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          8
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          4,
          9,
          14
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiFWE1FastICA2LogisticRegressionCV3": {
    "id": "MaxAbsScaler0UnivariateSelectChiFWE1FastICA2LogisticRegressionCV3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0FastICA1LogisticRegression2QuantileTransformer0f_classifPercentile4FastICA5_2BaggingClassifier6_1KBinsDiscretizerOrdinal0_4f_classifFPR8_4KernelPCA9ExtraTreesClassifier10_1MajorityVoting3_7_11": {
    "id": "KBinsDiscretizerOneHot0FastICA1LogisticRegression2QuantileTransformer0f_classifPercentile4FastICA5_2BaggingClassifier6_1KBinsDiscretizerOrdinal0_4f_classifFPR8_4KernelPCA9ExtraTreesClassifier10_1MajorityVoting3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "imputer0StandardScaler1f_classifFWE2KNeighborsClassifierPrim3": {
    "id": "imputer0StandardScaler1f_classifFWE2KNeighborsClassifierPrim3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0f_classifFPR1FastICA2BalancedRandomForestClassifier3f_classifFDR0IncrementalPCA5_2AdaBoostClassifier6f_classifFWE0_2IncrementalPCA8_6KNeighborsClassifierPrim9_2MajorityVoting4_7_10": {
    "id": "Normalizer0f_classifFPR1FastICA2BalancedRandomForestClassifier3f_classifFDR0IncrementalPCA5_2AdaBoostClassifier6f_classifFWE0_2IncrementalPCA8_6KNeighborsClassifierPrim9_2MajorityVoting4_7_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          6
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          4,
          7,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0f_classifFPR1FastICA2KNeighborsClassifierPrim3": {
    "id": "QuantileTransformer0f_classifFPR1FastICA2KNeighborsClassifierPrim3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0VarianceThreshold1FastICA2NearestCentroid3": {
    "id": "ImputerMedian0VarianceThreshold1FastICA2NearestCentroid3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0PowerTransformer1RFE_RandomForest2FastICA3PassiveAggressiveClassifier4ImputerOneHotEncoderPrim0_1PowerTransformer6_2f_classifKbest7_2TruncatedSVD8_1KNeighborsClassifierPrim9_1MajorityVoting5_10": {
    "id": "ImputerEncoderPrim0PowerTransformer1RFE_RandomForest2FastICA3PassiveAggressiveClassifier4ImputerOneHotEncoderPrim0_1PowerTransformer6_2f_classifKbest7_2TruncatedSVD8_1KNeighborsClassifierPrim9_1MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFDR1SGDClassifier2MaxAbsScaler0RFE_RandomForest4_1FastICA5_2RidgeClassifierCV6_2RandomForestMeta3_7": {
    "id": "RobustScaler0f_classifFDR1SGDClassifier2MaxAbsScaler0RFE_RandomForest4_1FastICA5_2RidgeClassifierCV6_2RandomForestMeta3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFDR1PassiveAggressiveClassifier2": {
    "id": "RobustScaler0f_classifFDR1PassiveAggressiveClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifFWE1XGBClassifier2": {
    "id": "MaxAbsScaler0f_classifFWE1XGBClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      }
    }
  },
  "f_classifFWE0TruncatedSVD1AdaBoostClassifier2": {
    "id": "f_classifFWE0TruncatedSVD1AdaBoostClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0mutual_info_classifPercentile1TruncatedSVD2RidgeClassifierCV3RobustScaler0mutual_info_classifKbest5IncrementalPCA6_1LinearSVC7MajorityVoting4_8": {
    "id": "Normalizer0mutual_info_classifPercentile1TruncatedSVD2RidgeClassifierCV3RobustScaler0mutual_info_classifKbest5IncrementalPCA6_1LinearSVC7MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFWE1KernelPCA2BaggingClassifier3": {
    "id": "RobustScaler0f_classifFWE1KernelPCA2BaggingClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      }
    }
  },
  "RFE_RandomForest0PCA_ARPACK1RUSBoostClassifier2mutual_info_classifPercentile0_1FastICA4_2EasyEnsembleClassifier5_2StandardScaler0RFE_GradientBoosting7_4FastICA8_2RidgeClassifierCV9_6RandomForestMeta3_6_10": {
    "id": "RFE_RandomForest0PCA_ARPACK1RUSBoostClassifier2mutual_info_classifPercentile0_1FastICA4_2EasyEnsembleClassifier5_2StandardScaler0RFE_GradientBoosting7_4FastICA8_2RidgeClassifierCV9_6RandomForestMeta3_6_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          3,
          6,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0f_classifKbest1FastICA2RidgeClassifierCV3MaxAbsScaler0_1f_classifPercentile5IncrementalPCA6PassiveAggressiveClassifier7_2MajorityVoting4_8": {
    "id": "QuantileTransformer0f_classifKbest1FastICA2RidgeClassifierCV3MaxAbsScaler0_1f_classifPercentile5IncrementalPCA6PassiveAggressiveClassifier7_2MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "LabelEncoder0PowerTransformer1f_classifFPR2IncrementalPCA3RUSBoostClassifier4": {
    "id": "LabelEncoder0PowerTransformer1f_classifFPR2IncrementalPCA3RUSBoostClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1TruncatedSVD2RF_classifier3PowerTransformer0_1f_classifPercentile5PCA_LAPACK6_1GaussianProcessClassifierPrim7_2RandomForestMeta4_8": {
    "id": "StandardScaler0f_classifFWE1TruncatedSVD2RF_classifier3PowerTransformer0_1f_classifPercentile5PCA_LAPACK6_1GaussianProcessClassifierPrim7_2RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerMedian0MinMaxScaler1VarianceThreshold2IncrementalPCA3LinearSVC4": {
    "id": "ImputerMedian0MinMaxScaler1VarianceThreshold2IncrementalPCA3LinearSVC4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0Normalizer1VarianceThreshold2FastICA3SVC4": {
    "id": "ImputerMedian0Normalizer1VarianceThreshold2FastICA3SVC4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      }
    }
  },
  "f_classifPercentile0IncrementalPCA1RidgeClassifierCV2": {
    "id": "f_classifPercentile0IncrementalPCA1RidgeClassifierCV2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      }
    }
  },
  "OneHotEncoder0RobustScaler1RFE_RandomForest2FastICA3LogisticRegression4imputer0_1RobustScaler6_2f_classifKbest7_3RandomTreesEmbedding8_3RidgeClassifierCV9_5MajorityVoting5_10": {
    "id": "OneHotEncoder0RobustScaler1RFE_RandomForest2FastICA3LogisticRegression4imputer0_1RobustScaler6_2f_classifKbest7_3RandomTreesEmbedding8_3RidgeClassifierCV9_5MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          3
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "imputer0QuantileTransformer1mutual_info_classifKbest2FastICA3EasyEnsembleClassifier4": {
    "id": "imputer0QuantileTransformer1mutual_info_classifKbest2FastICA3EasyEnsembleClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      }
    }
  },
  "VarianceThreshold0PCA_LAPACK1LinearSVC2": {
    "id": "VarianceThreshold0PCA_LAPACK1LinearSVC2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiFPR1PCA_ARPACK2BaggingClassifier3StandardScaler0_1VarianceThreshold5_2PCA_LAPACK6_3LinearDiscriminantAnalysisPrim7_2MaxAbsScaler0_1UnivariateSelectChiFWE9_2KernelPCA10_3RUSBoostClassifier11_8RandomForestMeta4_8_12": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiFPR1PCA_ARPACK2BaggingClassifier3StandardScaler0_1VarianceThreshold5_2PCA_LAPACK6_3LinearDiscriminantAnalysisPrim7_2MaxAbsScaler0_1UnivariateSelectChiFWE9_2KernelPCA10_3RUSBoostClassifier11_8RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          8
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0RFE_RandomForest1PCA_ARPACK2AdaBoostClassifier3": {
    "id": "MaxAbsScaler0RFE_RandomForest1PCA_ARPACK2AdaBoostClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0RFE_RandomForest1TruncatedSVD2KNeighborsClassifierPrim3StandardScaler0_1VarianceThreshold5_1TruncatedSVD6PassiveAggressiveClassifier7_1QuantileTransformer0_5f_classifKbest9_2KernelPCA10_6QuadraticDiscriminantAnalysis11_8MajorityVoting4_8_12": {
    "id": "QuantileTransformer0RFE_RandomForest1TruncatedSVD2KNeighborsClassifierPrim3StandardScaler0_1VarianceThreshold5_1TruncatedSVD6PassiveAggressiveClassifier7_1QuantileTransformer0_5f_classifKbest9_2KernelPCA10_6QuadraticDiscriminantAnalysis11_8MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          8
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiKbest1GaussianNBClassifier2MinMaxScaler0f_classifFPR4_1FastICA5PassiveAggressiveClassifier6_2MaxAbsScaler0_4UnivariateSelectChiKbest8_4RandomTreesEmbedding9_6EasyEnsembleClassifier10_5MajorityVoting3_7_11": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiKbest1GaussianNBClassifier2MinMaxScaler0f_classifFPR4_1FastICA5PassiveAggressiveClassifier6_2MaxAbsScaler0_4UnivariateSelectChiKbest8_4RandomTreesEmbedding9_6EasyEnsembleClassifier10_5MajorityVoting3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0VarianceThreshold1KernelPCA2XGBClassifier3PowerTransformer0_1mutual_info_classifPercentile5_2PCA_LAPACK6_2LinearSVC7RandomForestMeta4_8": {
    "id": "StandardScaler0VarianceThreshold1KernelPCA2XGBClassifier3PowerTransformer0_1mutual_info_classifPercentile5_2PCA_LAPACK6_2LinearSVC7RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0RFE_GradientBoosting1PCA_ARPACK2GaussianProcessClassifierPrim3": {
    "id": "RobustScaler0RFE_GradientBoosting1PCA_ARPACK2GaussianProcessClassifierPrim3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      }
    }
  },
  "LabelEncoder0VarianceThreshold1FastICA2GaussianNBClassifier3imputerIndicator0_1MinMaxScaler5UnivariateSelectChiPercentile6_1IncrementalPCA7_1EasyEnsembleClassifier8_1MajorityVoting4_9": {
    "id": "LabelEncoder0VarianceThreshold1FastICA2GaussianNBClassifier3imputerIndicator0_1MinMaxScaler5UnivariateSelectChiPercentile6_1IncrementalPCA7_1EasyEnsembleClassifier8_1MajorityVoting4_9",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          4,
          9
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0UnivariateSelectChiKbest1PCA_LAPACK2GaussianNBClassifier3": {
    "id": "QuantileTransformer0UnivariateSelectChiKbest1PCA_LAPACK2GaussianNBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0mutual_info_classifKbest1BaggingClassifier2": {
    "id": "QuantileTransformer0mutual_info_classifKbest1BaggingClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0UnivariateSelectChiKbest1IncrementalPCA2RF_classifier3": {
    "id": "MinMaxScaler0UnivariateSelectChiKbest1IncrementalPCA2RF_classifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0QuantileTransformer1mutual_info_classifKbest2LGBMClassifier3": {
    "id": "ImputerEncoderPrim0QuantileTransformer1mutual_info_classifKbest2LGBMClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiFPR1KernelPCA2BaggingClassifier3KBinsDiscretizerOneHot0mutual_info_classifKbest5_2IncrementalPCA6_1RUSBoostClassifier7_3RandomForestMeta4_8": {
    "id": "MaxAbsScaler0UnivariateSelectChiFPR1KernelPCA2BaggingClassifier3KBinsDiscretizerOneHot0mutual_info_classifKbest5_2IncrementalPCA6_1RUSBoostClassifier7_3RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerMedian0MinMaxScaler1VarianceThreshold2TruncatedSVD3LinearDiscriminantAnalysisPrim4": {
    "id": "ImputerMedian0MinMaxScaler1VarianceThreshold2TruncatedSVD3LinearDiscriminantAnalysisPrim4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputer0MinMaxScaler1f_classifFWE2KernelPCA3RUSBoostClassifier4": {
    "id": "imputer0MinMaxScaler1f_classifFWE2KernelPCA3RUSBoostClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0f_classifPercentile1RidgeClassifier2": {
    "id": "Normalizer0f_classifPercentile1RidgeClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0MaxAbsScaler1VarianceThreshold2FastICA3SVC4": {
    "id": "ImputerEncoderPrim0MaxAbsScaler1VarianceThreshold2FastICA3SVC4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0mutual_info_classifPercentile1IncrementalPCA2RandomForestMeta3": {
    "id": "StandardScaler0mutual_info_classifPercentile1IncrementalPCA2RandomForestMeta3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0PowerTransformer1f_classifFPR2RandomTreesEmbedding3ComplementNBClassifier4imputerIndicator0_1RobustScaler6RFE_GradientBoosting7_3FastICA8_2DecisionTreeClassifier9RandomForestMeta5_10": {
    "id": "ImputerEncoderPrim0PowerTransformer1f_classifFPR2RandomTreesEmbedding3ComplementNBClassifier4imputerIndicator0_1RobustScaler6RFE_GradientBoosting7_3FastICA8_2DecisionTreeClassifier9RandomForestMeta5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "ComplementNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "The Complement Naive Bayes classifier described in Rennie et al. (2003). The Complement Naive Bayes classifier was designed to correct the \u201csevere assumptions\u201d made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0UnivariateSelectChiKbest1IncrementalPCA2RF_classifier3": {
    "id": "QuantileTransformer0UnivariateSelectChiKbest1IncrementalPCA2RF_classifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0VarianceThreshold1IncrementalPCA2DecisionTreeClassifier3": {
    "id": "Normalizer0VarianceThreshold1IncrementalPCA2DecisionTreeClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0RobustScaler1f_classifFWE2PCA_LAPACK3RandomForestMeta4": {
    "id": "ImputerEncoderPrim0RobustScaler1f_classifFWE2PCA_LAPACK3RandomForestMeta4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0f_classifFDR1FastICA2BaggingClassifier3": {
    "id": "StandardScaler0f_classifFDR1FastICA2BaggingClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0mutual_info_classifKbest1PCA_ARPACK2EasyEnsembleClassifier3QuantileTransformer0_1UnivariateSelectChiFPR5_2PCA_Randomized6_2QuadraticDiscriminantAnalysis7_3RandomForestMeta4_8": {
    "id": "Normalizer0mutual_info_classifKbest1PCA_ARPACK2EasyEnsembleClassifier3QuantileTransformer0_1UnivariateSelectChiFPR5_2PCA_Randomized6_2QuadraticDiscriminantAnalysis7_3RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "LabelEncoder0StandardScaler1f_classifFPR2RandomTreesEmbedding3RF_classifier4": {
    "id": "LabelEncoder0StandardScaler1f_classifFPR2RandomTreesEmbedding3RF_classifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0VarianceThreshold1PCA_LAPACK2RF_classifier3": {
    "id": "MinMaxScaler0VarianceThreshold1PCA_LAPACK2RF_classifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0mutual_info_classifKbest1RandomTreesEmbedding2BalancedRandomForestClassifier3": {
    "id": "RobustScaler0mutual_info_classifKbest1RandomTreesEmbedding2BalancedRandomForestClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0mutual_info_classifKbest1TruncatedSVD2LinearDiscriminantAnalysisPrim3": {
    "id": "PowerTransformer0mutual_info_classifKbest1TruncatedSVD2LinearDiscriminantAnalysisPrim3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputer0StandardScaler1ExtraTreesClassifier2LabelEncoder0Normalizer4_1f_classifFWE5_1IncrementalPCA6_1SVC7_2imputer0_1MaxAbsScaler9_2f_classifKbest10_2BernoulliNBClassifier11_7RandomForestMeta3_8_12": {
    "id": "imputer0StandardScaler1ExtraTreesClassifier2LabelEncoder0Normalizer4_1f_classifFWE5_1IncrementalPCA6_1SVC7_2imputer0_1MaxAbsScaler9_2f_classifKbest10_2BernoulliNBClassifier11_7RandomForestMeta3_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          3,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "LabelEncoder0RobustScaler1f_classifKbest2TruncatedSVD3GradientBoostingClassifier4": {
    "id": "LabelEncoder0RobustScaler1f_classifKbest2TruncatedSVD3GradientBoostingClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputer0MinMaxScaler1f_classifFDR2PCA_LAPACK3AdaBoostClassifier4LabelEncoder0_1MaxAbsScaler6_2UnivariateSelectChiKbest7_2PCA_Randomized8_4LGBMClassifier9RandomForestMeta5_10": {
    "id": "imputer0MinMaxScaler1f_classifFDR2PCA_LAPACK3AdaBoostClassifier4LabelEncoder0_1MaxAbsScaler6_2UnivariateSelectChiKbest7_2PCA_Randomized8_4LGBMClassifier9RandomForestMeta5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0f_classifFPR1GradientBoostingClassifier2": {
    "id": "MinMaxScaler0f_classifFPR1GradientBoostingClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1FastICA2LogisticRegression3": {
    "id": "StandardScaler0f_classifFWE1FastICA2LogisticRegression3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFPR1IncrementalPCA2BaggingClassifier3": {
    "id": "StandardScaler0f_classifFPR1IncrementalPCA2BaggingClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      }
    }
  },
  "LabelEncoder0RobustScaler1f_classifFWE2PCA_LAPACK3LogisticRegression4LabelEncoder0_1Normalizer6_1RFE_RandomForest7FastICA8_1LGBMClassifier9_2ImputerMedian0_6Normalizer11_1RFE_GradientBoosting12_2TruncatedSVD13_3BernoulliNBClassifier14_3RandomForestMeta5_10_15": {
    "id": "LabelEncoder0RobustScaler1f_classifFWE2PCA_LAPACK3LogisticRegression4LabelEncoder0_1Normalizer6_1RFE_RandomForest7FastICA8_1LGBMClassifier9_2ImputerMedian0_6Normalizer11_1RFE_GradientBoosting12_2TruncatedSVD13_3BernoulliNBClassifier14_3RandomForestMeta5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0,
          6
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          2
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0UnivariateSelectChiFWE1TruncatedSVD2BaggingClassifier3": {
    "id": "MinMaxScaler0UnivariateSelectChiFWE1TruncatedSVD2BaggingClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0Normalizer1f_classifKbest2PCA_ARPACK3NearestCentroid4": {
    "id": "ImputerMedian0Normalizer1f_classifKbest2PCA_ARPACK3NearestCentroid4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0f_classifPercentile1IncrementalPCA2XGBClassifier3RobustScaler0_1RandomTreesEmbedding5_2BernoulliNBClassifier6_2MajorityVoting4_7": {
    "id": "QuantileTransformer0f_classifPercentile1IncrementalPCA2XGBClassifier3RobustScaler0_1RandomTreesEmbedding5_2BernoulliNBClassifier6_2MajorityVoting4_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          4,
          7
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerMedian0Normalizer1VarianceThreshold2ExtraTreesClassifier3imputer0Normalizer5RFE_RandomForest6_3RandomTreesEmbedding7_1LogisticRegressionCV8OneHotEncoder0_5QuantileTransformer10_1f_classifFDR11_5IncrementalPCA12_1BernoulliNBClassifier13_8RandomForestMeta4_9_14": {
    "id": "ImputerMedian0Normalizer1VarianceThreshold2ExtraTreesClassifier3imputer0Normalizer5RFE_RandomForest6_3RandomTreesEmbedding7_1LogisticRegressionCV8OneHotEncoder0_5QuantileTransformer10_1f_classifFDR11_5IncrementalPCA12_1BernoulliNBClassifier13_8RandomForestMeta4_9_14",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          5
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          8
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          4,
          9,
          14
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerMedian0QuantileTransformer1f_classifFDR2PCA_Randomized3RF_classifier4": {
    "id": "ImputerMedian0QuantileTransformer1f_classifFDR2PCA_Randomized3RF_classifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "Normalizer0RFE_GradientBoosting1GradientBoostingClassifier2Normalizer0_1mutual_info_classifKbest4IncrementalPCA5BaggingClassifier6RandomForestMeta3_7": {
    "id": "Normalizer0RFE_GradientBoosting1GradientBoostingClassifier2Normalizer0_1mutual_info_classifKbest4IncrementalPCA5BaggingClassifier6RandomForestMeta3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0RFE_RandomForest1FastICA2AdaBoostClassifier3MinMaxScaler0f_classifPercentile5_2TruncatedSVD6LGBMClassifier7_2MajorityVoting4_8": {
    "id": "RobustScaler0RFE_RandomForest1FastICA2AdaBoostClassifier3MinMaxScaler0f_classifPercentile5_2TruncatedSVD6LGBMClassifier7_2MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0IncrementalPCA1GradientBoostingClassifier2MinMaxScaler0_1f_classifFDR4_1FastICA5XGBClassifier6RandomForestMeta3_7": {
    "id": "RobustScaler0IncrementalPCA1GradientBoostingClassifier2MinMaxScaler0_1f_classifFDR4_1FastICA5XGBClassifier6RandomForestMeta3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0RFE_GradientBoosting1FastICA2XGBClassifier3": {
    "id": "RobustScaler0RFE_GradientBoosting1FastICA2XGBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0f_classifFPR1FastICA2RidgeClassifierCV3": {
    "id": "QuantileTransformer0f_classifFPR1FastICA2RidgeClassifierCV3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0f_classifPercentile1PCA_Randomized2BaggingClassifier3StandardScaler0_1RFE_GradientBoosting5PCA_LAPACK6_1SGDClassifier7RandomForestMeta4_8": {
    "id": "RobustScaler0f_classifPercentile1PCA_Randomized2BaggingClassifier3StandardScaler0_1RFE_GradientBoosting5PCA_LAPACK6_1SGDClassifier7RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "mutual_info_classifPercentile0FastICA1LinearSVC2": {
    "id": "mutual_info_classifPercentile0FastICA1LinearSVC2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifFPR1FastICA2DecisionTreeClassifier3RobustScaler0_1f_classifKbest5_2TruncatedSVD6RF_classifier7_4RFE_RandomForest0_5PCA_ARPACK9LGBMClassifier10_5MajorityVoting4_8_11": {
    "id": "KBinsDiscretizerOneHot0f_classifFPR1FastICA2DecisionTreeClassifier3RobustScaler0_1f_classifKbest5_2TruncatedSVD6RF_classifier7_4RFE_RandomForest0_5PCA_ARPACK9LGBMClassifier10_5MajorityVoting4_8_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          5
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          8,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0f_classifFPR1KernelPCA2RidgeClassifierCV3": {
    "id": "StandardScaler0f_classifFPR1KernelPCA2RidgeClassifierCV3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      }
    }
  },
  "PCA_ARPACK0BalancedRandomForestClassifier1": {
    "id": "PCA_ARPACK0BalancedRandomForestClassifier1",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0RobustScaler1VarianceThreshold2TruncatedSVD3BernoulliNBClassifier4ImputerMedian0_1QuantileTransformer6RFE_RandomForest7_3TruncatedSVD8_3BernoulliNBClassifier9_4MajorityVoting5_10": {
    "id": "ImputerMedian0RobustScaler1VarianceThreshold2TruncatedSVD3BernoulliNBClassifier4ImputerMedian0_1QuantileTransformer6RFE_RandomForest7_3TruncatedSVD8_3BernoulliNBClassifier9_4MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          4
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFPR1PCA_LAPACK2SGDClassifier3": {
    "id": "RobustScaler0f_classifFPR1PCA_LAPACK2SGDClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0f_classifFPR1GaussianNBClassifier2": {
    "id": "QuantileTransformer0f_classifFPR1GaussianNBClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0MinMaxScaler1UnivariateSelectChiPercentile2PCA_Randomized3GradientBoostingClassifier4": {
    "id": "ImputerMedian0MinMaxScaler1UnivariateSelectChiPercentile2PCA_Randomized3GradientBoostingClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      }
    }
  },
  "LabelEncoder0MaxAbsScaler1UnivariateSelectChiPercentile2RandomTreesEmbedding3RandomForestMeta4": {
    "id": "LabelEncoder0MaxAbsScaler1UnivariateSelectChiPercentile2RandomTreesEmbedding3RandomForestMeta4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFWE1IncrementalPCA2RF_classifier3": {
    "id": "RobustScaler0f_classifFWE1IncrementalPCA2RF_classifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0mutual_info_classifKbest1BaggingClassifier2": {
    "id": "MaxAbsScaler0mutual_info_classifKbest1BaggingClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0RobustScaler1mutual_info_classifPercentile2FastICA3LinearSVC4": {
    "id": "ImputerMedian0RobustScaler1mutual_info_classifPercentile2FastICA3LinearSVC4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0TruncatedSVD1RUSBoostClassifier2": {
    "id": "RobustScaler0TruncatedSVD1RUSBoostClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputerIndicator0RFE_RandomForest1IncrementalPCA2NearestCentroid3ImputerEncoderPrim0QuantileTransformer5_1f_classifFPR6PCA_ARPACK7_3BalancedRandomForestClassifier8_4RandomForestMeta4_9": {
    "id": "imputerIndicator0RFE_RandomForest1IncrementalPCA2NearestCentroid3ImputerEncoderPrim0QuantileTransformer5_1f_classifFPR6PCA_ARPACK7_3BalancedRandomForestClassifier8_4RandomForestMeta4_9",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          4,
          9
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0MaxAbsScaler1f_classifFWE2PCA_LAPACK3LogisticRegressionCV4": {
    "id": "ImputerEncoderPrim0MaxAbsScaler1f_classifFWE2PCA_LAPACK3LogisticRegressionCV4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1PCA_LAPACK2GaussianProcessClassifierPrim3": {
    "id": "StandardScaler0f_classifFWE1PCA_LAPACK2GaussianProcessClassifierPrim3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0VarianceThreshold1FastICA2SGDClassifier3": {
    "id": "QuantileTransformer0VarianceThreshold1FastICA2SGDClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0UnivariateSelectChiKbest1RUSBoostClassifier2PowerTransformer0_1f_classifPercentile4_1RandomTreesEmbedding5_2BernoulliNBClassifier6_1MajorityVoting3_7": {
    "id": "MinMaxScaler0UnivariateSelectChiKbest1RUSBoostClassifier2PowerTransformer0_1f_classifPercentile4_1RandomTreesEmbedding5_2BernoulliNBClassifier6_1MajorityVoting3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "PowerTransformer0VarianceThreshold1AdaBoostClassifier2RobustScaler0_1f_classifFPR4ExtraTreesClassifier5_3RandomForestMeta3_6": {
    "id": "PowerTransformer0VarianceThreshold1AdaBoostClassifier2RobustScaler0_1f_classifFPR4ExtraTreesClassifier5_3RandomForestMeta3_6",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          3,
          6
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "LabelEncoder0PowerTransformer1f_classifFPR2PCA_Randomized3BernoulliNBClassifier4": {
    "id": "LabelEncoder0PowerTransformer1f_classifFPR2PCA_Randomized3BernoulliNBClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1TruncatedSVD2NearestCentroid3": {
    "id": "StandardScaler0f_classifFWE1TruncatedSVD2NearestCentroid3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0MinMaxScaler1mutual_info_classifPercentile2IncrementalPCA3KNeighborsClassifierPrim4ImputerOneHotEncoderPrim0MinMaxScaler6_1RFE_RandomForest7_3TruncatedSVD8_1NearestCentroid9_1imputerIndicator0Normalizer11_6f_classifPercentile12_7KernelPCA13_1BaggingClassifier14_5RandomForestMeta5_10_15": {
    "id": "ImputerEncoderPrim0MinMaxScaler1mutual_info_classifPercentile2IncrementalPCA3KNeighborsClassifierPrim4ImputerOneHotEncoderPrim0MinMaxScaler6_1RFE_RandomForest7_3TruncatedSVD8_1NearestCentroid9_1imputerIndicator0Normalizer11_6f_classifPercentile12_7KernelPCA13_1BaggingClassifier14_5RandomForestMeta5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          7
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          5
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "LabelEncoder0QuantileTransformer1f_classifFPR2PCA_LAPACK3RUSBoostClassifier4imputer0_1RobustScaler6mutual_info_classifKbest7_3TruncatedSVD8BernoulliNBClassifier9_5RandomForestMeta5_10": {
    "id": "LabelEncoder0QuantileTransformer1f_classifFPR2PCA_LAPACK3RUSBoostClassifier4imputer0_1RobustScaler6mutual_info_classifKbest7_3TruncatedSVD8BernoulliNBClassifier9_5RandomForestMeta5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0f_classifFPR1PCA_ARPACK2XGBClassifier3": {
    "id": "StandardScaler0f_classifFPR1PCA_ARPACK2XGBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0RFE_RandomForest1IncrementalPCA2XGBClassifier3StandardScaler0_1f_classifPercentile5_1FastICA6_1AdaBoostClassifier7_1MaxAbsScaler0_5TruncatedSVD9_2SVC10_4MajorityVoting4_8_11": {
    "id": "QuantileTransformer0RFE_RandomForest1IncrementalPCA2XGBClassifier3StandardScaler0_1f_classifPercentile5_1FastICA6_1AdaBoostClassifier7_1MaxAbsScaler0_5TruncatedSVD9_2SVC10_4MajorityVoting4_8_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          4
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          8,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0QuantileTransformer1UnivariateSelectChiPercentile2FastICA3RF_classifier4ImputerMedian0MaxAbsScaler6RFE_GradientBoosting7_2TruncatedSVD8_2BaggingClassifier9_5ImputerEncoderPrim0StandardScaler11_7f_classifPercentile12_8PCA_ARPACK13_9LinearSVC14_4MajorityVoting5_10_15": {
    "id": "ImputerEncoderPrim0QuantileTransformer1UnivariateSelectChiPercentile2FastICA3RF_classifier4ImputerMedian0MaxAbsScaler6RFE_GradientBoosting7_2TruncatedSVD8_2BaggingClassifier9_5ImputerEncoderPrim0StandardScaler11_7f_classifPercentile12_8PCA_ARPACK13_9LinearSVC14_4MajorityVoting5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          8
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          9
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          4
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0f_classifPercentile1GradientBoostingClassifier2KBinsDiscretizerOneHot0UnivariateSelectChiFPR4PCA_Randomized5_1DecisionTreeClassifier6_1KBinsDiscretizerOrdinal0_1f_classifPercentile8_2KernelPCA9_4GaussianProcessClassifierPrim10_3MajorityVoting3_7_11": {
    "id": "StandardScaler0f_classifPercentile1GradientBoostingClassifier2KBinsDiscretizerOneHot0UnivariateSelectChiFPR4PCA_Randomized5_1DecisionTreeClassifier6_1KBinsDiscretizerOrdinal0_1f_classifPercentile8_2KernelPCA9_4GaussianProcessClassifierPrim10_3MajorityVoting3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          4
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0UnivariateSelectChiKbest1PCA_ARPACK2ExtraTreesClassifier3": {
    "id": "QuantileTransformer0UnivariateSelectChiKbest1PCA_ARPACK2ExtraTreesClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiKbest1ComplementNBClassifier2PowerTransformer0IncrementalPCA4_2SGDClassifier5KBinsDiscretizerOrdinal0f_classifFDR7KernelPCA8_1KNeighborsClassifierPrim9_5RandomForestMeta3_6_10": {
    "id": "MaxAbsScaler0UnivariateSelectChiKbest1ComplementNBClassifier2PowerTransformer0IncrementalPCA4_2SGDClassifier5KBinsDiscretizerOrdinal0f_classifFDR7KernelPCA8_1KNeighborsClassifierPrim9_5RandomForestMeta3_6_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "ComplementNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "The Complement Naive Bayes classifier described in Rennie et al. (2003). The Complement Naive Bayes classifier was designed to correct the \u201csevere assumptions\u201d made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          3,
          6,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifPercentile1PCA_Randomized2LGBMClassifier3StandardScaler0f_classifFWE5KernelPCA6_3EasyEnsembleClassifier7_2KBinsDiscretizerOneHot0_1f_classifFWE9_2PCA_LAPACK10_3BalancedRandomForestClassifier11_1MajorityVoting4_8_12": {
    "id": "RobustScaler0f_classifPercentile1PCA_Randomized2LGBMClassifier3StandardScaler0f_classifFWE5KernelPCA6_3EasyEnsembleClassifier7_2KBinsDiscretizerOneHot0_1f_classifFWE9_2PCA_LAPACK10_3BalancedRandomForestClassifier11_1MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0MaxAbsScaler1f_classifFWE2TruncatedSVD3KNeighborsClassifierPrim4": {
    "id": "ImputerEncoderPrim0MaxAbsScaler1f_classifFWE2TruncatedSVD3KNeighborsClassifierPrim4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0RFE_GradientBoosting1IncrementalPCA2BalancedRandomForestClassifier3": {
    "id": "StandardScaler0RFE_GradientBoosting1IncrementalPCA2BalancedRandomForestClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1FastICA2LGBMClassifier3": {
    "id": "StandardScaler0f_classifFWE1FastICA2LGBMClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifFWE1PCA_LAPACK2RF_classifier3RobustScaler0_1f_classifKbest5_1TruncatedSVD6_1DecisionTreeClassifier7_1MajorityVoting4_8": {
    "id": "KBinsDiscretizerOneHot0f_classifFWE1PCA_LAPACK2RF_classifier3RobustScaler0_1f_classifKbest5_1TruncatedSVD6_1DecisionTreeClassifier7_1MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifFWE1TruncatedSVD2ExtraTreesClassifier3KBinsDiscretizerOrdinal0_1f_classifFDR5_2FastICA6_3BalancedRandomForestClassifier7_3KBinsDiscretizerOneHot0mutual_info_classifKbest9_2KernelPCA10_1NearestCentroid11_2RandomForestMeta4_8_12": {
    "id": "MaxAbsScaler0f_classifFWE1TruncatedSVD2ExtraTreesClassifier3KBinsDiscretizerOrdinal0_1f_classifFDR5_2FastICA6_3BalancedRandomForestClassifier7_3KBinsDiscretizerOneHot0mutual_info_classifKbest9_2KernelPCA10_1NearestCentroid11_2RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          2
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0f_classifFDR1LogisticRegression2": {
    "id": "RobustScaler0f_classifFDR1LogisticRegression2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFPR1PCA_ARPACK2GaussianProcessClassifierPrim3": {
    "id": "StandardScaler0f_classifFPR1PCA_ARPACK2GaussianProcessClassifierPrim3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      }
    }
  },
  "OneHotEncoder0MaxAbsScaler1f_classifKbest2TruncatedSVD3KNeighborsClassifierPrim4ImputerEncoderPrim0Normalizer6_2mutual_info_classifKbest7_1PCA_Randomized8_3RUSBoostClassifier9_2RandomForestMeta5_10": {
    "id": "OneHotEncoder0MaxAbsScaler1f_classifKbest2TruncatedSVD3KNeighborsClassifierPrim4ImputerEncoderPrim0Normalizer6_2mutual_info_classifKbest7_1PCA_Randomized8_3RUSBoostClassifier9_2RandomForestMeta5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0mutual_info_classifPercentile1TruncatedSVD2PassiveAggressiveClassifier3": {
    "id": "KBinsDiscretizerOneHot0mutual_info_classifPercentile1TruncatedSVD2PassiveAggressiveClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      }
    }
  },
  "LabelEncoder0PowerTransformer1f_classifFWE2IncrementalPCA3QuadraticDiscriminantAnalysis4imputerIndicator0_1StandardScaler6_1f_classifFDR7_1IncrementalPCA8_2XGBClassifier9_2OneHotEncoder0MaxAbsScaler11_2f_classifFPR12_7KNeighborsClassifierPrim13RandomForestMeta5_10_14": {
    "id": "LabelEncoder0PowerTransformer1f_classifFWE2IncrementalPCA3QuadraticDiscriminantAnalysis4imputerIndicator0_1StandardScaler6_1f_classifFDR7_1IncrementalPCA8_2XGBClassifier9_2OneHotEncoder0MaxAbsScaler11_2f_classifFPR12_7KNeighborsClassifierPrim13RandomForestMeta5_10_14",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          2
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          7
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          5,
          10,
          14
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "PowerTransformer0f_classifFWE1IncrementalPCA2RF_classifier3": {
    "id": "PowerTransformer0f_classifFWE1IncrementalPCA2RF_classifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0TruncatedSVD1GaussianProcessClassifierPrim2RobustScaler0_1RFE_GradientBoosting4_1IncrementalPCA5_2GradientBoostingClassifier6RandomForestMeta3_7": {
    "id": "MinMaxScaler0TruncatedSVD1GaussianProcessClassifierPrim2RobustScaler0_1RFE_GradientBoosting4_1IncrementalPCA5_2GradientBoostingClassifier6RandomForestMeta3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "Normalizer0f_classifFPR1TruncatedSVD2EasyEnsembleClassifier3MinMaxScaler0f_classifFDR5_1PCA_LAPACK6_3DecisionTreeClassifier7_4MaxAbsScaler0RFE_GradientBoosting9_5BalancedRandomForestClassifier10_6RandomForestMeta4_8_11": {
    "id": "Normalizer0f_classifFPR1TruncatedSVD2EasyEnsembleClassifier3MinMaxScaler0f_classifFDR5_1PCA_LAPACK6_3DecisionTreeClassifier7_4MaxAbsScaler0RFE_GradientBoosting9_5BalancedRandomForestClassifier10_6RandomForestMeta4_8_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          8,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0UnivariateSelectChiKbest1TruncatedSVD2RidgeClassifierCV3RobustScaler0f_classifFDR5_1FastICA6_3EasyEnsembleClassifier7_3MajorityVoting4_8": {
    "id": "KBinsDiscretizerOneHot0UnivariateSelectChiKbest1TruncatedSVD2RidgeClassifierCV3RobustScaler0f_classifFDR5_1FastICA6_3EasyEnsembleClassifier7_3MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0f_classifFWE1AdaBoostClassifier2": {
    "id": "MinMaxScaler0f_classifFWE1AdaBoostClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerOneHotEncoderPrim0StandardScaler1VarianceThreshold2PCA_LAPACK3DecisionTreeClassifier4NumericData0XGBClassifier6_4RandomForestMeta5_7": {
    "id": "ImputerOneHotEncoderPrim0StandardScaler1VarianceThreshold2PCA_LAPACK3DecisionTreeClassifier4NumericData0XGBClassifier6_4RandomForestMeta5_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "NumericData",
          "hyperparams_run": {
            "default": true
          },
          "description": "Extracts only numeric data columns from input.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          4
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          5,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "imputerIndicator0Normalizer1f_classifFPR2IncrementalPCA3EasyEnsembleClassifier4": {
    "id": "imputerIndicator0Normalizer1f_classifFPR2IncrementalPCA3EasyEnsembleClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0f_classifFWE1GaussianProcessClassifierPrim2": {
    "id": "PowerTransformer0f_classifFWE1GaussianProcessClassifierPrim2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0RFE_RandomForest1PCA_LAPACK2GaussianNBClassifier3": {
    "id": "StandardScaler0RFE_RandomForest1PCA_LAPACK2GaussianNBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1PCA_ARPACK2LogisticRegression3Normalizer0mutual_info_classifKbest5IncrementalPCA6_1GaussianProcessClassifierPrim7_4RobustScaler0_5RFE_GradientBoosting9_5KernelPCA10_6GaussianNBClassifier11RandomForestMeta4_8_12": {
    "id": "StandardScaler0f_classifFWE1PCA_ARPACK2LogisticRegression3Normalizer0mutual_info_classifKbest5IncrementalPCA6_1GaussianProcessClassifierPrim7_4RobustScaler0_5RFE_GradientBoosting9_5KernelPCA10_6GaussianNBClassifier11RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          6
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "f_classifFDR0LogisticRegression1StandardScaler0VarianceThreshold3_1FastICA4_1GaussianProcessClassifierPrim5_2KBinsDiscretizerOneHot0_3f_classifFWE7FastICA8_4NearestCentroid9_3MajorityVoting2_6_10": {
    "id": "f_classifFDR0LogisticRegression1StandardScaler0VarianceThreshold3_1FastICA4_1GaussianProcessClassifierPrim5_2KBinsDiscretizerOneHot0_3f_classifFWE7FastICA8_4NearestCentroid9_3MajorityVoting2_6_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3,
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          0,
          3
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          2,
          6,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0StandardScaler1f_classifFWE2PCA_ARPACK3RidgeClassifier4ImputerMedian0_1Normalizer6f_classifFWE7PCA_LAPACK8_4QuadraticDiscriminantAnalysis9_2ImputerMedian0MinMaxScaler11_7VarianceThreshold12PCA_LAPACK13_8LinearDiscriminantAnalysisPrim14_8MajorityVoting5_10_15": {
    "id": "ImputerEncoderPrim0StandardScaler1f_classifFWE2PCA_ARPACK3RidgeClassifier4ImputerMedian0_1Normalizer6f_classifFWE7PCA_LAPACK8_4QuadraticDiscriminantAnalysis9_2ImputerMedian0MinMaxScaler11_7VarianceThreshold12PCA_LAPACK13_8LinearDiscriminantAnalysisPrim14_8MajorityVoting5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          7
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          8
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          8
        ],
        "primitive": {
          "name": "LinearDiscriminantAnalysisPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Discriminant Analysis. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerMedian0MinMaxScaler1UnivariateSelectChiFPR2FastICA3LogisticRegression4ImputerMedian0_1RobustScaler6_1VarianceThreshold7_2PCA_ARPACK8_4GaussianNBClassifier9MajorityVoting5_10": {
    "id": "ImputerMedian0MinMaxScaler1UnivariateSelectChiFPR2FastICA3LogisticRegression4ImputerMedian0_1RobustScaler6_1VarianceThreshold7_2PCA_ARPACK8_4GaussianNBClassifier9MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0StandardScaler1mutual_info_classifPercentile2TruncatedSVD3GaussianNBClassifier4": {
    "id": "ImputerEncoderPrim0StandardScaler1mutual_info_classifPercentile2TruncatedSVD3GaussianNBClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifFPR1IncrementalPCA2BaggingClassifier3MinMaxScaler0_1RFE_GradientBoosting5_1RandomTreesEmbedding6_3SVC7RandomForestMeta4_8": {
    "id": "KBinsDiscretizerOneHot0f_classifFPR1IncrementalPCA2BaggingClassifier3MinMaxScaler0_1RFE_GradientBoosting5_1RandomTreesEmbedding6_3SVC7RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "OneHotEncoder0StandardScaler1f_classifFDR2IncrementalPCA3AdaBoostClassifier4ImputerMedian0_1RobustScaler6f_classifFWE7_3PCA_ARPACK8_2QuadraticDiscriminantAnalysis9_3ImputerMedian0_1MaxAbsScaler11_1f_classifFWE12_8PCA_LAPACK13_4QuadraticDiscriminantAnalysis14_7MajorityVoting5_10_15": {
    "id": "OneHotEncoder0StandardScaler1f_classifFDR2IncrementalPCA3AdaBoostClassifier4ImputerMedian0_1RobustScaler6f_classifFWE7_3PCA_ARPACK8_2QuadraticDiscriminantAnalysis9_3ImputerMedian0_1MaxAbsScaler11_1f_classifFWE12_8PCA_LAPACK13_4QuadraticDiscriminantAnalysis14_7MajorityVoting5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          8
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          4
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          7
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0mutual_info_classifPercentile1PCA_LAPACK2RF_classifier3": {
    "id": "StandardScaler0mutual_info_classifPercentile1PCA_LAPACK2RF_classifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "f_classifFPR0RandomTreesEmbedding1MultinomialNB2MaxAbsScaler0mutual_info_classifPercentile4_1TruncatedSVD5_2BalancedRandomForestClassifier6_1Normalizer0f_classifFDR8_1PCA_LAPACK9_5LogisticRegression10_1MajorityVoting3_7_11": {
    "id": "f_classifFPR0RandomTreesEmbedding1MultinomialNB2MaxAbsScaler0mutual_info_classifPercentile4_1TruncatedSVD5_2BalancedRandomForestClassifier6_1Normalizer0f_classifFDR8_1PCA_LAPACK9_5LogisticRegression10_1MajorityVoting3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "MultinomialNB",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multinomial models. The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "mutual_info_classifPercentile0PCA_LAPACK1RidgeClassifierCV2MaxAbsScaler0UnivariateSelectChiKbest4FastICA5_1EasyEnsembleClassifier6_2MajorityVoting3_7": {
    "id": "mutual_info_classifPercentile0PCA_LAPACK1RidgeClassifierCV2MaxAbsScaler0UnivariateSelectChiKbest4FastICA5_1EasyEnsembleClassifier6_2MajorityVoting3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiKbest1IncrementalPCA2RF_classifier3KBinsDiscretizerOrdinal0UnivariateSelectChiFDR5PCA_Randomized6_1LogisticRegressionCV7_2RandomForestMeta4_8": {
    "id": "MaxAbsScaler0UnivariateSelectChiKbest1IncrementalPCA2RF_classifier3KBinsDiscretizerOrdinal0UnivariateSelectChiFDR5PCA_Randomized6_1LogisticRegressionCV7_2RandomForestMeta4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "UnivariateSelectChiFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with Chi-square. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerMedian0RobustScaler1VarianceThreshold2PCA_LAPACK3ExtraTreesClassifier4": {
    "id": "ImputerMedian0RobustScaler1VarianceThreshold2PCA_LAPACK3ExtraTreesClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputer0PowerTransformer1f_classifKbest2BernoulliNBClassifier3": {
    "id": "imputer0PowerTransformer1f_classifKbest2BernoulliNBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      }
    }
  },
  "RobustScaler0VarianceThreshold1TruncatedSVD2BernoulliNBClassifier3PowerTransformer0_1f_classifFPR5_1PCA_LAPACK6_3EasyEnsembleClassifier7_1RobustScaler0_1mutual_info_classifPercentile9PCA_LAPACK10_2LGBMClassifier11_6RandomForestMeta4_8_12": {
    "id": "RobustScaler0VarianceThreshold1TruncatedSVD2BernoulliNBClassifier3PowerTransformer0_1f_classifFPR5_1PCA_LAPACK6_3EasyEnsembleClassifier7_1RobustScaler0_1mutual_info_classifPercentile9PCA_LAPACK10_2LGBMClassifier11_6RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          6
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "RobustScaler0RFE_GradientBoosting1TruncatedSVD2BernoulliNBClassifier3": {
    "id": "RobustScaler0RFE_GradientBoosting1TruncatedSVD2BernoulliNBClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0PowerTransformer1RFE_RandomForest2IncrementalPCA3LinearSVC4ImputerEncoderPrim0Normalizer6_1f_classifPercentile7RandomTreesEmbedding8_3GradientBoostingClassifier9_4MajorityVoting5_10": {
    "id": "ImputerMedian0PowerTransformer1RFE_RandomForest2IncrementalPCA3LinearSVC4ImputerEncoderPrim0Normalizer6_1f_classifPercentile7RandomTreesEmbedding8_3GradientBoostingClassifier9_4MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          3
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          4
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "QuantileTransformer0f_classifPercentile1PCA_ARPACK2QuadraticDiscriminantAnalysis3MinMaxScaler0RFE_RandomForest5RandomTreesEmbedding6SVC7_2RobustScaler0RFE_GradientBoosting9_5PCA_ARPACK10_2RandomForestMeta4_8_11": {
    "id": "QuantileTransformer0f_classifPercentile1PCA_ARPACK2QuadraticDiscriminantAnalysis3MinMaxScaler0RFE_RandomForest5RandomTreesEmbedding6SVC7_2RobustScaler0RFE_GradientBoosting9_5PCA_ARPACK10_2RandomForestMeta4_8_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          8,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0VarianceThreshold1DecisionTreeClassifier2": {
    "id": "ImputerEncoderPrim0VarianceThreshold1DecisionTreeClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      }
    }
  },
  "f_classifFWE0RandomTreesEmbedding1SVC2": {
    "id": "f_classifFWE0RandomTreesEmbedding1SVC2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      }
    }
  },
  "OneHotEncoder0MinMaxScaler1FastICA2RandomForestMeta3": {
    "id": "OneHotEncoder0MinMaxScaler1FastICA2RandomForestMeta3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0mutual_info_classifPercentile1PCA_Randomized2LGBMClassifier3": {
    "id": "MaxAbsScaler0mutual_info_classifPercentile1PCA_Randomized2LGBMClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LGBMClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "LightGBM is a gradient boosting framework that uses tree based learning algorithms.",
          "type": "Classifier"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifFPR1PCA_ARPACK2NearestCentroid3": {
    "id": "KBinsDiscretizerOneHot0f_classifFPR1PCA_ARPACK2NearestCentroid3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "NearestCentroid",
          "hyperparams_run": {
            "default": true
          },
          "description": "Nearest centroid classifier. Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0f_classifFPR1FastICA2EasyEnsembleClassifier3": {
    "id": "PowerTransformer0f_classifFPR1FastICA2EasyEnsembleClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0RFE_RandomForest1IncrementalPCA2LogisticRegression3": {
    "id": "PowerTransformer0RFE_RandomForest1IncrementalPCA2LogisticRegression3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputerIndicator0RobustScaler1f_classifKbest2FastICA3RidgeClassifier4imputerIndicator0_1RobustScaler6_1f_classifPercentile7_2IncrementalPCA8SVC9MajorityVoting5_10": {
    "id": "imputerIndicator0RobustScaler1f_classifKbest2FastICA3RidgeClassifier4imputerIndicator0_1RobustScaler6_1f_classifPercentile7_2IncrementalPCA8SVC9MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "Normalizer0f_classifFWE1SGDClassifier2": {
    "id": "Normalizer0f_classifFWE1SGDClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "SGDClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0UnivariateSelectChiFDR1FastICA2AdaBoostClassifier3StandardScaler0f_classifPercentile5_2KernelPCA6_2SVC7_4MajorityVoting4_8": {
    "id": "QuantileTransformer0UnivariateSelectChiFDR1FastICA2AdaBoostClassifier3StandardScaler0f_classifPercentile5_2KernelPCA6_2SVC7_4MajorityVoting4_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with Chi-square. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          4,
          8
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "KBinsDiscretizerOneHot0f_classifFPR1GradientBoostingClassifier2StandardScaler0f_classifFWE4PCA_LAPACK5_1SVC6_1KBinsDiscretizerOneHot0_4RFE_RandomForest8_1PCA_LAPACK9_2XGBClassifier10_3MajorityVoting3_7_11": {
    "id": "KBinsDiscretizerOneHot0f_classifFPR1GradientBoostingClassifier2StandardScaler0f_classifFWE4PCA_LAPACK5_1SVC6_1KBinsDiscretizerOneHot0_4RFE_RandomForest8_1PCA_LAPACK9_2XGBClassifier10_3MajorityVoting3_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0,
          4
        ],
        "primitive": {
          "name": "KBinsDiscretizerOneHot",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. One-Hot",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          3,
          7,
          11
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0f_classifFWE1PassiveAggressiveClassifier2PowerTransformer0_1f_classifFPR4_2KernelPCA5PassiveAggressiveClassifier6_2MajorityVoting3_7": {
    "id": "StandardScaler0f_classifFWE1PassiveAggressiveClassifier2PowerTransformer0_1f_classifFPR4_2KernelPCA5PassiveAggressiveClassifier6_2MajorityVoting3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "PowerTransformer0f_classifFWE1IncrementalPCA2LogisticRegression3": {
    "id": "PowerTransformer0f_classifFWE1IncrementalPCA2LogisticRegression3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "RFE_GradientBoosting0TruncatedSVD1GaussianProcessClassifierPrim2RobustScaler0f_classifFPR4IncrementalPCA5_2BalancedRandomForestClassifier6_1RandomForestMeta3_7": {
    "id": "RFE_GradientBoosting0TruncatedSVD1GaussianProcessClassifierPrim2RobustScaler0f_classifFPR4IncrementalPCA5_2BalancedRandomForestClassifier6_1RandomForestMeta3_7",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "BalancedRandomForestClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A balanced random forest classifier. A balanced random forest randomly under-samples each boostrap sample to balance it.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          3,
          7
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MaxAbsScaler0UnivariateSelectChiFPR1TruncatedSVD2PassiveAggressiveClassifier3KBinsDiscretizerOrdinal0_1mutual_info_classifKbest5_2TruncatedSVD6_3LogisticRegressionCV7_4Normalizer0UnivariateSelectChiKbest9KernelPCA10_1RUSBoostClassifier11_4MajorityVoting4_8_12": {
    "id": "MaxAbsScaler0UnivariateSelectChiFPR1TruncatedSVD2PassiveAggressiveClassifier3KBinsDiscretizerOrdinal0_1mutual_info_classifKbest5_2TruncatedSVD6_3LogisticRegressionCV7_4Normalizer0UnivariateSelectChiKbest9KernelPCA10_1RUSBoostClassifier11_4MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "KBinsDiscretizerOrdinal",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bin continuous data into intervals. Ordinal.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          4
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0MinMaxScaler1UnivariateSelectChiFPR2TruncatedSVD3RidgeClassifier4": {
    "id": "ImputerEncoderPrim0MinMaxScaler1UnivariateSelectChiFPR2TruncatedSVD3RidgeClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      }
    }
  },
  "MinMaxScaler0UnivariateSelectChiFPR1TruncatedSVD2RidgeClassifier3MaxAbsScaler0f_classifKbest5TruncatedSVD6_3LogisticRegressionCV7_4Normalizer0_5f_classifFWE9_1KernelPCA10_1RUSBoostClassifier11_4MajorityVoting4_8_12": {
    "id": "MinMaxScaler0UnivariateSelectChiFPR1TruncatedSVD2RidgeClassifier3MaxAbsScaler0f_classifKbest5TruncatedSVD6_3LogisticRegressionCV7_4Normalizer0_5f_classifFWE9_1KernelPCA10_1RUSBoostClassifier11_4MajorityVoting4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          5
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          4
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "Normalizer0mutual_info_classifPercentile1PCA_LAPACK2LogisticRegressionCV3": {
    "id": "Normalizer0mutual_info_classifPercentile1PCA_LAPACK2LogisticRegressionCV3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputer0Normalizer1mutual_info_classifPercentile2IncrementalPCA3AdaBoostClassifier4": {
    "id": "imputer0Normalizer1mutual_info_classifPercentile2IncrementalPCA3AdaBoostClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0RobustScaler1f_classifFPR2TruncatedSVD3BernoulliNBClassifier4OneHotEncoder0PowerTransformer6RFE_GradientBoosting7_2PCA_ARPACK8GaussianNBClassifier9_4imputerIndicator0MinMaxScaler11f_classifKbest12IncrementalPCA13_6ExtraTreesClassifier14_7MajorityVoting5_10_15": {
    "id": "ImputerMedian0RobustScaler1f_classifFPR2TruncatedSVD3BernoulliNBClassifier4OneHotEncoder0PowerTransformer6RFE_GradientBoosting7_2PCA_ARPACK8GaussianNBClassifier9_4imputerIndicator0MinMaxScaler11f_classifKbest12IncrementalPCA13_6ExtraTreesClassifier14_7MajorityVoting5_10_15",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          4
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          13,
          6
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "15": {
        "index": 15,
        "inputs": [
          14,
          7
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "16": {
        "index": 16,
        "inputs": [
          5,
          10,
          15
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "imputer0mutual_info_classifKbest1PCA_ARPACK2GradientBoostingClassifier3": {
    "id": "imputer0mutual_info_classifKbest1PCA_ARPACK2GradientBoostingClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0RobustScaler1RFE_GradientBoosting2PCA_Randomized3RidgeClassifier4": {
    "id": "ImputerEncoderPrim0RobustScaler1RFE_GradientBoosting2PCA_Randomized3RidgeClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      }
    }
  },
  "f_classifFPR0PCA_ARPACK1BaggingClassifier2RFE_RandomForest0PCA_ARPACK4_2KNeighborsClassifierPrim5_2MinMaxScaler0UnivariateSelectChiFPR7PCA_ARPACK8_2GradientBoostingClassifier9_6RandomForestMeta3_6_10": {
    "id": "f_classifFPR0PCA_ARPACK1BaggingClassifier2RFE_RandomForest0PCA_ARPACK4_2KNeighborsClassifierPrim5_2MinMaxScaler0UnivariateSelectChiFPR7PCA_ARPACK8_2GradientBoostingClassifier9_6RandomForestMeta3_6_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "BaggingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A Bagging classifier. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4,
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "KNeighborsClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier implementing the k-nearest neighbors vote.",
          "type": "Classifier"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "GradientBoostingClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          3,
          6,
          10
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0MaxAbsScaler1f_classifFDR2KernelPCA3RF_classifier4": {
    "id": "ImputerEncoderPrim0MaxAbsScaler1f_classifFDR2KernelPCA3RF_classifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RF_classifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "Classifier"
        }
      }
    }
  },
  "QuantileTransformer0f_classifFPR1TruncatedSVD2LogisticRegressionCV3PowerTransformer0mutual_info_classifPercentile5RandomTreesEmbedding6_1RUSBoostClassifier7_1StandardScaler0RFE_RandomForest9_2PCA_ARPACK10_3GaussianProcessClassifierPrim11_1RandomForestMeta4_8_12": {
    "id": "QuantileTransformer0f_classifFPR1TruncatedSVD2LogisticRegressionCV3PowerTransformer0mutual_info_classifPercentile5RandomTreesEmbedding6_1RUSBoostClassifier7_1StandardScaler0RFE_RandomForest9_2PCA_ARPACK10_3GaussianProcessClassifierPrim11_1RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "mutual_info_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "GaussianProcessClassifierPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams. Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian. Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "Normalizer0f_classifFWE1PCA_Randomized2RidgeClassifier3QuantileTransformer0_1mutual_info_classifKbest5_1PCA_LAPACK6_1DecisionTreeClassifier7_3MinMaxScaler0UnivariateSelectChiFWE9_6PCA_LAPACK10_3SVC11_5RandomForestMeta4_8_12": {
    "id": "Normalizer0f_classifFWE1PCA_Randomized2RidgeClassifier3QuantileTransformer0_1mutual_info_classifKbest5_1PCA_LAPACK6_1DecisionTreeClassifier7_3MinMaxScaler0UnivariateSelectChiFWE9_6PCA_LAPACK10_3SVC11_5RandomForestMeta4_8_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RidgeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Classifier using Ridge regression.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          1
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          3
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          6
        ],
        "primitive": {
          "name": "UnivariateSelectChiFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with Chi-square.",
          "type": "feature selection"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          3
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          5
        ],
        "primitive": {
          "name": "SVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          4,
          8,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "MinMaxScaler0UnivariateSelectChiFPR1RandomTreesEmbedding2LogisticRegression3": {
    "id": "MinMaxScaler0UnivariateSelectChiFPR1RandomTreesEmbedding2LogisticRegression3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "UnivariateSelectChiFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with Chi-square. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "PowerTransformer0f_classifFWE1PCA_LAPACK2Normalizer0_1VarianceThreshold4PCA_Randomized5_2PassiveAggressiveClassifier6_1": {
    "id": "PowerTransformer0f_classifFWE1PCA_LAPACK2Normalizer0_1VarianceThreshold4PCA_Randomized5_2PassiveAggressiveClassifier6_1",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "PassiveAggressiveClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Passive Aggressive Classifier",
          "type": "Classifier"
        }
      }
    }
  },
  "imputerIndicator0PowerTransformer1f_classifKbest2PCA_ARPACK3XGBClassifier4LabelEncoder0_1MinMaxScaler6_1f_classifKbest7_1TruncatedSVD8QuadraticDiscriminantAnalysis9_5MajorityVoting5_10": {
    "id": "imputerIndicator0PowerTransformer1f_classifKbest2PCA_ARPACK3XGBClassifier4LabelEncoder0_1MinMaxScaler6_1f_classifKbest7_1TruncatedSVD8QuadraticDiscriminantAnalysis9_5MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          5
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "OneHotEncoder0QuantileTransformer1f_classifFPR2IncrementalPCA3AdaBoostClassifier4": {
    "id": "OneHotEncoder0QuantileTransformer1f_classifFPR2IncrementalPCA3AdaBoostClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      }
    }
  },
  "MaxAbsScaler0f_classifFDR1RandomTreesEmbedding2QuadraticDiscriminantAnalysis3PowerTransformer0_1PCA_LAPACK5AdaBoostClassifier6StandardScaler0RFE_GradientBoosting8_1RandomTreesEmbedding9_1QuadraticDiscriminantAnalysis10_7RandomForestMeta4_7_11": {
    "id": "MaxAbsScaler0f_classifFDR1RandomTreesEmbedding2QuadraticDiscriminantAnalysis3PowerTransformer0_1PCA_LAPACK5AdaBoostClassifier6StandardScaler0RFE_GradientBoosting8_1RandomTreesEmbedding9_1QuadraticDiscriminantAnalysis10_7RandomForestMeta4_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifFDR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the p-values for an estimated false discovery rate with ANOVA F-value between label/feature for classification tasks. This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "PCA_LAPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "LAPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "RFE_GradientBoosting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Gradient-Boosting classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          7
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "StandardScaler0f_classifPercentile1RandomTreesEmbedding2ExtraTreesClassifier3PowerTransformer0f_classifPercentile5_2ExtraTreesClassifier6_3MinMaxScaler0VarianceThreshold8_2RandomTreesEmbedding9_1RUSBoostClassifier10_4RandomForestMeta4_7_11": {
    "id": "StandardScaler0f_classifPercentile1RandomTreesEmbedding2ExtraTreesClassifier3PowerTransformer0f_classifPercentile5_2ExtraTreesClassifier6_3MinMaxScaler0VarianceThreshold8_2RandomTreesEmbedding9_1RUSBoostClassifier10_4RandomForestMeta4_7_11",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5,
          2
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          1
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          4
        ],
        "primitive": {
          "name": "RUSBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Random under-sampling integrating in the learning of an AdaBoost classifier. During learning, the problem of class balancing is alleviated by random under-sampling the sample at each iteration of the boosting algorithm.",
          "type": "Classifier"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          4,
          7,
          11
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "LabelEncoder0PCA_ARPACK1DecisionTreeClassifier2": {
    "id": "LabelEncoder0PCA_ARPACK1DecisionTreeClassifier2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "LabelEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerEncoderPrim0Normalizer1VarianceThreshold2PCA_ARPACK3RidgeClassifierCV4": {
    "id": "ImputerEncoderPrim0Normalizer1VarianceThreshold2PCA_ARPACK3RidgeClassifierCV4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "RidgeClassifierCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ridge classifier with built-in cross-validation. By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0MaxAbsScaler1RFE_RandomForest2IncrementalPCA3LogisticRegression4": {
    "id": "ImputerMedian0MaxAbsScaler1RFE_RandomForest2IncrementalPCA3LogisticRegression4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "IncrementalPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Incremental principal components analysis (IPCA). Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA. This algorithm has constant memory complexity.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LogisticRegression",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.) This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerMedian0Normalizer1f_classifFWE2PCA_ARPACK3EasyEnsembleClassifier4ImputerEncoderPrim0_1QuantileTransformer6_2f_classifFPR7_2KernelPCA8_4QuadraticDiscriminantAnalysis9_4MajorityVoting5_10": {
    "id": "ImputerMedian0Normalizer1f_classifFWE2PCA_ARPACK3EasyEnsembleClassifier4ImputerEncoderPrim0_1QuantileTransformer6_2f_classifFPR7_2KernelPCA8_4QuadraticDiscriminantAnalysis9_4MajorityVoting5_10",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_ARPACK",
          "hyperparams_run": {
            "default": true
          },
          "description": "ARPACK principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "EasyEnsembleClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Bag of balanced boosted learners also known as EasyEnsemble. This algorithm is known as EasyEnsemble [1]. The classifier is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "f_classifFPR",
          "hyperparams_run": {
            "default": true
          },
          "description": "Filter: Select the pvalues below alpha based on a FPR test with ANOVA F-value between label/feature for classification tasks. FPR test stands for False Positive Rate test. It controls the total amount of false detections.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          4
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          4
        ],
        "primitive": {
          "name": "QuadraticDiscriminantAnalysis",
          "hyperparams_run": {
            "default": true
          },
          "description": "Quadratic Discriminant Analysis. A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          5,
          10
        ],
        "primitive": {
          "name": "MajorityVoting",
          "hyperparams_run": {
            "default": true
          },
          "description": "Ensemble method taking multiple classifier probability predictions and outputs a prediction by a majority voting.",
          "type": "ensemble"
        }
      }
    }
  },
  "ImputerEncoderPrim0PowerTransformer1AdaBoostClassifier2NumericData0imputerIndicator0StandardScaler5f_classifKbest6_2TruncatedSVD7_1LogisticRegressionCV8_1": {
    "id": "ImputerEncoderPrim0PowerTransformer1AdaBoostClassifier2NumericData0imputerIndicator0StandardScaler5f_classifKbest6_2TruncatedSVD7_1LogisticRegressionCV8_1",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode labels with value between 0 and n_classes-1.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "PowerTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. By default, zero-mean, unit-variance normalization is applied to the transformed data.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "AdaBoostClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An AdaBoost classifier. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME.",
          "type": "Classifier"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "NumericData",
          "hyperparams_run": {
            "default": true
          },
          "description": "Extracts only numeric data columns from input.",
          "type": "data preprocess"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "f_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          1
        ],
        "primitive": {
          "name": "TruncatedSVD",
          "hyperparams_run": {
            "default": true
          },
          "description": "Dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently. In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA). This estimator supports two algorithms: a fast randomized SVD solver, and a \u201cnaive\u201d algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "LogisticRegressionCV",
          "hyperparams_run": {
            "default": true
          },
          "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for cross-validation estimator. This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data. For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputerIndicator0Normalizer1f_classifFWE2DecisionTreeClassifier3": {
    "id": "imputerIndicator0Normalizer1f_classifFWE2DecisionTreeClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputerIndicator",
          "hyperparams_run": {
            "default": true
          },
          "description": "All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "Normalizer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion). Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputer0MinMaxScaler1UnivariateSelectChiKbest2FastICA3XGBClassifier4ImputerMedian0MaxAbsScaler6_2f_classifPercentile7KernelPCA8_1LinearSVC9NumericData0_1XGBClassifier11_9RandomForestMeta5_10_12": {
    "id": "imputer0MinMaxScaler1UnivariateSelectChiKbest2FastICA3XGBClassifier4ImputerMedian0MaxAbsScaler6_2f_classifPercentile7KernelPCA8_1LinearSVC9NumericData0_1XGBClassifier11_9RandomForestMeta5_10_12",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7
        ],
        "primitive": {
          "name": "f_classifPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          8,
          1
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "NumericData",
          "hyperparams_run": {
            "default": true
          },
          "description": "Extracts only numeric data columns from input.",
          "type": "data preprocess"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          9
        ],
        "primitive": {
          "name": "XGBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.",
          "type": "Classifier"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          5,
          10,
          12
        ],
        "primitive": {
          "name": "RandomForestMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "A random forest classifier. A meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
          "type": "ensemble"
        }
      }
    }
  },
  "imputer0RobustScaler1VarianceThreshold2PCA_Randomized3DecisionTreeClassifier4OneHotEncoder0_1VarianceThreshold6_2PCA_Randomized7_4ImputerMedian0_1RobustScaler9_2f_classifFWE10_1PCA_Randomized11_1GaussianNBClassifier12_7GradientBoostingClassifierMeta5_13_8": {
    "id": "imputer0RobustScaler1VarianceThreshold2PCA_Randomized3DecisionTreeClassifier4OneHotEncoder0_1VarianceThreshold6_2PCA_Randomized7_4ImputerMedian0_1RobustScaler9_2f_classifFWE10_1PCA_Randomized11_1GaussianNBClassifier12_7GradientBoostingClassifierMeta5_13_8",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "DecisionTreeClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "A decision tree classifier.",
          "type": "Classifier"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6,
          2
        ],
        "primitive": {
          "name": "VarianceThreshold",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature selector that removes all low-variance features.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          4
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "9": {
        "index": 9,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "10": {
        "index": 10,
        "inputs": [
          9,
          2
        ],
        "primitive": {
          "name": "RobustScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method. Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",
          "type": "feature preprocess"
        }
      },
      "11": {
        "index": 11,
        "inputs": [
          10,
          1
        ],
        "primitive": {
          "name": "f_classifFWE",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select the p-values corresponding to Family-wise error rate with ANOVA F-value between label/feature for classification tasks.",
          "type": "feature selection"
        }
      },
      "12": {
        "index": 12,
        "inputs": [
          11,
          1
        ],
        "primitive": {
          "name": "PCA_Randomized",
          "hyperparams_run": {
            "default": true
          },
          "description": "Randomized SVD principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.",
          "type": "feature engineering"
        }
      },
      "13": {
        "index": 13,
        "inputs": [
          12,
          7
        ],
        "primitive": {
          "name": "GaussianNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gaussian Naive Bayes (GaussianNB). Can perform online updates to model parameters via partial_fit method.",
          "type": "Classifier"
        }
      },
      "14": {
        "index": 14,
        "inputs": [
          5,
          13,
          8
        ],
        "primitive": {
          "name": "GradientBoostingClassifierMeta",
          "hyperparams_run": {
            "default": true
          },
          "description": "Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
          "type": "ensemble"
        }
      }
    }
  },
  "imputer0MaxAbsScaler1mutual_info_classifKbest2KernelPCA3BernoulliNBClassifier4": {
    "id": "imputer0MaxAbsScaler1mutual_info_classifKbest2KernelPCA3BernoulliNBClassifier4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MaxAbsScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Scale each feature by its maximum absolute value. his estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. This scaler can also be applied to sparse CSR or CSC matrices.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "KernelPCA",
          "hyperparams_run": {
            "default": true
          },
          "description": "Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      }
    }
  },
  "ImputerOneHotEncoderPrim0MinMaxScaler1UnivariateSelectChiPercentile2BernoulliNBClassifier3OneHotEncoder0_1QuantileTransformer5mutual_info_classifKbest6FastICA7_2": {
    "id": "ImputerOneHotEncoderPrim0MinMaxScaler1UnivariateSelectChiPercentile2BernoulliNBClassifier3OneHotEncoder0_1QuantileTransformer5mutual_info_classifKbest6FastICA7_2",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerOneHotEncoderPrim",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values and encode categorical one-hot.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "MinMaxScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "UnivariateSelectChiPercentile",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to a percentile of the highest scores with Chi-square",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "BernoulliNBClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
          "type": "Classifier"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          0,
          1
        ],
        "primitive": {
          "name": "OneHotEncoder",
          "hyperparams_run": {
            "default": true
          },
          "description": "Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.",
          "type": "data preprocess"
        }
      },
      "6": {
        "index": 6,
        "inputs": [
          5
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "7": {
        "index": 7,
        "inputs": [
          6
        ],
        "primitive": {
          "name": "mutual_info_classifKbest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Select features according to the k highest scores with Mutual information for a discrete target.",
          "type": "feature selection"
        }
      },
      "8": {
        "index": 8,
        "inputs": [
          7,
          2
        ],
        "primitive": {
          "name": "FastICA",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      }
    }
  },
  "ImputerMedian0StandardScaler1RFE_RandomForest2RandomTreesEmbedding3LinearSVC4": {
    "id": "ImputerMedian0StandardScaler1RFE_RandomForest2RandomTreesEmbedding3LinearSVC4",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "ImputerMedian",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by median.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "StandardScaler",
          "hyperparams_run": {
            "default": true
          },
          "description": "Standardize features by removing the mean and scaling to unit variance",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "RandomTreesEmbedding",
          "hyperparams_run": {
            "default": true
          },
          "description": "FastICA: a fast algorithm for Independent Component Analysis.",
          "type": "feature engineering"
        }
      },
      "5": {
        "index": 5,
        "inputs": [
          4
        ],
        "primitive": {
          "name": "LinearSVC",
          "hyperparams_run": {
            "default": true
          },
          "description": "Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "type": "Classifier"
        }
      }
    }
  },
  "imputer0QuantileTransformer1RFE_RandomForest2ExtraTreesClassifier3": {
    "id": "imputer0QuantileTransformer1RFE_RandomForest2ExtraTreesClassifier3",
    "steps": {
      "1": {
        "index": 1,
        "inputs": [
          0
        ],
        "primitive": {
          "name": "imputer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Imputation transformer for completing missing values by mean.",
          "type": "data preprocess"
        }
      },
      "2": {
        "index": 2,
        "inputs": [
          1
        ],
        "primitive": {
          "name": "QuantileTransformer",
          "hyperparams_run": {
            "default": true
          },
          "description": "Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. The transformation is applied on each feature independently. The cumulative distribution function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.",
          "type": "feature preprocess"
        }
      },
      "3": {
        "index": 3,
        "inputs": [
          2
        ],
        "primitive": {
          "name": "RFE_RandomForest",
          "hyperparams_run": {
            "default": true
          },
          "description": "Feature ranking with recursive feature elimination with Random-Forest classifier. Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.",
          "type": "feature selection"
        }
      },
      "4": {
        "index": 4,
        "inputs": [
          3
        ],
        "primitive": {
          "name": "ExtraTreesClassifier",
          "hyperparams_run": {
            "default": true
          },
          "description": "An extra-trees classifier. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
          "type": "Classifier"
        }
      }
    }
  }
}